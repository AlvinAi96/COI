{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/22/2019 11:19:53 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "11/22/2019 11:19:53 - INFO - pytorch_transformers.tokenization_utils -   Model name '../model/chinese_roberta_wwm_large_ext_pytorch' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming '../model/chinese_roberta_wwm_large_ext_pytorch' is a path or url to a directory containing tokenizer files.\n",
      "11/22/2019 11:19:53 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../model/chinese_roberta_wwm_large_ext_pytorch/added_tokens.json. We won't load it.\n",
      "11/22/2019 11:19:53 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../model/chinese_roberta_wwm_large_ext_pytorch/special_tokens_map.json. We won't load it.\n",
      "11/22/2019 11:19:53 - INFO - pytorch_transformers.tokenization_utils -   loading file ../model/chinese_roberta_wwm_large_ext_pytorch/vocab.txt\n",
      "11/22/2019 11:19:53 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/22/2019 11:19:53 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/22/2019 11:19:53 - INFO - pytorch_transformers.modeling_utils -   loading configuration file ../model/chinese_roberta_wwm_large_ext_pytorch/config.json\n",
      "11/22/2019 11:19:53 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 3,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "11/22/2019 11:19:53 - INFO - pytorch_transformers.modeling_utils -   loading weights file ../model/chinese_roberta_wwm_large_ext_pytorch/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "11/22/2019 11:20:06 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'W.0.weight', 'W.0.bias', 'gru.0.weight_ih_l0', 'gru.0.weight_hh_l0', 'gru.0.bias_ih_l0', 'gru.0.bias_hh_l0', 'gru.0.weight_ih_l0_reverse', 'gru.0.weight_hh_l0_reverse', 'gru.0.bias_ih_l0_reverse', 'gru.0.bias_hh_l0_reverse']\n",
      "11/22/2019 11:20:06 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "11/22/2019 11:20:06 - INFO - __main__ -   *** Example ***\n",
      "11/22/2019 11:20:06 - INFO - __main__ -   idx: 0\n",
      "11/22/2019 11:20:06 - INFO - __main__ -   guid: 7a3dd79f90ee419da87190cff60f7a86\n",
      "11/22/2019 11:20:06 - INFO - __main__ -   tokens: [CLS] 问 责 领 导 ( 上 黄 镇 党 委 书 记 张 涛 ， 宣 国 才 真 能 一 手 遮 天 吗 ？ ) [SEP] 这 几 天 看 了 有 人 举 报 施 某 某 的 贴 子 ， 经 与 举 报 人 联 系 证 实 ， 是 宣 某 当 天 中 午 请 举 报 人 和 枪 手 喝 酒 后 ， 晚 上 才 发 的 贴 子 ！ 本 人 不 去 讨 论 前 二 天 的 举 报 ， 相 信 总 归 会 有 说 法 的 ！ 今 天 一 看 施 全 军 2017 年 1 月 2 日 实 名 举 报 上 黄 镇 宣 国 才 的 贴 子 （ 仍 被 锁 定 禁 止 评 论 ） 已 经 正 好 一 整 年 了 = 750 ) window . open ( ' http : / / img . js ##ly ##001 . com / at ##ta ##ch ##ment / mon _ 180 ##1 / 4 _ 291 ##08 ##5 _ c ##79 ##6 ##a ##6 ##a ##86 ##e ##17 ##12 ##1 . jpg ? 123 ' ) ; \" on ##load = \" if ( this . off ##set ##wi ##dt ##h > ' 750 ' ) this . wi ##dt ##h = ' 750 ' ; \" sr ##c = \" http : / / img . js ##ly ##001 . com / at ##ta ##ch ##ment / mon _ 180 ##1 / 4 _ 291 ##08 ##5 _ c ##79 ##6 ##a ##6 ##a ##86 ##e ##17 ##12 ##1 . jpg ? 123 \" style = \" max - wi ##dt ##h : 750 ##px ; \" / > 图 片 : / home / al ##ida ##ta / www / data / tm ##p / q ##fu ##pl ##oa ##d / 4 _ 291 ##08 ##5 _ 151 ##49 ##81 ##47 ##14 ##78 ##95 ##2 . jpg 施 全 军 实 名 举 报 50 天 后 ， 上 黄 镇 党 委 政 府 回 复 如 下 图 ： = 750 ) window . open ( ' http : / / img . js ##ly ##001 . com / at ##ta ##ch ##ment / mon _ 180 ##1 / 4 _ 291 ##08 ##5 _ a9 ##b ##11 ##b ##7 ##ea ##2 ##b ##1 ##ce ##9 . jpg ? 90 ' ) ; \" on ##load = \" if ( this . off ##set ##wi ##dt ##h > ' 750 ' ) this . wi ##dt ##h = ' 750 ' ; \" sr ##c = \" http : / / img . js ##ly ##001 . com / at ##ta ##ch ##ment / mon _ 180 ##1 / 4 _ 291 ##08 ##5 _ a9 ##b ##11 ##b ##7 ##ea ##2 ##b ##1 ##ce ##9 . jpg ? 90 \" style = \" max - wi ##dt ##h : 750 ##px ; \" / > 图 片 : / home / [SEP]\n",
      "11/22/2019 11:20:06 - INFO - __main__ -   input_ids: 101 7309 6569 7566 2193 113 677 7942 7252 1054 1999 741 6381 2476 3875 8024 2146 1744 2798 4696 5543 671 2797 6902 1921 1408 8043 114 102 6821 1126 1921 4692 749 3300 782 715 2845 3177 3378 3378 4638 6585 2094 8024 5307 680 715 2845 782 5468 5143 6395 2141 8024 3221 2146 3378 2496 1921 704 1286 6435 715 2845 782 1469 3366 2797 1600 6983 1400 8024 3241 677 2798 1355 4638 6585 2094 8013 3315 782 679 1343 6374 6389 1184 753 1921 4638 715 2845 8024 4685 928 2600 2495 833 3300 6432 3791 4638 8013 791 1921 671 4692 3177 1059 1092 8109 2399 122 3299 123 3189 2141 1399 715 2845 677 7942 7252 2146 1744 2798 4638 6585 2094 8020 793 6158 7219 2137 4881 3632 6397 6389 8021 2347 5307 3633 1962 671 3146 2399 749 134 9180 114 12158 119 8893 113 112 8184 131 120 120 11412 119 9016 8436 9942 119 8134 120 8243 8383 8370 8631 120 8556 142 8420 8148 120 125 142 11777 9153 8157 142 145 9495 8158 8139 8158 8139 9219 8154 8408 8455 8148 119 9248 136 8604 112 114 132 107 8281 11713 134 107 8898 113 8554 119 9594 9852 10958 12672 8199 135 112 9180 112 114 8554 119 8541 12672 8199 134 112 9180 112 132 107 12109 8177 134 107 8184 131 120 120 11412 119 9016 8436 9942 119 8134 120 8243 8383 8370 8631 120 8556 142 8420 8148 120 125 142 11777 9153 8157 142 145 9495 8158 8139 8158 8139 9219 8154 8408 8455 8148 119 9248 136 8604 107 8969 134 107 8621 118 8541 12672 8199 131 9180 10605 132 107 120 135 1745 4275 131 120 8563 120 9266 12708 8383 120 8173 120 9000 120 9908 8187 120 159 12043 12569 11355 8168 120 125 142 11777 9153 8157 142 9564 9500 9313 9050 8717 9136 9102 8144 119 9248 3177 1059 1092 2141 1399 715 2845 8145 1921 1400 8024 677 7942 7252 1054 1999 3124 2424 1726 1908 1963 678 1745 8038 134 9180 114 12158 119 8893 113 112 8184 131 120 120 11412 119 9016 8436 9942 119 8134 120 8243 8383 8370 8631 120 8556 142 8420 8148 120 125 142 11777 9153 8157 142 11094 8204 8452 8204 8161 10073 8144 8204 8148 8328 8160 119 9248 136 8192 112 114 132 107 8281 11713 134 107 8898 113 8554 119 9594 9852 10958 12672 8199 135 112 9180 112 114 8554 119 8541 12672 8199 134 112 9180 112 132 107 12109 8177 134 107 8184 131 120 120 11412 119 9016 8436 9942 119 8134 120 8243 8383 8370 8631 120 8556 142 8420 8148 120 125 142 11777 9153 8157 142 11094 8204 8452 8204 8161 10073 8144 8204 8148 8328 8160 119 9248 136 8192 107 8969 134 107 8621 118 8541 12672 8199 131 9180 10605 132 107 120 135 1745 4275 131 120 8563 120 102\n",
      "11/22/2019 11:20:06 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/22/2019 11:20:06 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/22/2019 11:20:06 - INFO - __main__ -   label: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/22/2019 11:20:55 - INFO - __main__ -   ***** Running training *****\n",
      "11/22/2019 11:20:55 - INFO - __main__ -     Num examples = 11755\n",
      "11/22/2019 11:20:55 - INFO - __main__ -     Batch size = 4\n",
      "11/22/2019 11:20:55 - INFO - __main__ -     Num steps = 30000\n",
      "  0%|                                                 | 0/30000 [00:00<?, ?it/s]11/22/2019 11:21:07 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/22/2019 11:21:07 - INFO - __main__ -     Num examples = 2941\n",
      "11/22/2019 11:21:07 - INFO - __main__ -     Batch size = 64\n"
     ]
    }
   ],
   "source": [
    "# 5121 \n",
    "!python run_bert_2562.py \\\n",
    "--model_type bert \\\n",
    "--model_name_or_path ../model/chinese_roberta_wwm_large_ext_pytorch  \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_test \\\n",
    "--data_dir ../data/data_StratifiedKFold_42/data_origin_0 \\\n",
    "--output_dir ../model/roberta_wwm_large_5121_42/roberta_wwm_large_5121_42_0 \\\n",
    "--max_seq_length 512 \\\n",
    "--split_num 1 \\\n",
    "--lstm_hidden_size 512 \\\n",
    "--lstm_layers 1 \\\n",
    "--lstm_dropout 0.1 \\\n",
    "--eval_steps 200 \\\n",
    "--per_gpu_train_batch_size 4 \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--warmup_steps 0 \\\n",
    "--per_gpu_eval_batch_size 64 \\\n",
    "--learning_rate 1e-5 \\\n",
    "--adam_epsilon 1e-6 \\\n",
    "--weight_decay 0 \\\n",
    "--train_steps 30000 \\\n",
    "--freeze 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/07/2019 17:13:21 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "11/07/2019 17:13:21 - INFO - pytorch_transformers.tokenization_utils -   Model name '../BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/chinese_roberta_wwm_large_ext_pytorch' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming '../BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/chinese_roberta_wwm_large_ext_pytorch' is a path or url to a directory containing tokenizer files.\n",
      "11/07/2019 17:13:21 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/chinese_roberta_wwm_large_ext_pytorch/added_tokens.json. We won't load it.\n",
      "11/07/2019 17:13:21 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/chinese_roberta_wwm_large_ext_pytorch/special_tokens_map.json. We won't load it.\n",
      "11/07/2019 17:13:21 - INFO - pytorch_transformers.tokenization_utils -   loading file ../BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/chinese_roberta_wwm_large_ext_pytorch/vocab.txt\n",
      "11/07/2019 17:13:21 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/07/2019 17:13:21 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/07/2019 17:13:21 - INFO - pytorch_transformers.modeling_utils -   loading configuration file ../BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/chinese_roberta_wwm_large_ext_pytorch/config.json\n",
      "11/07/2019 17:13:21 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 3,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "11/07/2019 17:13:21 - INFO - pytorch_transformers.modeling_utils -   loading weights file ../BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/chinese_roberta_wwm_large_ext_pytorch/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "11/07/2019 17:13:37 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'W.0.weight', 'W.0.bias', 'gru.0.weight_ih_l0', 'gru.0.weight_hh_l0', 'gru.0.bias_ih_l0', 'gru.0.bias_hh_l0', 'gru.0.weight_ih_l0_reverse', 'gru.0.weight_hh_l0_reverse', 'gru.0.bias_ih_l0_reverse', 'gru.0.bias_hh_l0_reverse']\n",
      "11/07/2019 17:13:37 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "11/07/2019 17:13:38 - INFO - __main__ -   *** Example ***\n",
      "11/07/2019 17:13:38 - INFO - __main__ -   idx: 0\n",
      "11/07/2019 17:13:38 - INFO - __main__ -   guid: 7a3dd79f90ee419da87190cff60f7a86\n",
      "11/07/2019 17:13:38 - INFO - __main__ -   tokens: [CLS] 问 责 领 导 ( 上 黄 镇 党 委 书 记 张 涛 ， 宣 国 才 真 能 一 手 遮 天 吗 ？ ) [SEP] 这 几 天 看 了 有 人 举 报 施 某 某 的 贴 子 ， 经 与 举 报 人 联 系 证 实 ， 是 宣 某 当 天 中 午 请 举 报 人 和 枪 手 喝 酒 后 ， 晚 上 才 发 的 贴 子 ！ 本 人 不 去 讨 论 前 二 天 的 举 报 ， 相 信 总 归 会 有 说 法 的 ！ 今 天 一 看 施 全 军 2017 年 1 月 2 日 实 名 举 报 上 黄 镇 宣 国 才 的 贴 子 （ 仍 被 锁 定 禁 止 评 论 ） 已 经 正 好 一 整 年 了 = 750 ) window . open ( ' http : / / img . js ##ly ##001 . com / at ##ta ##ch ##ment / mon _ 180 ##1 / 4 _ 291 ##08 ##5 _ c ##79 ##6 ##a ##6 ##a ##86 ##e ##17 ##12 ##1 . jpg ? 123 ' ) ; \" on ##load = \" if ( this . off ##set ##wi ##dt ##h > ' 750 ' ) this . wi ##dt ##h = ' 750 ' ; \" sr ##c = \" http : / / img . js ##ly ##001 . com / at ##ta ##ch ##ment / mon _ 180 ##1 / 4 _ 291 ##08 ##5 _ c ##79 ##6 ##a ##6 ##a ##86 ##e ##17 ##12 ##1 . jpg ? 123 \" style = \" max - wi ##dt ##h : 750 ##px ; \" / > 图 片 : / home / al ##ida ##ta / www / data / tm ##p / q ##fu ##pl ##oa ##d / 4 _ 291 ##08 ##5 _ 151 ##49 ##81 ##47 ##14 ##78 ##95 ##2 . jpg 施 全 军 实 名 举 报 50 天 后 ， 上 黄 镇 党 委 政 府 回 复 如 下 图 ： = 750 ) window . open ( ' http : / / img . js ##ly ##001 . com / at ##ta ##ch ##ment / mon _ 180 ##1 / 4 _ 291 ##08 ##5 _ a9 ##b ##11 ##b ##7 ##ea ##2 ##b ##1 ##ce ##9 . jpg ? 90 ' ) ; \" on ##load = \" if ( this . off ##set ##wi ##dt ##h > ' 750 ' ) this . wi ##dt ##h = ' 750 ' ; \" sr ##c = \" http : / / img . js ##ly ##001 . com / at ##ta ##ch ##ment / mon _ 180 ##1 / 4 _ 291 ##08 ##5 _ a9 ##b ##11 ##b ##7 ##ea ##2 ##b ##1 ##ce ##9 . jpg ? 90 \" style = \" max - wi ##dt ##h : 750 ##px ; \" / > 图 片 : / home / [SEP]\n",
      "11/07/2019 17:13:38 - INFO - __main__ -   input_ids: 101 7309 6569 7566 2193 113 677 7942 7252 1054 1999 741 6381 2476 3875 8024 2146 1744 2798 4696 5543 671 2797 6902 1921 1408 8043 114 102 6821 1126 1921 4692 749 3300 782 715 2845 3177 3378 3378 4638 6585 2094 8024 5307 680 715 2845 782 5468 5143 6395 2141 8024 3221 2146 3378 2496 1921 704 1286 6435 715 2845 782 1469 3366 2797 1600 6983 1400 8024 3241 677 2798 1355 4638 6585 2094 8013 3315 782 679 1343 6374 6389 1184 753 1921 4638 715 2845 8024 4685 928 2600 2495 833 3300 6432 3791 4638 8013 791 1921 671 4692 3177 1059 1092 8109 2399 122 3299 123 3189 2141 1399 715 2845 677 7942 7252 2146 1744 2798 4638 6585 2094 8020 793 6158 7219 2137 4881 3632 6397 6389 8021 2347 5307 3633 1962 671 3146 2399 749 134 9180 114 12158 119 8893 113 112 8184 131 120 120 11412 119 9016 8436 9942 119 8134 120 8243 8383 8370 8631 120 8556 142 8420 8148 120 125 142 11777 9153 8157 142 145 9495 8158 8139 8158 8139 9219 8154 8408 8455 8148 119 9248 136 8604 112 114 132 107 8281 11713 134 107 8898 113 8554 119 9594 9852 10958 12672 8199 135 112 9180 112 114 8554 119 8541 12672 8199 134 112 9180 112 132 107 12109 8177 134 107 8184 131 120 120 11412 119 9016 8436 9942 119 8134 120 8243 8383 8370 8631 120 8556 142 8420 8148 120 125 142 11777 9153 8157 142 145 9495 8158 8139 8158 8139 9219 8154 8408 8455 8148 119 9248 136 8604 107 8969 134 107 8621 118 8541 12672 8199 131 9180 10605 132 107 120 135 1745 4275 131 120 8563 120 9266 12708 8383 120 8173 120 9000 120 9908 8187 120 159 12043 12569 11355 8168 120 125 142 11777 9153 8157 142 9564 9500 9313 9050 8717 9136 9102 8144 119 9248 3177 1059 1092 2141 1399 715 2845 8145 1921 1400 8024 677 7942 7252 1054 1999 3124 2424 1726 1908 1963 678 1745 8038 134 9180 114 12158 119 8893 113 112 8184 131 120 120 11412 119 9016 8436 9942 119 8134 120 8243 8383 8370 8631 120 8556 142 8420 8148 120 125 142 11777 9153 8157 142 11094 8204 8452 8204 8161 10073 8144 8204 8148 8328 8160 119 9248 136 8192 112 114 132 107 8281 11713 134 107 8898 113 8554 119 9594 9852 10958 12672 8199 135 112 9180 112 114 8554 119 8541 12672 8199 134 112 9180 112 132 107 12109 8177 134 107 8184 131 120 120 11412 119 9016 8436 9942 119 8134 120 8243 8383 8370 8631 120 8556 142 8420 8148 120 125 142 11777 9153 8157 142 11094 8204 8452 8204 8161 10073 8144 8204 8148 8328 8160 119 9248 136 8192 107 8969 134 107 8621 118 8541 12672 8199 131 9180 10605 132 107 120 135 1745 4275 131 120 8563 120 102\n",
      "11/07/2019 17:13:38 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/07/2019 17:13:38 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/07/2019 17:13:38 - INFO - __main__ -   label: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/07/2019 17:15:12 - INFO - __main__ -   ***** Running training *****\n",
      "11/07/2019 17:15:12 - INFO - __main__ -     Num examples = 11756\n",
      "11/07/2019 17:15:12 - INFO - __main__ -     Batch size = 4\n",
      "11/07/2019 17:15:12 - INFO - __main__ -     Num steps = 30000\n",
      "  0%|                                                 | 0/30000 [00:00<?, ?it/s]11/07/2019 17:15:35 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 17:15:35 - INFO - __main__ -     Num examples = 2940\n",
      "11/07/2019 17:15:35 - INFO - __main__ -     Batch size = 64\n",
      "/root/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "11/07/2019 17:17:44 - INFO - __main__ -     eval_F1 = 0.06324671257734495\n",
      "11/07/2019 17:17:44 - INFO - __main__ -     eval_loss = 1.1599756012792173\n",
      "11/07/2019 17:17:44 - INFO - __main__ -     global_step = 0\n",
      "================================================================================\n",
      "Best F1 0.06324671257734495\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.6856:   3%|▋                       | 799/30000 [04:48<1:28:04,  5.53it/s]11/07/2019 17:20:01 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 17:20:01 - INFO - __main__ -     global_step = 200\n",
      "11/07/2019 17:20:01 - INFO - __main__ -     train loss = 0.6856\n",
      "loss 0.4948:   5%|█▏                     | 1599/30000 [07:02<1:26:04,  5.50it/s]11/07/2019 17:22:15 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 17:22:15 - INFO - __main__ -     global_step = 400\n",
      "11/07/2019 17:22:15 - INFO - __main__ -     train loss = 0.4948\n",
      "loss 0.4478:   8%|█▊                     | 2399/30000 [09:17<1:23:41,  5.50it/s]11/07/2019 17:24:30 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 17:24:30 - INFO - __main__ -     global_step = 600\n",
      "11/07/2019 17:24:30 - INFO - __main__ -     train loss = 0.4478\n",
      "loss 0.4244:  11%|██▍                    | 3199/30000 [11:31<1:22:25,  5.42it/s]11/07/2019 17:26:44 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 17:26:44 - INFO - __main__ -     global_step = 800\n",
      "11/07/2019 17:26:44 - INFO - __main__ -     train loss = 0.4244\n",
      "loss 0.4209:  13%|███                    | 3999/30000 [13:48<1:24:01,  5.16it/s]11/07/2019 17:29:00 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 17:29:00 - INFO - __main__ -     global_step = 1000\n",
      "11/07/2019 17:29:00 - INFO - __main__ -     train loss = 0.4209\n",
      "loss 0.4196:  16%|███▋                   | 4799/30000 [16:03<1:19:11,  5.30it/s]11/07/2019 17:31:16 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 17:31:16 - INFO - __main__ -     global_step = 1200\n",
      "11/07/2019 17:31:16 - INFO - __main__ -     train loss = 0.4196\n",
      "loss 0.3982:  19%|████▎                  | 5599/30000 [18:18<1:15:24,  5.39it/s]11/07/2019 17:33:31 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 17:33:31 - INFO - __main__ -     global_step = 1400\n",
      "11/07/2019 17:33:31 - INFO - __main__ -     train loss = 0.3982\n",
      "loss 0.3901:  21%|████▉                  | 6399/30000 [20:33<1:13:32,  5.35it/s]11/07/2019 17:35:46 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 17:35:46 - INFO - __main__ -     global_step = 1600\n",
      "11/07/2019 17:35:46 - INFO - __main__ -     train loss = 0.3901\n",
      "loss 0.3937:  24%|█████▌                 | 7199/30000 [22:48<1:09:34,  5.46it/s]11/07/2019 17:38:01 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 17:38:01 - INFO - __main__ -     global_step = 1800\n",
      "11/07/2019 17:38:01 - INFO - __main__ -     train loss = 0.3937\n",
      "loss 0.4057:  27%|██████▏                | 7999/30000 [25:03<1:07:26,  5.44it/s]11/07/2019 17:40:16 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 17:40:16 - INFO - __main__ -     global_step = 2000\n",
      "11/07/2019 17:40:16 - INFO - __main__ -     train loss = 0.4057\n",
      "loss 0.3693:  29%|██████▋                | 8799/30000 [27:17<1:03:45,  5.54it/s]11/07/2019 17:42:30 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 17:42:30 - INFO - __main__ -     global_step = 2200\n",
      "11/07/2019 17:42:30 - INFO - __main__ -     train loss = 0.3693\n",
      "loss 0.4068:  32%|███████▎               | 9599/30000 [29:30<1:01:18,  5.55it/s]11/07/2019 17:44:43 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 17:44:43 - INFO - __main__ -     global_step = 2400\n",
      "11/07/2019 17:44:43 - INFO - __main__ -     train loss = 0.4068\n",
      "loss 0.3624:  35%|███████▋              | 10399/30000 [31:44<1:01:25,  5.32it/s]11/07/2019 17:46:57 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 17:46:57 - INFO - __main__ -     global_step = 2600\n",
      "11/07/2019 17:46:57 - INFO - __main__ -     train loss = 0.3624\n",
      "loss 0.3396:  37%|████████▉               | 11199/30000 [33:58<56:54,  5.51it/s]11/07/2019 17:49:10 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 17:49:10 - INFO - __main__ -     global_step = 2800\n",
      "11/07/2019 17:49:10 - INFO - __main__ -     train loss = 0.3396\n",
      "11/07/2019 17:49:33 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 17:49:33 - INFO - __main__ -     Num examples = 2940\n",
      "11/07/2019 17:49:33 - INFO - __main__ -     Batch size = 64\n",
      "11/07/2019 17:51:44 - INFO - __main__ -     eval_F1 = 0.7624427366052142\n",
      "11/07/2019 17:51:44 - INFO - __main__ -     eval_loss = 0.35907633545929973\n",
      "11/07/2019 17:51:44 - INFO - __main__ -     global_step = 2800\n",
      "11/07/2019 17:51:44 - INFO - __main__ -     loss = 0.3396\n",
      "================================================================================\n",
      "Best F1 0.7624427366052142\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.3622:  40%|█████████▌              | 11999/30000 [38:48<55:04,  5.45it/s]11/07/2019 17:54:01 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 17:54:01 - INFO - __main__ -     global_step = 3000\n",
      "11/07/2019 17:54:01 - INFO - __main__ -     train loss = 0.3622\n",
      "11/07/2019 17:54:24 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 17:54:24 - INFO - __main__ -     Num examples = 2940\n",
      "11/07/2019 17:54:24 - INFO - __main__ -     Batch size = 64\n",
      "11/07/2019 17:56:35 - INFO - __main__ -     eval_F1 = 0.791642466912993\n",
      "11/07/2019 17:56:35 - INFO - __main__ -     eval_loss = 0.37058810120367486\n",
      "11/07/2019 17:56:35 - INFO - __main__ -     global_step = 3000\n",
      "11/07/2019 17:56:35 - INFO - __main__ -     loss = 0.3622\n",
      "================================================================================\n",
      "Best F1 0.791642466912993\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.2565:  43%|██████████▏             | 12799/30000 [43:40<52:26,  5.47it/s]11/07/2019 17:58:53 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 17:58:53 - INFO - __main__ -     global_step = 3200\n",
      "11/07/2019 17:58:53 - INFO - __main__ -     train loss = 0.2565\n",
      "11/07/2019 17:59:16 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 17:59:16 - INFO - __main__ -     Num examples = 2940\n",
      "11/07/2019 17:59:16 - INFO - __main__ -     Batch size = 64\n",
      "11/07/2019 18:01:27 - INFO - __main__ -     eval_F1 = 0.7540040127593909\n",
      "11/07/2019 18:01:27 - INFO - __main__ -     eval_loss = 0.3783335822593907\n",
      "11/07/2019 18:01:27 - INFO - __main__ -     global_step = 3200\n",
      "11/07/2019 18:01:27 - INFO - __main__ -     loss = 0.2565\n",
      "================================================================================\n",
      "loss 0.2664:  45%|██████████▉             | 13599/30000 [48:28<49:46,  5.49it/s]11/07/2019 18:03:40 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 18:03:40 - INFO - __main__ -     global_step = 3400\n",
      "11/07/2019 18:03:40 - INFO - __main__ -     train loss = 0.2664\n",
      "11/07/2019 18:04:03 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 18:04:03 - INFO - __main__ -     Num examples = 2940\n",
      "11/07/2019 18:04:03 - INFO - __main__ -     Batch size = 64\n",
      "11/07/2019 18:06:14 - INFO - __main__ -     eval_F1 = 0.785092064623426\n",
      "11/07/2019 18:06:14 - INFO - __main__ -     eval_loss = 0.36579862828163995\n",
      "11/07/2019 18:06:14 - INFO - __main__ -     global_step = 3400\n",
      "11/07/2019 18:06:14 - INFO - __main__ -     loss = 0.2664\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.23:  48%|████████████▍             | 14399/30000 [53:14<47:14,  5.50it/s]11/07/2019 18:08:27 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 18:08:27 - INFO - __main__ -     global_step = 3600\n",
      "11/07/2019 18:08:27 - INFO - __main__ -     train loss = 0.23\n",
      "11/07/2019 18:08:49 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 18:08:49 - INFO - __main__ -     Num examples = 2940\n",
      "11/07/2019 18:08:49 - INFO - __main__ -     Batch size = 64\n",
      "11/07/2019 18:11:00 - INFO - __main__ -     eval_F1 = 0.7818695020988464\n",
      "11/07/2019 18:11:00 - INFO - __main__ -     eval_loss = 0.4187798039258822\n",
      "11/07/2019 18:11:00 - INFO - __main__ -     global_step = 3600\n",
      "11/07/2019 18:11:00 - INFO - __main__ -     loss = 0.23\n",
      "================================================================================\n",
      "loss 0.2435:  51%|████████████▏           | 15199/30000 [58:01<45:02,  5.48it/s]11/07/2019 18:13:14 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 18:13:14 - INFO - __main__ -     global_step = 3800\n",
      "11/07/2019 18:13:14 - INFO - __main__ -     train loss = 0.2435\n",
      "11/07/2019 18:13:37 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 18:13:37 - INFO - __main__ -     Num examples = 2940\n",
      "11/07/2019 18:13:37 - INFO - __main__ -     Batch size = 64\n",
      "11/07/2019 18:15:48 - INFO - __main__ -     eval_F1 = 0.7760716311805215\n",
      "11/07/2019 18:15:48 - INFO - __main__ -     eval_loss = 0.39183093996151636\n",
      "11/07/2019 18:15:48 - INFO - __main__ -     global_step = 3800\n",
      "11/07/2019 18:15:48 - INFO - __main__ -     loss = 0.2435\n",
      "================================================================================\n",
      "loss 0.2394:  53%|███████████▋          | 15999/30000 [1:02:49<42:46,  5.46it/s]11/07/2019 18:18:02 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 18:18:02 - INFO - __main__ -     global_step = 4000\n",
      "11/07/2019 18:18:02 - INFO - __main__ -     train loss = 0.2394\n",
      "11/07/2019 18:18:25 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 18:18:25 - INFO - __main__ -     Num examples = 2940\n",
      "11/07/2019 18:18:25 - INFO - __main__ -     Batch size = 64\n",
      "11/07/2019 18:20:35 - INFO - __main__ -     eval_F1 = 0.80239286370484\n",
      "11/07/2019 18:20:35 - INFO - __main__ -     eval_loss = 0.3958621569301771\n",
      "11/07/2019 18:20:35 - INFO - __main__ -     global_step = 4000\n",
      "11/07/2019 18:20:35 - INFO - __main__ -     loss = 0.2394\n",
      "================================================================================\n",
      "Best F1 0.80239286370484\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.2102:  56%|████████████▎         | 16799/30000 [1:07:41<39:54,  5.51it/s]11/07/2019 18:22:54 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 18:22:54 - INFO - __main__ -     global_step = 4200\n",
      "11/07/2019 18:22:54 - INFO - __main__ -     train loss = 0.2102\n",
      "11/07/2019 18:23:16 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 18:23:16 - INFO - __main__ -     Num examples = 2940\n",
      "11/07/2019 18:23:16 - INFO - __main__ -     Batch size = 64\n",
      "11/07/2019 18:25:27 - INFO - __main__ -     eval_F1 = 0.7549294503592875\n",
      "11/07/2019 18:25:27 - INFO - __main__ -     eval_loss = 0.4105102265172679\n",
      "11/07/2019 18:25:27 - INFO - __main__ -     global_step = 4200\n",
      "11/07/2019 18:25:27 - INFO - __main__ -     loss = 0.2102\n",
      "================================================================================\n",
      "loss 0.2599:  59%|████████████▉         | 17599/30000 [1:12:29<39:56,  5.18it/s]11/07/2019 18:27:41 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 18:27:41 - INFO - __main__ -     global_step = 4400\n",
      "11/07/2019 18:27:41 - INFO - __main__ -     train loss = 0.2599\n",
      "11/07/2019 18:28:04 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 18:28:04 - INFO - __main__ -     Num examples = 2940\n",
      "11/07/2019 18:28:04 - INFO - __main__ -     Batch size = 64\n",
      "11/07/2019 18:30:15 - INFO - __main__ -     eval_F1 = 0.7909650343485634\n",
      "11/07/2019 18:30:15 - INFO - __main__ -     eval_loss = 0.38498143803166307\n",
      "11/07/2019 18:30:15 - INFO - __main__ -     global_step = 4400\n",
      "11/07/2019 18:30:15 - INFO - __main__ -     loss = 0.2599\n",
      "================================================================================\n",
      "loss 0.2224:  61%|█████████████▍        | 18399/30000 [1:17:16<35:10,  5.50it/s]11/07/2019 18:32:29 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 18:32:29 - INFO - __main__ -     global_step = 4600\n",
      "11/07/2019 18:32:29 - INFO - __main__ -     train loss = 0.2224\n",
      "11/07/2019 18:32:52 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 18:32:52 - INFO - __main__ -     Num examples = 2940\n",
      "11/07/2019 18:32:52 - INFO - __main__ -     Batch size = 64\n",
      "11/07/2019 18:35:03 - INFO - __main__ -     eval_F1 = 0.8002480519441071\n",
      "11/07/2019 18:35:03 - INFO - __main__ -     eval_loss = 0.3862013777151056\n",
      "11/07/2019 18:35:03 - INFO - __main__ -     global_step = 4600\n",
      "11/07/2019 18:35:03 - INFO - __main__ -     loss = 0.2224\n",
      "================================================================================\n",
      "loss 0.1995:  64%|██████████████        | 19199/30000 [1:22:04<33:17,  5.41it/s]11/07/2019 18:37:16 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 18:37:16 - INFO - __main__ -     global_step = 4800\n",
      "11/07/2019 18:37:16 - INFO - __main__ -     train loss = 0.1995\n",
      "11/07/2019 18:37:39 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 18:37:39 - INFO - __main__ -     Num examples = 2940\n",
      "11/07/2019 18:37:39 - INFO - __main__ -     Batch size = 64\n",
      "11/07/2019 18:39:50 - INFO - __main__ -     eval_F1 = 0.7883475319746789\n",
      "11/07/2019 18:39:50 - INFO - __main__ -     eval_loss = 0.4064967458176872\n",
      "11/07/2019 18:39:50 - INFO - __main__ -     global_step = 4800\n",
      "11/07/2019 18:39:50 - INFO - __main__ -     loss = 0.1995\n",
      "================================================================================\n",
      "loss 0.2511:  67%|██████████████▋       | 19999/30000 [1:26:51<30:29,  5.47it/s]11/07/2019 18:42:04 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 18:42:04 - INFO - __main__ -     global_step = 5000\n",
      "11/07/2019 18:42:04 - INFO - __main__ -     train loss = 0.2511\n",
      "11/07/2019 18:42:26 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 18:42:26 - INFO - __main__ -     Num examples = 2940\n",
      "11/07/2019 18:42:26 - INFO - __main__ -     Batch size = 64\n",
      "11/07/2019 18:44:37 - INFO - __main__ -     eval_F1 = 0.7931617742738064\n",
      "11/07/2019 18:44:37 - INFO - __main__ -     eval_loss = 0.3818662650883198\n",
      "11/07/2019 18:44:37 - INFO - __main__ -     global_step = 5000\n",
      "11/07/2019 18:44:37 - INFO - __main__ -     loss = 0.2511\n",
      "================================================================================\n",
      "loss 0.2076:  69%|███████████████▎      | 20799/30000 [1:31:37<28:03,  5.47it/s]11/07/2019 18:46:50 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 18:46:50 - INFO - __main__ -     global_step = 5200\n",
      "11/07/2019 18:46:50 - INFO - __main__ -     train loss = 0.2076\n",
      "11/07/2019 18:47:11 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 18:47:11 - INFO - __main__ -     Num examples = 2940\n",
      "11/07/2019 18:47:11 - INFO - __main__ -     Batch size = 64\n",
      "11/07/2019 18:49:22 - INFO - __main__ -     eval_F1 = 0.7896173565593219\n",
      "11/07/2019 18:49:22 - INFO - __main__ -     eval_loss = 0.3871398797339719\n",
      "11/07/2019 18:49:22 - INFO - __main__ -     global_step = 5200\n",
      "11/07/2019 18:49:22 - INFO - __main__ -     loss = 0.2076\n",
      "================================================================================\n",
      "loss 0.2491:  72%|███████████████▊      | 21599/30000 [1:36:22<25:35,  5.47it/s]11/07/2019 18:51:34 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 18:51:34 - INFO - __main__ -     global_step = 5400\n",
      "11/07/2019 18:51:34 - INFO - __main__ -     train loss = 0.2491\n",
      "11/07/2019 18:51:56 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 18:51:56 - INFO - __main__ -     Num examples = 2940\n",
      "11/07/2019 18:51:56 - INFO - __main__ -     Batch size = 64\n",
      "11/07/2019 18:54:07 - INFO - __main__ -     eval_F1 = 0.7882147784415121\n",
      "11/07/2019 18:54:07 - INFO - __main__ -     eval_loss = 0.37154050851645676\n",
      "11/07/2019 18:54:07 - INFO - __main__ -     global_step = 5400\n",
      "11/07/2019 18:54:07 - INFO - __main__ -     loss = 0.2491\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.1827:  75%|████████████████▍     | 22399/30000 [1:41:06<23:02,  5.50it/s]11/07/2019 18:56:19 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 18:56:19 - INFO - __main__ -     global_step = 5600\n",
      "11/07/2019 18:56:19 - INFO - __main__ -     train loss = 0.1827\n",
      "11/07/2019 18:56:41 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 18:56:41 - INFO - __main__ -     Num examples = 2940\n",
      "11/07/2019 18:56:41 - INFO - __main__ -     Batch size = 64\n",
      "11/07/2019 18:58:52 - INFO - __main__ -     eval_F1 = 0.7888697098053922\n",
      "11/07/2019 18:58:52 - INFO - __main__ -     eval_loss = 0.3814440776148568\n",
      "11/07/2019 18:58:52 - INFO - __main__ -     global_step = 5600\n",
      "11/07/2019 18:58:52 - INFO - __main__ -     loss = 0.1827\n",
      "================================================================================\n",
      "loss 0.1723:  77%|█████████████████     | 23199/30000 [1:45:52<20:42,  5.48it/s]11/07/2019 19:01:05 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 19:01:05 - INFO - __main__ -     global_step = 5800\n",
      "11/07/2019 19:01:05 - INFO - __main__ -     train loss = 0.1723\n",
      "11/07/2019 19:01:27 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 19:01:27 - INFO - __main__ -     Num examples = 2940\n",
      "11/07/2019 19:01:27 - INFO - __main__ -     Batch size = 64\n",
      "11/07/2019 19:03:38 - INFO - __main__ -     eval_F1 = 0.7829648863766\n",
      "11/07/2019 19:03:38 - INFO - __main__ -     eval_loss = 0.4124052105066569\n",
      "11/07/2019 19:03:38 - INFO - __main__ -     global_step = 5800\n",
      "11/07/2019 19:03:38 - INFO - __main__ -     loss = 0.1723\n",
      "================================================================================\n",
      "loss 0.1989:  80%|█████████████████▌    | 23999/30000 [1:50:40<18:20,  5.45it/s]11/07/2019 19:05:52 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 19:05:52 - INFO - __main__ -     global_step = 6000\n",
      "11/07/2019 19:05:52 - INFO - __main__ -     train loss = 0.1989\n",
      "11/07/2019 19:06:14 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 19:06:14 - INFO - __main__ -     Num examples = 2940\n",
      "11/07/2019 19:06:14 - INFO - __main__ -     Batch size = 64\n",
      "11/07/2019 19:08:26 - INFO - __main__ -     eval_F1 = 0.7839678879395565\n",
      "11/07/2019 19:08:26 - INFO - __main__ -     eval_loss = 0.3868840033107478\n",
      "11/07/2019 19:08:26 - INFO - __main__ -     global_step = 6000\n",
      "11/07/2019 19:08:26 - INFO - __main__ -     loss = 0.1989\n",
      "================================================================================\n",
      "loss 0.1257:  83%|██████████████████▏   | 24799/30000 [1:55:28<16:05,  5.39it/s]11/07/2019 19:10:41 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 19:10:41 - INFO - __main__ -     global_step = 6200\n",
      "11/07/2019 19:10:41 - INFO - __main__ -     train loss = 0.1257\n",
      "11/07/2019 19:11:02 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 19:11:02 - INFO - __main__ -     Num examples = 2940\n",
      "11/07/2019 19:11:02 - INFO - __main__ -     Batch size = 64\n",
      "11/07/2019 19:13:15 - INFO - __main__ -     eval_F1 = 0.789316536245801\n",
      "11/07/2019 19:13:15 - INFO - __main__ -     eval_loss = 0.40080178610008693\n",
      "11/07/2019 19:13:15 - INFO - __main__ -     global_step = 6200\n",
      "11/07/2019 19:13:15 - INFO - __main__ -     loss = 0.1257\n",
      "================================================================================\n",
      "loss 0.1421:  85%|██████████████████▊   | 25599/30000 [2:00:17<13:28,  5.45it/s]11/07/2019 19:15:30 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 19:15:30 - INFO - __main__ -     global_step = 6400\n",
      "11/07/2019 19:15:30 - INFO - __main__ -     train loss = 0.1421\n",
      "11/07/2019 19:15:51 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 19:15:51 - INFO - __main__ -     Num examples = 2940\n",
      "11/07/2019 19:15:51 - INFO - __main__ -     Batch size = 64\n",
      "11/07/2019 19:18:03 - INFO - __main__ -     eval_F1 = 0.7872482817602936\n",
      "11/07/2019 19:18:03 - INFO - __main__ -     eval_loss = 0.4149509936409152\n",
      "11/07/2019 19:18:03 - INFO - __main__ -     global_step = 6400\n",
      "11/07/2019 19:18:03 - INFO - __main__ -     loss = 0.1421\n",
      "================================================================================\n",
      "loss 0.1107:  88%|███████████████████▎  | 26399/30000 [2:05:05<10:58,  5.47it/s]11/07/2019 19:20:18 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 19:20:18 - INFO - __main__ -     global_step = 6600\n",
      "11/07/2019 19:20:18 - INFO - __main__ -     train loss = 0.1107\n",
      "11/07/2019 19:20:39 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 19:20:39 - INFO - __main__ -     Num examples = 2940\n",
      "11/07/2019 19:20:39 - INFO - __main__ -     Batch size = 64\n",
      "11/07/2019 19:22:51 - INFO - __main__ -     eval_F1 = 0.7952639531655548\n",
      "11/07/2019 19:22:51 - INFO - __main__ -     eval_loss = 0.4337169374136821\n",
      "11/07/2019 19:22:51 - INFO - __main__ -     global_step = 6600\n",
      "11/07/2019 19:22:51 - INFO - __main__ -     loss = 0.1107\n",
      "================================================================================\n",
      "loss 0.1274:  91%|███████████████████▉  | 27199/30000 [2:09:52<08:30,  5.49it/s]11/07/2019 19:25:05 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 19:25:05 - INFO - __main__ -     global_step = 6800\n",
      "11/07/2019 19:25:05 - INFO - __main__ -     train loss = 0.1274\n",
      "11/07/2019 19:25:27 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 19:25:27 - INFO - __main__ -     Num examples = 2940\n",
      "11/07/2019 19:25:27 - INFO - __main__ -     Batch size = 64\n",
      "11/07/2019 19:27:38 - INFO - __main__ -     eval_F1 = 0.7935136143277143\n",
      "11/07/2019 19:27:38 - INFO - __main__ -     eval_loss = 0.44069082460001757\n",
      "11/07/2019 19:27:38 - INFO - __main__ -     global_step = 6800\n",
      "11/07/2019 19:27:38 - INFO - __main__ -     loss = 0.1274\n",
      "================================================================================\n",
      "loss 0.1062:  93%|████████████████████▌ | 27999/30000 [2:14:38<06:05,  5.47it/s]11/07/2019 19:29:51 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 19:29:51 - INFO - __main__ -     global_step = 7000\n",
      "11/07/2019 19:29:51 - INFO - __main__ -     train loss = 0.1062\n",
      "11/07/2019 19:30:12 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 19:30:12 - INFO - __main__ -     Num examples = 2940\n",
      "11/07/2019 19:30:12 - INFO - __main__ -     Batch size = 64\n",
      "11/07/2019 19:32:23 - INFO - __main__ -     eval_F1 = 0.7879898144922582\n",
      "11/07/2019 19:32:23 - INFO - __main__ -     eval_loss = 0.45025672585419985\n",
      "11/07/2019 19:32:23 - INFO - __main__ -     global_step = 7000\n",
      "11/07/2019 19:32:23 - INFO - __main__ -     loss = 0.1062\n",
      "================================================================================\n",
      "loss 0.1296:  96%|█████████████████████ | 28799/30000 [2:19:24<03:41,  5.41it/s]11/07/2019 19:34:37 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 19:34:37 - INFO - __main__ -     global_step = 7200\n",
      "11/07/2019 19:34:37 - INFO - __main__ -     train loss = 0.1296\n",
      "11/07/2019 19:34:59 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 19:34:59 - INFO - __main__ -     Num examples = 2940\n",
      "11/07/2019 19:34:59 - INFO - __main__ -     Batch size = 64\n",
      "11/07/2019 19:37:10 - INFO - __main__ -     eval_F1 = 0.7906325256505969\n",
      "11/07/2019 19:37:10 - INFO - __main__ -     eval_loss = 0.44887472025078273\n",
      "11/07/2019 19:37:10 - INFO - __main__ -     global_step = 7200\n",
      "11/07/2019 19:37:10 - INFO - __main__ -     loss = 0.1296\n",
      "================================================================================\n",
      "loss 0.1422:  99%|█████████████████████▋| 29599/30000 [2:24:11<01:12,  5.50it/s]11/07/2019 19:39:24 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 19:39:24 - INFO - __main__ -     global_step = 7400\n",
      "11/07/2019 19:39:24 - INFO - __main__ -     train loss = 0.1422\n",
      "11/07/2019 19:39:45 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 19:39:45 - INFO - __main__ -     Num examples = 2940\n",
      "11/07/2019 19:39:45 - INFO - __main__ -     Batch size = 64\n",
      "11/07/2019 19:41:56 - INFO - __main__ -     eval_F1 = 0.7927592703504757\n",
      "11/07/2019 19:41:56 - INFO - __main__ -     eval_loss = 0.4454306521169517\n",
      "11/07/2019 19:41:56 - INFO - __main__ -     global_step = 7400\n",
      "11/07/2019 19:41:56 - INFO - __main__ -     loss = 0.1422\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.1114: 100%|██████████████████████| 30000/30000 [2:27:50<00:00,  5.97it/s]\n",
      "11/07/2019 19:43:03 - INFO - pytorch_transformers.modeling_utils -   loading weights file ./model/roberta_wwm_large_5121_42/roberta_wwm_large_5121_42_1/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "dev 0.80239286370484\n",
      "Traceback (most recent call last):\n",
      "  File \"../BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/run_bert_2562.py\", line 841, in <module>\n",
      "    main()\n",
      "  File \"../BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/run_bert_2562.py\", line 757, in main\n",
      "    logits = model(input_ids=input_ids, token_type_ids=segment_ids, attention_mask=input_mask).detach().cpu().numpy()\n",
      "  File \"/root/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 547, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/root/code/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 1017, in forward\n",
      "    attention_mask=flat_attention_mask, head_mask=head_mask)\n",
      "  File \"/root/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 547, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/root/code/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 727, in forward\n",
      "    head_mask=head_mask)\n",
      "  File \"/root/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 547, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/root/code/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 440, in forward\n",
      "    layer_outputs = layer_module(hidden_states, attention_mask, head_mask[i])\n",
      "  File \"/root/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 547, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/root/code/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 420, in forward\n",
      "    intermediate_output = self.intermediate(attention_output)\n",
      "  File \"/root/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 547, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/root/code/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 392, in forward\n",
      "    hidden_states = self.intermediate_act_fn(hidden_states)\n",
      "  File \"/root/code/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 142, in gelu\n",
      "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 10.73 GiB total capacity; 8.04 GiB already allocated; 495.62 MiB free; 1.29 GiB cached)\n"
     ]
    }
   ],
   "source": [
    "!python run_bert_2562.py \\\n",
    "--model_type bert \\\n",
    "--model_name_or_path ../model/chinese_roberta_wwm_large_ext_pytorch  \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_test \\\n",
    "--data_dir ../data/data_StratifiedKFold_42/data_origin_1 \\\n",
    "--output_dir ../model/roberta_wwm_large_5121_42/roberta_wwm_large_5121_42_1 \\\n",
    "--max_seq_length 512 \\\n",
    "--split_num 1 \\\n",
    "--lstm_hidden_size 512 \\\n",
    "--lstm_layers 1 \\\n",
    "--lstm_dropout 0.1 \\\n",
    "--eval_steps 200 \\\n",
    "--per_gpu_train_batch_size 4 \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--warmup_steps 0 \\\n",
    "--per_gpu_eval_batch_size 64 \\\n",
    "--learning_rate 1e-5 \\\n",
    "--adam_epsilon 1e-6 \\\n",
    "--weight_decay 0 \\\n",
    "--train_steps 30000 \\\n",
    "--freeze 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/07/2019 19:46:44 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "11/07/2019 19:46:44 - INFO - pytorch_transformers.tokenization_utils -   Model name '../BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/chinese_roberta_wwm_large_ext_pytorch' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming '../BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/chinese_roberta_wwm_large_ext_pytorch' is a path or url to a directory containing tokenizer files.\n",
      "11/07/2019 19:46:44 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/chinese_roberta_wwm_large_ext_pytorch/added_tokens.json. We won't load it.\n",
      "11/07/2019 19:46:44 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/chinese_roberta_wwm_large_ext_pytorch/special_tokens_map.json. We won't load it.\n",
      "11/07/2019 19:46:44 - INFO - pytorch_transformers.tokenization_utils -   loading file ../BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/chinese_roberta_wwm_large_ext_pytorch/vocab.txt\n",
      "11/07/2019 19:46:44 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/07/2019 19:46:44 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/07/2019 19:46:44 - INFO - pytorch_transformers.modeling_utils -   loading configuration file ../BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/chinese_roberta_wwm_large_ext_pytorch/config.json\n",
      "11/07/2019 19:46:44 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 3,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "11/07/2019 19:46:44 - INFO - pytorch_transformers.modeling_utils -   loading weights file ../BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/chinese_roberta_wwm_large_ext_pytorch/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "11/07/2019 19:46:59 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'W.0.weight', 'W.0.bias', 'gru.0.weight_ih_l0', 'gru.0.weight_hh_l0', 'gru.0.bias_ih_l0', 'gru.0.bias_hh_l0', 'gru.0.weight_ih_l0_reverse', 'gru.0.weight_hh_l0_reverse', 'gru.0.bias_ih_l0_reverse', 'gru.0.bias_hh_l0_reverse']\n",
      "11/07/2019 19:46:59 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "11/07/2019 19:47:00 - INFO - __main__ -   *** Example ***\n",
      "11/07/2019 19:47:00 - INFO - __main__ -   idx: 0\n",
      "11/07/2019 19:47:00 - INFO - __main__ -   guid: 7a3dd79f90ee419da87190cff60f7a86\n",
      "11/07/2019 19:47:00 - INFO - __main__ -   tokens: [CLS] 问 责 领 导 ( 上 黄 镇 党 委 书 记 张 涛 ， 宣 国 才 真 能 一 手 遮 天 吗 ？ ) [SEP] 这 几 天 看 了 有 人 举 报 施 某 某 的 贴 子 ， 经 与 举 报 人 联 系 证 实 ， 是 宣 某 当 天 中 午 请 举 报 人 和 枪 手 喝 酒 后 ， 晚 上 才 发 的 贴 子 ！ 本 人 不 去 讨 论 前 二 天 的 举 报 ， 相 信 总 归 会 有 说 法 的 ！ 今 天 一 看 施 全 军 2017 年 1 月 2 日 实 名 举 报 上 黄 镇 宣 国 才 的 贴 子 （ 仍 被 锁 定 禁 止 评 论 ） 已 经 正 好 一 整 年 了 = 750 ) window . open ( ' http : / / img . js ##ly ##001 . com / at ##ta ##ch ##ment / mon _ 180 ##1 / 4 _ 291 ##08 ##5 _ c ##79 ##6 ##a ##6 ##a ##86 ##e ##17 ##12 ##1 . jpg ? 123 ' ) ; \" on ##load = \" if ( this . off ##set ##wi ##dt ##h > ' 750 ' ) this . wi ##dt ##h = ' 750 ' ; \" sr ##c = \" http : / / img . js ##ly ##001 . com / at ##ta ##ch ##ment / mon _ 180 ##1 / 4 _ 291 ##08 ##5 _ c ##79 ##6 ##a ##6 ##a ##86 ##e ##17 ##12 ##1 . jpg ? 123 \" style = \" max - wi ##dt ##h : 750 ##px ; \" / > 图 片 : / home / al ##ida ##ta / www / data / tm ##p / q ##fu ##pl ##oa ##d / 4 _ 291 ##08 ##5 _ 151 ##49 ##81 ##47 ##14 ##78 ##95 ##2 . jpg 施 全 军 实 名 举 报 50 天 后 ， 上 黄 镇 党 委 政 府 回 复 如 下 图 ： = 750 ) window . open ( ' http : / / img . js ##ly ##001 . com / at ##ta ##ch ##ment / mon _ 180 ##1 / 4 _ 291 ##08 ##5 _ a9 ##b ##11 ##b ##7 ##ea ##2 ##b ##1 ##ce ##9 . jpg ? 90 ' ) ; \" on ##load = \" if ( this . off ##set ##wi ##dt ##h > ' 750 ' ) this . wi ##dt ##h = ' 750 ' ; \" sr ##c = \" http : / / img . js ##ly ##001 . com / at ##ta ##ch ##ment / mon _ 180 ##1 / 4 _ 291 ##08 ##5 _ a9 ##b ##11 ##b ##7 ##ea ##2 ##b ##1 ##ce ##9 . jpg ? 90 \" style = \" max - wi ##dt ##h : 750 ##px ; \" / > 图 片 : / home / [SEP]\n",
      "11/07/2019 19:47:00 - INFO - __main__ -   input_ids: 101 7309 6569 7566 2193 113 677 7942 7252 1054 1999 741 6381 2476 3875 8024 2146 1744 2798 4696 5543 671 2797 6902 1921 1408 8043 114 102 6821 1126 1921 4692 749 3300 782 715 2845 3177 3378 3378 4638 6585 2094 8024 5307 680 715 2845 782 5468 5143 6395 2141 8024 3221 2146 3378 2496 1921 704 1286 6435 715 2845 782 1469 3366 2797 1600 6983 1400 8024 3241 677 2798 1355 4638 6585 2094 8013 3315 782 679 1343 6374 6389 1184 753 1921 4638 715 2845 8024 4685 928 2600 2495 833 3300 6432 3791 4638 8013 791 1921 671 4692 3177 1059 1092 8109 2399 122 3299 123 3189 2141 1399 715 2845 677 7942 7252 2146 1744 2798 4638 6585 2094 8020 793 6158 7219 2137 4881 3632 6397 6389 8021 2347 5307 3633 1962 671 3146 2399 749 134 9180 114 12158 119 8893 113 112 8184 131 120 120 11412 119 9016 8436 9942 119 8134 120 8243 8383 8370 8631 120 8556 142 8420 8148 120 125 142 11777 9153 8157 142 145 9495 8158 8139 8158 8139 9219 8154 8408 8455 8148 119 9248 136 8604 112 114 132 107 8281 11713 134 107 8898 113 8554 119 9594 9852 10958 12672 8199 135 112 9180 112 114 8554 119 8541 12672 8199 134 112 9180 112 132 107 12109 8177 134 107 8184 131 120 120 11412 119 9016 8436 9942 119 8134 120 8243 8383 8370 8631 120 8556 142 8420 8148 120 125 142 11777 9153 8157 142 145 9495 8158 8139 8158 8139 9219 8154 8408 8455 8148 119 9248 136 8604 107 8969 134 107 8621 118 8541 12672 8199 131 9180 10605 132 107 120 135 1745 4275 131 120 8563 120 9266 12708 8383 120 8173 120 9000 120 9908 8187 120 159 12043 12569 11355 8168 120 125 142 11777 9153 8157 142 9564 9500 9313 9050 8717 9136 9102 8144 119 9248 3177 1059 1092 2141 1399 715 2845 8145 1921 1400 8024 677 7942 7252 1054 1999 3124 2424 1726 1908 1963 678 1745 8038 134 9180 114 12158 119 8893 113 112 8184 131 120 120 11412 119 9016 8436 9942 119 8134 120 8243 8383 8370 8631 120 8556 142 8420 8148 120 125 142 11777 9153 8157 142 11094 8204 8452 8204 8161 10073 8144 8204 8148 8328 8160 119 9248 136 8192 112 114 132 107 8281 11713 134 107 8898 113 8554 119 9594 9852 10958 12672 8199 135 112 9180 112 114 8554 119 8541 12672 8199 134 112 9180 112 132 107 12109 8177 134 107 8184 131 120 120 11412 119 9016 8436 9942 119 8134 120 8243 8383 8370 8631 120 8556 142 8420 8148 120 125 142 11777 9153 8157 142 11094 8204 8452 8204 8161 10073 8144 8204 8148 8328 8160 119 9248 136 8192 107 8969 134 107 8621 118 8541 12672 8199 131 9180 10605 132 107 120 135 1745 4275 131 120 8563 120 102\n",
      "11/07/2019 19:47:00 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/07/2019 19:47:00 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/07/2019 19:47:00 - INFO - __main__ -   label: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/07/2019 19:48:31 - INFO - __main__ -   ***** Running training *****\n",
      "11/07/2019 19:48:31 - INFO - __main__ -     Num examples = 11757\n",
      "11/07/2019 19:48:31 - INFO - __main__ -     Batch size = 4\n",
      "11/07/2019 19:48:31 - INFO - __main__ -     Num steps = 30000\n",
      "  0%|                                                 | 0/30000 [00:00<?, ?it/s]11/07/2019 19:48:53 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 19:48:53 - INFO - __main__ -     Num examples = 2939\n",
      "11/07/2019 19:48:53 - INFO - __main__ -     Batch size = 64\n",
      "/root/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "11/07/2019 19:51:02 - INFO - __main__ -     eval_F1 = 0.062493575907081926\n",
      "11/07/2019 19:51:02 - INFO - __main__ -     eval_loss = 1.1599747668141904\n",
      "11/07/2019 19:51:02 - INFO - __main__ -     global_step = 0\n",
      "================================================================================\n",
      "Best F1 0.062493575907081926\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.7095:   3%|▋                       | 799/30000 [04:43<1:28:30,  5.50it/s]11/07/2019 19:53:15 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 19:53:15 - INFO - __main__ -     global_step = 200\n",
      "11/07/2019 19:53:15 - INFO - __main__ -     train loss = 0.7095\n",
      "loss 0.5001:   5%|█▏                     | 1599/30000 [06:55<1:25:55,  5.51it/s]11/07/2019 19:55:27 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 19:55:27 - INFO - __main__ -     global_step = 400\n",
      "11/07/2019 19:55:27 - INFO - __main__ -     train loss = 0.5001\n",
      "loss 0.4415:   8%|█▊                     | 2399/30000 [09:09<1:22:56,  5.55it/s]11/07/2019 19:57:40 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 19:57:40 - INFO - __main__ -     global_step = 600\n",
      "11/07/2019 19:57:40 - INFO - __main__ -     train loss = 0.4415\n",
      "loss 0.4504:  11%|██▍                    | 3199/30000 [11:22<1:27:32,  5.10it/s]11/07/2019 19:59:53 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 19:59:53 - INFO - __main__ -     global_step = 800\n",
      "11/07/2019 19:59:53 - INFO - __main__ -     train loss = 0.4504\n",
      "loss 0.3902:  13%|███                    | 3999/30000 [13:36<1:21:00,  5.35it/s]11/07/2019 20:02:07 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 20:02:07 - INFO - __main__ -     global_step = 1000\n",
      "11/07/2019 20:02:07 - INFO - __main__ -     train loss = 0.3902\n",
      "loss 0.4184:  16%|███▋                   | 4799/30000 [15:49<1:17:14,  5.44it/s]11/07/2019 20:04:21 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 20:04:21 - INFO - __main__ -     global_step = 1200\n",
      "11/07/2019 20:04:21 - INFO - __main__ -     train loss = 0.4184\n",
      "loss 0.4138:  19%|████▎                  | 5599/30000 [18:04<1:14:19,  5.47it/s]11/07/2019 20:06:35 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 20:06:35 - INFO - __main__ -     global_step = 1400\n",
      "11/07/2019 20:06:35 - INFO - __main__ -     train loss = 0.4138\n",
      "loss 0.3665:  21%|████▉                  | 6399/30000 [20:17<1:12:04,  5.46it/s]11/07/2019 20:08:48 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 20:08:48 - INFO - __main__ -     global_step = 1600\n",
      "11/07/2019 20:08:48 - INFO - __main__ -     train loss = 0.3665\n",
      "loss 0.3835:  24%|█████▌                 | 7199/30000 [22:32<1:09:43,  5.45it/s]11/07/2019 20:11:03 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 20:11:03 - INFO - __main__ -     global_step = 1800\n",
      "11/07/2019 20:11:03 - INFO - __main__ -     train loss = 0.3835\n",
      "loss 0.3621:  27%|██████▏                | 7999/30000 [24:46<1:06:49,  5.49it/s]11/07/2019 20:13:17 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 20:13:17 - INFO - __main__ -     global_step = 2000\n",
      "11/07/2019 20:13:17 - INFO - __main__ -     train loss = 0.3621\n",
      "loss 0.4545:  29%|██████▋                | 8799/30000 [27:00<1:04:58,  5.44it/s]11/07/2019 20:15:31 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 20:15:31 - INFO - __main__ -     global_step = 2200\n",
      "11/07/2019 20:15:31 - INFO - __main__ -     train loss = 0.4545\n",
      "loss 0.3619:  32%|███████▎               | 9599/30000 [29:14<1:02:14,  5.46it/s]11/07/2019 20:17:45 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 20:17:45 - INFO - __main__ -     global_step = 2400\n",
      "11/07/2019 20:17:45 - INFO - __main__ -     train loss = 0.3619\n",
      "loss 0.4066:  35%|████████▎               | 10399/30000 [31:27<59:18,  5.51it/s]11/07/2019 20:19:58 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 20:19:58 - INFO - __main__ -     global_step = 2600\n",
      "11/07/2019 20:19:58 - INFO - __main__ -     train loss = 0.4066\n",
      "loss 0.3548:  37%|████████▉               | 11199/30000 [33:40<56:34,  5.54it/s]11/07/2019 20:22:11 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 20:22:11 - INFO - __main__ -     global_step = 2800\n",
      "11/07/2019 20:22:11 - INFO - __main__ -     train loss = 0.3548\n",
      "11/07/2019 20:22:33 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 20:22:33 - INFO - __main__ -     Num examples = 2939\n",
      "11/07/2019 20:22:33 - INFO - __main__ -     Batch size = 64\n",
      "11/07/2019 20:24:44 - INFO - __main__ -     eval_F1 = 0.7509738338432091\n",
      "11/07/2019 20:24:44 - INFO - __main__ -     eval_loss = 0.3583116918478323\n",
      "11/07/2019 20:24:44 - INFO - __main__ -     global_step = 2800\n",
      "11/07/2019 20:24:44 - INFO - __main__ -     loss = 0.3548\n",
      "================================================================================\n",
      "Best F1 0.7509738338432091\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.3297:  40%|█████████▌              | 11999/30000 [38:30<54:45,  5.48it/s]11/07/2019 20:27:01 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 20:27:01 - INFO - __main__ -     global_step = 3000\n",
      "11/07/2019 20:27:01 - INFO - __main__ -     train loss = 0.3297\n",
      "11/07/2019 20:27:24 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 20:27:24 - INFO - __main__ -     Num examples = 2939\n",
      "11/07/2019 20:27:24 - INFO - __main__ -     Batch size = 64\n",
      "11/07/2019 20:29:36 - INFO - __main__ -     eval_F1 = 0.7834839088111519\n",
      "11/07/2019 20:29:36 - INFO - __main__ -     eval_loss = 0.3575142795301002\n",
      "11/07/2019 20:29:36 - INFO - __main__ -     global_step = 3000\n",
      "11/07/2019 20:29:36 - INFO - __main__ -     loss = 0.3297\n",
      "================================================================================\n",
      "Best F1 0.7834839088111519\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.3076:  43%|██████████▏             | 12799/30000 [43:22<53:09,  5.39it/s]11/07/2019 20:31:53 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 20:31:53 - INFO - __main__ -     global_step = 3200\n",
      "11/07/2019 20:31:53 - INFO - __main__ -     train loss = 0.3076\n",
      "11/07/2019 20:32:16 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 20:32:16 - INFO - __main__ -     Num examples = 2939\n",
      "11/07/2019 20:32:16 - INFO - __main__ -     Batch size = 64\n",
      "11/07/2019 20:34:28 - INFO - __main__ -     eval_F1 = 0.7697889057926445\n",
      "11/07/2019 20:34:28 - INFO - __main__ -     eval_loss = 0.38171163476679637\n",
      "11/07/2019 20:34:28 - INFO - __main__ -     global_step = 3200\n",
      "11/07/2019 20:34:28 - INFO - __main__ -     loss = 0.3076\n",
      "================================================================================\n",
      "loss 0.2765:  45%|██████████▉             | 13599/30000 [48:11<50:23,  5.42it/s]11/07/2019 20:36:42 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 20:36:42 - INFO - __main__ -     global_step = 3400\n",
      "11/07/2019 20:36:42 - INFO - __main__ -     train loss = 0.2765\n",
      "11/07/2019 20:37:05 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 20:37:05 - INFO - __main__ -     Num examples = 2939\n",
      "11/07/2019 20:37:05 - INFO - __main__ -     Batch size = 64\n",
      "11/07/2019 20:39:17 - INFO - __main__ -     eval_F1 = 0.7880611406069976\n",
      "11/07/2019 20:39:17 - INFO - __main__ -     eval_loss = 0.3766916919013728\n",
      "11/07/2019 20:39:17 - INFO - __main__ -     global_step = 3400\n",
      "11/07/2019 20:39:17 - INFO - __main__ -     loss = 0.2765\n",
      "================================================================================\n",
      "Best F1 0.7880611406069976\n",
      "Saving Model......\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "loss 0.2606:  48%|███████████▌            | 14399/30000 [53:04<47:26,  5.48it/s]11/07/2019 20:41:35 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 20:41:35 - INFO - __main__ -     global_step = 3600\n",
      "11/07/2019 20:41:35 - INFO - __main__ -     train loss = 0.2606\n",
      "11/07/2019 20:41:58 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 20:41:58 - INFO - __main__ -     Num examples = 2939\n",
      "11/07/2019 20:41:58 - INFO - __main__ -     Batch size = 64\n",
      "11/07/2019 20:44:10 - INFO - __main__ -     eval_F1 = 0.794929378070771\n",
      "11/07/2019 20:44:10 - INFO - __main__ -     eval_loss = 0.3808179170541141\n",
      "11/07/2019 20:44:10 - INFO - __main__ -     global_step = 3600\n",
      "11/07/2019 20:44:10 - INFO - __main__ -     loss = 0.2606\n",
      "================================================================================\n",
      "Best F1 0.794929378070771\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.2627:  51%|████████████▏           | 15199/30000 [57:56<45:32,  5.42it/s]11/07/2019 20:46:27 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 20:46:27 - INFO - __main__ -     global_step = 3800\n",
      "11/07/2019 20:46:27 - INFO - __main__ -     train loss = 0.2627\n",
      "11/07/2019 20:46:50 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 20:46:50 - INFO - __main__ -     Num examples = 2939\n",
      "11/07/2019 20:46:50 - INFO - __main__ -     Batch size = 64\n",
      "11/07/2019 20:49:02 - INFO - __main__ -     eval_F1 = 0.7898147848272538\n",
      "11/07/2019 20:49:02 - INFO - __main__ -     eval_loss = 0.38988770552627416\n",
      "11/07/2019 20:49:02 - INFO - __main__ -     global_step = 3800\n",
      "11/07/2019 20:49:02 - INFO - __main__ -     loss = 0.2627\n",
      "================================================================================\n",
      "loss 0.197:  53%|████████████▎          | 15999/30000 [1:02:45<42:58,  5.43it/s]11/07/2019 20:51:17 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 20:51:17 - INFO - __main__ -     global_step = 4000\n",
      "11/07/2019 20:51:17 - INFO - __main__ -     train loss = 0.197\n",
      "11/07/2019 20:51:39 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 20:51:39 - INFO - __main__ -     Num examples = 2939\n",
      "11/07/2019 20:51:39 - INFO - __main__ -     Batch size = 64\n",
      "11/07/2019 20:53:51 - INFO - __main__ -     eval_F1 = 0.7841753325245687\n",
      "11/07/2019 20:53:51 - INFO - __main__ -     eval_loss = 0.4061268182552379\n",
      "11/07/2019 20:53:51 - INFO - __main__ -     global_step = 4000\n",
      "11/07/2019 20:53:51 - INFO - __main__ -     loss = 0.197\n",
      "================================================================================\n",
      "loss 0.234:  56%|████████████▉          | 16799/30000 [1:07:33<40:17,  5.46it/s]11/07/2019 20:56:04 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 20:56:04 - INFO - __main__ -     global_step = 4200\n",
      "11/07/2019 20:56:04 - INFO - __main__ -     train loss = 0.234\n",
      "11/07/2019 20:56:27 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 20:56:27 - INFO - __main__ -     Num examples = 2939\n",
      "11/07/2019 20:56:27 - INFO - __main__ -     Batch size = 64\n",
      "11/07/2019 20:58:38 - INFO - __main__ -     eval_F1 = 0.773024744681003\n",
      "11/07/2019 20:58:38 - INFO - __main__ -     eval_loss = 0.4075007096904775\n",
      "11/07/2019 20:58:38 - INFO - __main__ -     global_step = 4200\n",
      "11/07/2019 20:58:38 - INFO - __main__ -     loss = 0.234\n",
      "================================================================================\n",
      "loss 0.256:  59%|█████████████▍         | 17599/30000 [1:12:20<37:21,  5.53it/s]11/07/2019 21:00:51 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 21:00:51 - INFO - __main__ -     global_step = 4400\n",
      "11/07/2019 21:00:51 - INFO - __main__ -     train loss = 0.256\n",
      "11/07/2019 21:01:14 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 21:01:14 - INFO - __main__ -     Num examples = 2939\n",
      "11/07/2019 21:01:14 - INFO - __main__ -     Batch size = 64\n",
      "11/07/2019 21:03:24 - INFO - __main__ -     eval_F1 = 0.785835547369529\n",
      "11/07/2019 21:03:24 - INFO - __main__ -     eval_loss = 0.3864024777127349\n",
      "11/07/2019 21:03:24 - INFO - __main__ -     global_step = 4400\n",
      "11/07/2019 21:03:24 - INFO - __main__ -     loss = 0.256\n",
      "================================================================================\n",
      "loss 0.2156:  61%|█████████████▍        | 18399/30000 [1:17:06<35:06,  5.51it/s]11/07/2019 21:05:37 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 21:05:37 - INFO - __main__ -     global_step = 4600\n",
      "11/07/2019 21:05:37 - INFO - __main__ -     train loss = 0.2156\n",
      "11/07/2019 21:05:59 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 21:05:59 - INFO - __main__ -     Num examples = 2939\n",
      "11/07/2019 21:05:59 - INFO - __main__ -     Batch size = 64\n",
      "11/07/2019 21:08:10 - INFO - __main__ -     eval_F1 = 0.7984504018382016\n",
      "11/07/2019 21:08:10 - INFO - __main__ -     eval_loss = 0.3812704492846261\n",
      "11/07/2019 21:08:10 - INFO - __main__ -     global_step = 4600\n",
      "11/07/2019 21:08:10 - INFO - __main__ -     loss = 0.2156\n",
      "================================================================================\n",
      "Best F1 0.7984504018382016\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.2024:  64%|██████████████        | 19199/30000 [1:21:57<33:07,  5.44it/s]11/07/2019 21:10:28 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 21:10:28 - INFO - __main__ -     global_step = 4800\n",
      "11/07/2019 21:10:28 - INFO - __main__ -     train loss = 0.2024\n",
      "11/07/2019 21:10:51 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 21:10:51 - INFO - __main__ -     Num examples = 2939\n",
      "11/07/2019 21:10:51 - INFO - __main__ -     Batch size = 64\n",
      "11/07/2019 21:13:03 - INFO - __main__ -     eval_F1 = 0.7967091801478586\n",
      "11/07/2019 21:13:03 - INFO - __main__ -     eval_loss = 0.37775882268729416\n",
      "11/07/2019 21:13:03 - INFO - __main__ -     global_step = 4800\n",
      "11/07/2019 21:13:03 - INFO - __main__ -     loss = 0.2024\n",
      "================================================================================\n",
      "loss 0.2176:  67%|██████████████▋       | 19999/30000 [1:26:46<30:32,  5.46it/s]11/07/2019 21:15:17 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 21:15:17 - INFO - __main__ -     global_step = 5000\n",
      "11/07/2019 21:15:17 - INFO - __main__ -     train loss = 0.2176\n",
      "11/07/2019 21:15:43 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 21:15:43 - INFO - __main__ -     Num examples = 2939\n",
      "11/07/2019 21:15:43 - INFO - __main__ -     Batch size = 64\n",
      "11/07/2019 21:17:54 - INFO - __main__ -     eval_F1 = 0.7964309242421278\n",
      "11/07/2019 21:17:54 - INFO - __main__ -     eval_loss = 0.3848260997430138\n",
      "11/07/2019 21:17:54 - INFO - __main__ -     global_step = 5000\n",
      "11/07/2019 21:17:54 - INFO - __main__ -     loss = 0.2176\n",
      "================================================================================\n",
      "loss 0.2673:  69%|███████████████▎      | 20799/30000 [1:31:37<28:14,  5.43it/s]11/07/2019 21:20:09 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 21:20:09 - INFO - __main__ -     global_step = 5200\n",
      "11/07/2019 21:20:09 - INFO - __main__ -     train loss = 0.2673\n",
      "11/07/2019 21:20:32 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 21:20:32 - INFO - __main__ -     Num examples = 2939\n",
      "11/07/2019 21:20:32 - INFO - __main__ -     Batch size = 64\n",
      "11/07/2019 21:22:44 - INFO - __main__ -     eval_F1 = 0.8027552880691396\n",
      "11/07/2019 21:22:44 - INFO - __main__ -     eval_loss = 0.3799340004992226\n",
      "11/07/2019 21:22:44 - INFO - __main__ -     global_step = 5200\n",
      "11/07/2019 21:22:44 - INFO - __main__ -     loss = 0.2673\n",
      "================================================================================\n",
      "Best F1 0.8027552880691396\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.2445:  72%|███████████████▊      | 21599/30000 [1:36:31<25:32,  5.48it/s]11/07/2019 21:25:02 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 21:25:02 - INFO - __main__ -     global_step = 5400\n",
      "11/07/2019 21:25:02 - INFO - __main__ -     train loss = 0.2445\n",
      "11/07/2019 21:25:26 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 21:25:26 - INFO - __main__ -     Num examples = 2939\n",
      "11/07/2019 21:25:26 - INFO - __main__ -     Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/07/2019 21:27:37 - INFO - __main__ -     eval_F1 = 0.8022152671256985\n",
      "11/07/2019 21:27:37 - INFO - __main__ -     eval_loss = 0.35474243646730547\n",
      "11/07/2019 21:27:37 - INFO - __main__ -     global_step = 5400\n",
      "11/07/2019 21:27:37 - INFO - __main__ -     loss = 0.2445\n",
      "================================================================================\n",
      "loss 0.2266:  75%|████████████████▍     | 22399/30000 [1:41:21<23:22,  5.42it/s]11/07/2019 21:29:52 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 21:29:52 - INFO - __main__ -     global_step = 5600\n",
      "11/07/2019 21:29:52 - INFO - __main__ -     train loss = 0.2266\n",
      "11/07/2019 21:30:15 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 21:30:15 - INFO - __main__ -     Num examples = 2939\n",
      "11/07/2019 21:30:15 - INFO - __main__ -     Batch size = 64\n",
      "11/07/2019 21:32:27 - INFO - __main__ -     eval_F1 = 0.8042809184065279\n",
      "11/07/2019 21:32:27 - INFO - __main__ -     eval_loss = 0.39221857180413994\n",
      "11/07/2019 21:32:27 - INFO - __main__ -     global_step = 5600\n",
      "11/07/2019 21:32:27 - INFO - __main__ -     loss = 0.2266\n",
      "================================================================================\n",
      "Best F1 0.8042809184065279\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.2181:  77%|█████████████████     | 23199/30000 [1:46:15<21:33,  5.26it/s]11/07/2019 21:34:46 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 21:34:46 - INFO - __main__ -     global_step = 5800\n",
      "11/07/2019 21:34:46 - INFO - __main__ -     train loss = 0.2181\n",
      "11/07/2019 21:35:09 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 21:35:09 - INFO - __main__ -     Num examples = 2939\n",
      "11/07/2019 21:35:09 - INFO - __main__ -     Batch size = 64\n",
      "11/07/2019 21:37:21 - INFO - __main__ -     eval_F1 = 0.796615888141854\n",
      "11/07/2019 21:37:21 - INFO - __main__ -     eval_loss = 0.36256481690899184\n",
      "11/07/2019 21:37:21 - INFO - __main__ -     global_step = 5800\n",
      "11/07/2019 21:37:21 - INFO - __main__ -     loss = 0.2181\n",
      "================================================================================\n",
      "loss 0.2096:  80%|█████████████████▌    | 23999/30000 [1:51:04<18:24,  5.44it/s]11/07/2019 21:39:35 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 21:39:35 - INFO - __main__ -     global_step = 6000\n",
      "11/07/2019 21:39:35 - INFO - __main__ -     train loss = 0.2096\n",
      "11/07/2019 21:39:57 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 21:39:57 - INFO - __main__ -     Num examples = 2939\n",
      "11/07/2019 21:39:57 - INFO - __main__ -     Batch size = 64\n",
      "11/07/2019 21:42:09 - INFO - __main__ -     eval_F1 = 0.8047393293119137\n",
      "11/07/2019 21:42:09 - INFO - __main__ -     eval_loss = 0.36918030007053976\n",
      "11/07/2019 21:42:09 - INFO - __main__ -     global_step = 6000\n",
      "11/07/2019 21:42:09 - INFO - __main__ -     loss = 0.2096\n",
      "================================================================================\n",
      "Best F1 0.8047393293119137\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.2085:  83%|██████████████████▏   | 24799/30000 [1:55:57<17:04,  5.08it/s]11/07/2019 21:44:29 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 21:44:29 - INFO - __main__ -     global_step = 6200\n",
      "11/07/2019 21:44:29 - INFO - __main__ -     train loss = 0.2085\n",
      "11/07/2019 21:44:51 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 21:44:51 - INFO - __main__ -     Num examples = 2939\n",
      "11/07/2019 21:44:51 - INFO - __main__ -     Batch size = 64\n",
      "11/07/2019 21:47:03 - INFO - __main__ -     eval_F1 = 0.7919173752507085\n",
      "11/07/2019 21:47:03 - INFO - __main__ -     eval_loss = 0.38666767865905294\n",
      "11/07/2019 21:47:03 - INFO - __main__ -     global_step = 6200\n",
      "11/07/2019 21:47:03 - INFO - __main__ -     loss = 0.2085\n",
      "================================================================================\n",
      "loss 0.1479:  85%|██████████████████▊   | 25599/30000 [2:00:46<13:31,  5.42it/s]11/07/2019 21:49:17 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 21:49:17 - INFO - __main__ -     global_step = 6400\n",
      "11/07/2019 21:49:17 - INFO - __main__ -     train loss = 0.1479\n",
      "11/07/2019 21:49:40 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 21:49:40 - INFO - __main__ -     Num examples = 2939\n",
      "11/07/2019 21:49:40 - INFO - __main__ -     Batch size = 64\n",
      "11/07/2019 21:51:52 - INFO - __main__ -     eval_F1 = 0.8036900138936937\n",
      "11/07/2019 21:51:52 - INFO - __main__ -     eval_loss = 0.3752755907404682\n",
      "11/07/2019 21:51:52 - INFO - __main__ -     global_step = 6400\n",
      "11/07/2019 21:51:52 - INFO - __main__ -     loss = 0.1479\n",
      "================================================================================\n",
      "loss 0.1511:  88%|███████████████████▎  | 26399/30000 [2:05:35<11:03,  5.43it/s]11/07/2019 21:54:06 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 21:54:06 - INFO - __main__ -     global_step = 6600\n",
      "11/07/2019 21:54:06 - INFO - __main__ -     train loss = 0.1511\n",
      "11/07/2019 21:54:29 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 21:54:29 - INFO - __main__ -     Num examples = 2939\n",
      "11/07/2019 21:54:29 - INFO - __main__ -     Batch size = 64\n",
      "11/07/2019 21:56:41 - INFO - __main__ -     eval_F1 = 0.8058172511623906\n",
      "11/07/2019 21:56:41 - INFO - __main__ -     eval_loss = 0.3864420539011126\n",
      "11/07/2019 21:56:41 - INFO - __main__ -     global_step = 6600\n",
      "11/07/2019 21:56:41 - INFO - __main__ -     loss = 0.1511\n",
      "================================================================================\n",
      "Best F1 0.8058172511623906\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.1375:  91%|███████████████████▉  | 27199/30000 [2:10:27<08:35,  5.43it/s]11/07/2019 21:58:59 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 21:58:59 - INFO - __main__ -     global_step = 6800\n",
      "11/07/2019 21:58:59 - INFO - __main__ -     train loss = 0.1375\n",
      "11/07/2019 21:59:21 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 21:59:21 - INFO - __main__ -     Num examples = 2939\n",
      "11/07/2019 21:59:21 - INFO - __main__ -     Batch size = 64\n",
      "11/07/2019 22:01:33 - INFO - __main__ -     eval_F1 = 0.8021719228329678\n",
      "11/07/2019 22:01:33 - INFO - __main__ -     eval_loss = 0.40507435895826505\n",
      "11/07/2019 22:01:33 - INFO - __main__ -     global_step = 6800\n",
      "11/07/2019 22:01:33 - INFO - __main__ -     loss = 0.1375\n",
      "================================================================================\n",
      "loss 0.1028:  93%|████████████████████▌ | 27999/30000 [2:15:17<06:09,  5.41it/s]11/07/2019 22:03:48 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 22:03:48 - INFO - __main__ -     global_step = 7000\n",
      "11/07/2019 22:03:48 - INFO - __main__ -     train loss = 0.1028\n",
      "11/07/2019 22:04:11 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 22:04:11 - INFO - __main__ -     Num examples = 2939\n",
      "11/07/2019 22:04:11 - INFO - __main__ -     Batch size = 64\n",
      "11/07/2019 22:06:23 - INFO - __main__ -     eval_F1 = 0.8010069832514203\n",
      "11/07/2019 22:06:23 - INFO - __main__ -     eval_loss = 0.43469429728777514\n",
      "11/07/2019 22:06:23 - INFO - __main__ -     global_step = 7000\n",
      "11/07/2019 22:06:23 - INFO - __main__ -     loss = 0.1028\n",
      "================================================================================\n",
      "loss 0.1267:  96%|█████████████████████ | 28799/30000 [2:20:06<03:41,  5.42it/s]11/07/2019 22:08:37 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 22:08:37 - INFO - __main__ -     global_step = 7200\n",
      "11/07/2019 22:08:37 - INFO - __main__ -     train loss = 0.1267\n",
      "11/07/2019 22:09:00 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 22:09:00 - INFO - __main__ -     Num examples = 2939\n",
      "11/07/2019 22:09:00 - INFO - __main__ -     Batch size = 64\n",
      "11/07/2019 22:11:11 - INFO - __main__ -     eval_F1 = 0.80431223751234\n",
      "11/07/2019 22:11:11 - INFO - __main__ -     eval_loss = 0.42502142472759535\n",
      "11/07/2019 22:11:11 - INFO - __main__ -     global_step = 7200\n",
      "11/07/2019 22:11:11 - INFO - __main__ -     loss = 0.1267\n",
      "================================================================================\n",
      "loss 0.1433:  99%|█████████████████████▋| 29599/30000 [2:24:54<01:13,  5.44it/s]11/07/2019 22:13:25 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 22:13:25 - INFO - __main__ -     global_step = 7400\n",
      "11/07/2019 22:13:25 - INFO - __main__ -     train loss = 0.1433\n",
      "11/07/2019 22:13:48 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 22:13:48 - INFO - __main__ -     Num examples = 2939\n",
      "11/07/2019 22:13:48 - INFO - __main__ -     Batch size = 64\n",
      "11/07/2019 22:16:00 - INFO - __main__ -     eval_F1 = 0.7985451992952219\n",
      "11/07/2019 22:16:00 - INFO - __main__ -     eval_loss = 0.420814475449531\n",
      "11/07/2019 22:16:00 - INFO - __main__ -     global_step = 7400\n",
      "11/07/2019 22:16:00 - INFO - __main__ -     loss = 0.1433\n",
      "================================================================================\n",
      "loss 0.109: 100%|███████████████████████| 30000/30000 [2:28:36<00:00,  6.07it/s]\n",
      "11/07/2019 22:17:08 - INFO - pytorch_transformers.modeling_utils -   loading weights file ./model/roberta_wwm_large_5121_42/roberta_wwm_large_5121_42_2/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "dev 0.8058172511623906\n",
      "Traceback (most recent call last):\n",
      "  File \"../BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/run_bert_2562.py\", line 841, in <module>\n",
      "    main()\n",
      "  File \"../BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/run_bert_2562.py\", line 757, in main\n",
      "    logits = model(input_ids=input_ids, token_type_ids=segment_ids, attention_mask=input_mask).detach().cpu().numpy()\n",
      "  File \"/root/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 547, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/root/code/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 1017, in forward\n",
      "    attention_mask=flat_attention_mask, head_mask=head_mask)\n",
      "  File \"/root/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 547, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/root/code/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 727, in forward\n",
      "    head_mask=head_mask)\n",
      "  File \"/root/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 547, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/root/code/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 440, in forward\n",
      "    layer_outputs = layer_module(hidden_states, attention_mask, head_mask[i])\n",
      "  File \"/root/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 547, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/root/code/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 420, in forward\n",
      "    intermediate_output = self.intermediate(attention_output)\n",
      "  File \"/root/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 547, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/root/code/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 392, in forward\n",
      "    hidden_states = self.intermediate_act_fn(hidden_states)\n",
      "  File \"/root/code/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 142, in gelu\n",
      "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 10.73 GiB total capacity; 8.04 GiB already allocated; 493.62 MiB free; 1.29 GiB cached)\n"
     ]
    }
   ],
   "source": [
    "!python run_bert_2562.py \\\n",
    "--model_type bert \\\n",
    "--model_name_or_path ../model/chinese_roberta_wwm_large_ext_pytorch  \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_test \\\n",
    "--data_dir ../data/data_StratifiedKFold_42/data_origin_2 \\\n",
    "--output_dir ../model/roberta_wwm_large_5121_42/roberta_wwm_large_5121_42_2 \\\n",
    "--max_seq_length 512 \\\n",
    "--split_num 1 \\\n",
    "--lstm_hidden_size 512 \\\n",
    "--lstm_layers 1 \\\n",
    "--lstm_dropout 0.1 \\\n",
    "--eval_steps 200 \\\n",
    "--per_gpu_train_batch_size 4 \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--warmup_steps 0 \\\n",
    "--per_gpu_eval_batch_size 64 \\\n",
    "--learning_rate 1e-5 \\\n",
    "--adam_epsilon 1e-6 \\\n",
    "--weight_decay 0 \\\n",
    "--train_steps 30000 \\\n",
    "--freeze 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/07/2019 22:20:50 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "11/07/2019 22:20:50 - INFO - pytorch_transformers.tokenization_utils -   Model name '../BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/chinese_roberta_wwm_large_ext_pytorch' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming '../BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/chinese_roberta_wwm_large_ext_pytorch' is a path or url to a directory containing tokenizer files.\n",
      "11/07/2019 22:20:50 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/chinese_roberta_wwm_large_ext_pytorch/added_tokens.json. We won't load it.\n",
      "11/07/2019 22:20:50 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/chinese_roberta_wwm_large_ext_pytorch/special_tokens_map.json. We won't load it.\n",
      "11/07/2019 22:20:50 - INFO - pytorch_transformers.tokenization_utils -   loading file ../BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/chinese_roberta_wwm_large_ext_pytorch/vocab.txt\n",
      "11/07/2019 22:20:50 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/07/2019 22:20:50 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/07/2019 22:20:50 - INFO - pytorch_transformers.modeling_utils -   loading configuration file ../BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/chinese_roberta_wwm_large_ext_pytorch/config.json\n",
      "11/07/2019 22:20:50 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 3,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "11/07/2019 22:20:50 - INFO - pytorch_transformers.modeling_utils -   loading weights file ../BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/chinese_roberta_wwm_large_ext_pytorch/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "11/07/2019 22:21:06 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'W.0.weight', 'W.0.bias', 'gru.0.weight_ih_l0', 'gru.0.weight_hh_l0', 'gru.0.bias_ih_l0', 'gru.0.bias_hh_l0', 'gru.0.weight_ih_l0_reverse', 'gru.0.weight_hh_l0_reverse', 'gru.0.bias_ih_l0_reverse', 'gru.0.bias_hh_l0_reverse']\n",
      "11/07/2019 22:21:06 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "11/07/2019 22:21:07 - INFO - __main__ -   *** Example ***\n",
      "11/07/2019 22:21:07 - INFO - __main__ -   idx: 0\n",
      "11/07/2019 22:21:07 - INFO - __main__ -   guid: 7640a5589bc7486ca199eeeb38af79dd\n",
      "11/07/2019 22:21:07 - INFO - __main__ -   tokens: [CLS] 江 歌 事 件 : 教 会 孩 子 ， 善 良 的 同 时 更 要 懂 得 保 护 自 己 ! [SEP] 过 去 一 年 的 江 歌 悲 剧 ， 这 几 日 再 次 刷 屏 ： 住 在 东 京 都 中 野 区 的 中 国 女 留 学 生 江 歌 ， 收 留 了 被 前 男 友 陈 世 锋 恶 意 纠 缠 的 闺 蜜 刘 鑫 ， 两 人 在 回 到 江 歌 公 寓 楼 时 ， 陈 世 锋 已 经 等 在 楼 下 ， 叫 嚣 着 要 刘 鑫 给 自 己 一 个 说 法 （ 男 友 此 时 的 情 绪 处 于 濒 临 崩 溃 的 状 态 ） 。 江 歌 为 了 保 护 刘 鑫 ， 就 让 她 先 进 了 房 间 ， 自 己 拦 在 外 面 要 求 陈 世 锋 离 开 。 结 果 江 歌 被 陈 世 锋 用 刀 多 处 刺 伤 脖 子 和 胸 部 ， 刀 刀 毙 命 ， 残 忍 至 极 ， 最 终 因 失 血 过 多 丧 生 。 十 几 刀 ， 刘 鑫 躲 在 屋 里 ， 躲 在 门 后 ， 亲 耳 听 着 闺 蜜 江 歌 的 声 声 求 助 及 惨 叫 ， 却 始 终 没 有 打 开 门 。 连 邻 居 都 听 到 呼 救 纷 纷 开 门 查 看 究 竟 ， 那 扇 可 以 救 命 的 们 始 终 没 有 打 开 。 江 歌 死 后 ， 刘 鑫 面 对 警 方 的 询 问 ， 称 自 己 一 无 所 知 ， 什 么 都 没 有 听 见 ， 不 肯 出 来 指 证 凶 手 、 为 江 歌 伸 冤 ， 只 想 着 撇 清 关 系 、 澄 清 自 己 ， 任 江 歌 遭 外 界 议 论 指 摘 ， 甚 至 拒 绝 同 江 歌 妈 妈 联 系 ， 自 己 做 了 新 头 发 买 了 新 包 包 快 快 乐 乐 地 过 着 自 己 的 生 活 ， 暗 自 庆 幸 自 己 终 于 摆 脱 了 那 个 纠 缠 自 己 的 前 男 友 。 迫 于 舆 论 的 压 力 ， 刘 鑫 终 于 出 现 了 。 江 歌 为 刘 鑫 失 去 了 生 命 ， 可 在 刘 鑫 眼 里 ， 江 歌 的 一 条 命 抵 不 过 自 己 的 名 声 ， 她 只 在 意 自 己 及 家 人 的 生 活 受 到 了 严 重 的 影 响 ， 认 为 自 己 知 道 错 了 江 歌 妈 妈 就 该 原 谅 她 了 ， 不 应 该 揪 着 她 不 放 。 甚 至 [SEP]\n",
      "11/07/2019 22:21:07 - INFO - __main__ -   input_ids: 101 3736 3625 752 816 131 3136 833 2111 2094 8024 1587 5679 4638 1398 3198 3291 6206 2743 2533 924 2844 5632 2346 106 102 6814 1343 671 2399 4638 3736 3625 2650 1196 8024 6821 1126 3189 1086 3613 1170 2242 8038 857 1762 691 776 6963 704 7029 1277 4638 704 1744 1957 4522 2110 4495 3736 3625 8024 3119 4522 749 6158 1184 4511 1351 7357 686 7226 2626 2692 5272 5362 4638 7318 6057 1155 7144 8024 697 782 1762 1726 1168 3736 3625 1062 2171 3517 3198 8024 7357 686 7226 2347 5307 5023 1762 3517 678 8024 1373 1709 4708 6206 1155 7144 5314 5632 2346 671 702 6432 3791 8020 4511 1351 3634 3198 4638 2658 5328 1905 754 4085 707 2309 3971 4638 4307 2578 8021 511 3736 3625 711 749 924 2844 1155 7144 8024 2218 6375 1961 1044 6822 749 2791 7313 8024 5632 2346 2882 1762 1912 7481 6206 3724 7357 686 7226 4895 2458 511 5310 3362 3736 3625 6158 7357 686 7226 4500 1143 1914 1905 1173 839 5556 2094 1469 5541 6956 8024 1143 1143 3687 1462 8024 3655 2556 5635 3353 8024 3297 5303 1728 1927 6117 6814 1914 700 4495 511 1282 1126 1143 8024 1155 7144 6719 1762 2238 7027 8024 6719 1762 7305 1400 8024 779 5455 1420 4708 7318 6057 3736 3625 4638 1898 1898 3724 1221 1350 2673 1373 8024 1316 1993 5303 3766 3300 2802 2458 7305 511 6825 6943 2233 6963 1420 1168 1461 3131 5290 5290 2458 7305 3389 4692 4955 4994 8024 6929 2794 1377 809 3131 1462 4638 812 1993 5303 3766 3300 2802 2458 511 3736 3625 3647 1400 8024 1155 7144 7481 2190 6356 3175 4638 6418 7309 8024 4917 5632 2346 671 3187 2792 4761 8024 784 720 6963 3766 3300 1420 6224 8024 679 5507 1139 3341 2900 6395 1136 2797 510 711 3736 3625 847 1096 8024 1372 2682 4708 3050 3926 1068 5143 510 4067 3926 5632 2346 8024 818 3736 3625 6901 1912 4518 6379 6389 2900 3036 8024 4493 5635 2867 5318 1398 3736 3625 1968 1968 5468 5143 8024 5632 2346 976 749 3173 1928 1355 743 749 3173 1259 1259 2571 2571 727 727 1765 6814 4708 5632 2346 4638 4495 3833 8024 3266 5632 2412 2401 5632 2346 5303 754 3030 5564 749 6929 702 5272 5362 5632 2346 4638 1184 4511 1351 511 6833 754 5644 6389 4638 1327 1213 8024 1155 7144 5303 754 1139 4385 749 511 3736 3625 711 1155 7144 1927 1343 749 4495 1462 8024 1377 1762 1155 7144 4706 7027 8024 3736 3625 4638 671 3340 1462 2850 679 6814 5632 2346 4638 1399 1898 8024 1961 1372 1762 2692 5632 2346 1350 2157 782 4638 4495 3833 1358 1168 749 698 7028 4638 2512 1510 8024 6371 711 5632 2346 4761 6887 7231 749 3736 3625 1968 1968 2218 6421 1333 6446 1961 749 8024 679 2418 6421 2998 4708 1961 679 3123 511 4493 5635 102\n",
      "11/07/2019 22:21:07 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/07/2019 22:21:07 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/07/2019 22:21:07 - INFO - __main__ -   label: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/07/2019 22:22:45 - INFO - __main__ -   ***** Running training *****\n",
      "11/07/2019 22:22:45 - INFO - __main__ -     Num examples = 11758\n",
      "11/07/2019 22:22:45 - INFO - __main__ -     Batch size = 4\n",
      "11/07/2019 22:22:45 - INFO - __main__ -     Num steps = 30000\n",
      "  0%|                                                 | 0/30000 [00:00<?, ?it/s]11/07/2019 22:23:10 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 22:23:10 - INFO - __main__ -     Num examples = 2938\n",
      "11/07/2019 22:23:10 - INFO - __main__ -     Batch size = 64\n",
      "/root/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "11/07/2019 22:25:20 - INFO - __main__ -     eval_F1 = 0.06251285214887929\n",
      "11/07/2019 22:25:20 - INFO - __main__ -     eval_loss = 1.1604295372962952\n",
      "11/07/2019 22:25:20 - INFO - __main__ -     global_step = 0\n",
      "================================================================================\n",
      "Best F1 0.06251285214887929\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.7554:   3%|▋                       | 799/30000 [04:51<1:28:38,  5.49it/s]11/07/2019 22:27:36 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 22:27:36 - INFO - __main__ -     global_step = 200\n",
      "11/07/2019 22:27:36 - INFO - __main__ -     train loss = 0.7554\n",
      "loss 0.4797:   5%|█▏                     | 1599/30000 [07:06<1:27:39,  5.40it/s]11/07/2019 22:29:51 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 22:29:51 - INFO - __main__ -     global_step = 400\n",
      "11/07/2019 22:29:51 - INFO - __main__ -     train loss = 0.4797\n",
      "loss 0.4401:   8%|█▊                     | 2399/30000 [09:21<1:25:12,  5.40it/s]11/07/2019 22:32:07 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 22:32:07 - INFO - __main__ -     global_step = 600\n",
      "11/07/2019 22:32:07 - INFO - __main__ -     train loss = 0.4401\n",
      "loss 0.4023:  11%|██▍                    | 3199/30000 [11:36<1:21:37,  5.47it/s]11/07/2019 22:34:22 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 22:34:22 - INFO - __main__ -     global_step = 800\n",
      "11/07/2019 22:34:22 - INFO - __main__ -     train loss = 0.4023\n",
      "loss 0.3811:  13%|███                    | 3999/30000 [13:51<1:19:01,  5.48it/s]11/07/2019 22:36:37 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 22:36:37 - INFO - __main__ -     global_step = 1000\n",
      "11/07/2019 22:36:37 - INFO - __main__ -     train loss = 0.3811\n",
      "loss 0.4242:  16%|███▋                   | 4799/30000 [16:06<1:16:57,  5.46it/s]11/07/2019 22:38:51 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 22:38:51 - INFO - __main__ -     global_step = 1200\n",
      "11/07/2019 22:38:51 - INFO - __main__ -     train loss = 0.4242\n",
      "loss 0.3639:  19%|████▎                  | 5599/30000 [18:21<1:14:21,  5.47it/s]11/07/2019 22:41:06 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 22:41:06 - INFO - __main__ -     global_step = 1400\n",
      "11/07/2019 22:41:06 - INFO - __main__ -     train loss = 0.3639\n",
      "loss 0.4108:  21%|████▉                  | 6399/30000 [20:35<1:12:26,  5.43it/s]11/07/2019 22:43:21 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 22:43:21 - INFO - __main__ -     global_step = 1600\n",
      "11/07/2019 22:43:21 - INFO - __main__ -     train loss = 0.4108\n",
      "loss 0.3887:  24%|█████▌                 | 7199/30000 [22:50<1:09:52,  5.44it/s]11/07/2019 22:45:36 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 22:45:36 - INFO - __main__ -     global_step = 1800\n",
      "11/07/2019 22:45:36 - INFO - __main__ -     train loss = 0.3887\n",
      "loss 0.3548:  27%|██████▏                | 7999/30000 [25:04<1:07:01,  5.47it/s]11/07/2019 22:47:50 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 22:47:50 - INFO - __main__ -     global_step = 2000\n",
      "11/07/2019 22:47:50 - INFO - __main__ -     train loss = 0.3548\n",
      "loss 0.3697:  29%|██████▋                | 8799/30000 [27:19<1:05:15,  5.41it/s]11/07/2019 22:50:05 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 22:50:05 - INFO - __main__ -     global_step = 2200\n",
      "11/07/2019 22:50:05 - INFO - __main__ -     train loss = 0.3697\n",
      "loss 0.3832:  32%|███████▎               | 9599/30000 [29:34<1:02:04,  5.48it/s]11/07/2019 22:52:20 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 22:52:20 - INFO - __main__ -     global_step = 2400\n",
      "11/07/2019 22:52:20 - INFO - __main__ -     train loss = 0.3832\n",
      "loss 0.3709:  35%|███████▋              | 10399/30000 [31:50<1:00:24,  5.41it/s]11/07/2019 22:54:36 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 22:54:36 - INFO - __main__ -     global_step = 2600\n",
      "11/07/2019 22:54:36 - INFO - __main__ -     train loss = 0.3709\n",
      "loss 0.3643:  37%|████████▉               | 11199/30000 [34:05<57:26,  5.45it/s]11/07/2019 22:56:51 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 22:56:51 - INFO - __main__ -     global_step = 2800\n",
      "11/07/2019 22:56:51 - INFO - __main__ -     train loss = 0.3643\n",
      "11/07/2019 22:57:14 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 22:57:14 - INFO - __main__ -     Num examples = 2938\n",
      "11/07/2019 22:57:14 - INFO - __main__ -     Batch size = 64\n",
      "11/07/2019 22:59:26 - INFO - __main__ -     eval_F1 = 0.808983588004225\n",
      "11/07/2019 22:59:26 - INFO - __main__ -     eval_loss = 0.3409061239141485\n",
      "11/07/2019 22:59:26 - INFO - __main__ -     global_step = 2800\n",
      "11/07/2019 22:59:26 - INFO - __main__ -     loss = 0.3643\n",
      "================================================================================\n",
      "Best F1 0.808983588004225\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.4153:  40%|█████████▌              | 11999/30000 [38:58<55:03,  5.45it/s]11/07/2019 23:01:44 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 23:01:44 - INFO - __main__ -     global_step = 3000\n",
      "11/07/2019 23:01:44 - INFO - __main__ -     train loss = 0.4153\n",
      "11/07/2019 23:02:08 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 23:02:08 - INFO - __main__ -     Num examples = 2938\n",
      "11/07/2019 23:02:08 - INFO - __main__ -     Batch size = 64\n",
      "11/07/2019 23:04:20 - INFO - __main__ -     eval_F1 = 0.8139966483668841\n",
      "11/07/2019 23:04:20 - INFO - __main__ -     eval_loss = 0.3276168388193068\n",
      "11/07/2019 23:04:20 - INFO - __main__ -     global_step = 3000\n",
      "11/07/2019 23:04:20 - INFO - __main__ -     loss = 0.4153\n",
      "================================================================================\n",
      "Best F1 0.8139966483668841\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.2586:  43%|██████████▏             | 12799/30000 [43:53<53:03,  5.40it/s]11/07/2019 23:06:39 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 23:06:39 - INFO - __main__ -     global_step = 3200\n",
      "11/07/2019 23:06:39 - INFO - __main__ -     train loss = 0.2586\n",
      "11/07/2019 23:07:03 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 23:07:03 - INFO - __main__ -     Num examples = 2938\n",
      "11/07/2019 23:07:03 - INFO - __main__ -     Batch size = 64\n",
      "11/07/2019 23:09:15 - INFO - __main__ -     eval_F1 = 0.7947350799011694\n",
      "11/07/2019 23:09:15 - INFO - __main__ -     eval_loss = 0.33967695028885553\n",
      "11/07/2019 23:09:15 - INFO - __main__ -     global_step = 3200\n",
      "11/07/2019 23:09:15 - INFO - __main__ -     loss = 0.2586\n",
      "================================================================================\n",
      "loss 0.2736:  45%|██████████▉             | 13599/30000 [48:44<50:41,  5.39it/s]11/07/2019 23:11:29 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 23:11:29 - INFO - __main__ -     global_step = 3400\n",
      "11/07/2019 23:11:29 - INFO - __main__ -     train loss = 0.2736\n",
      "11/07/2019 23:11:52 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 23:11:52 - INFO - __main__ -     Num examples = 2938\n",
      "11/07/2019 23:11:52 - INFO - __main__ -     Batch size = 64\n",
      "11/07/2019 23:14:04 - INFO - __main__ -     eval_F1 = 0.7974086260361624\n",
      "11/07/2019 23:14:04 - INFO - __main__ -     eval_loss = 0.3746784578037003\n",
      "11/07/2019 23:14:04 - INFO - __main__ -     global_step = 3400\n",
      "11/07/2019 23:14:04 - INFO - __main__ -     loss = 0.2736\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.2295:  48%|███████████▌            | 14399/30000 [53:33<47:39,  5.46it/s]11/07/2019 23:16:19 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 23:16:19 - INFO - __main__ -     global_step = 3600\n",
      "11/07/2019 23:16:19 - INFO - __main__ -     train loss = 0.2295\n",
      "11/07/2019 23:16:45 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 23:16:45 - INFO - __main__ -     Num examples = 2938\n",
      "11/07/2019 23:16:45 - INFO - __main__ -     Batch size = 64\n",
      "11/07/2019 23:18:56 - INFO - __main__ -     eval_F1 = 0.7847181351820258\n",
      "11/07/2019 23:18:56 - INFO - __main__ -     eval_loss = 0.40863544793556567\n",
      "11/07/2019 23:18:56 - INFO - __main__ -     global_step = 3600\n",
      "11/07/2019 23:18:56 - INFO - __main__ -     loss = 0.2295\n",
      "================================================================================\n",
      "loss 0.2141:  51%|████████████▏           | 15199/30000 [58:26<46:38,  5.29it/s]11/07/2019 23:21:11 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 23:21:11 - INFO - __main__ -     global_step = 3800\n",
      "11/07/2019 23:21:11 - INFO - __main__ -     train loss = 0.2141\n",
      "11/07/2019 23:21:36 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 23:21:36 - INFO - __main__ -     Num examples = 2938\n",
      "11/07/2019 23:21:36 - INFO - __main__ -     Batch size = 64\n",
      "11/07/2019 23:23:48 - INFO - __main__ -     eval_F1 = 0.7770165975454972\n",
      "11/07/2019 23:23:48 - INFO - __main__ -     eval_loss = 0.390330925381378\n",
      "11/07/2019 23:23:48 - INFO - __main__ -     global_step = 3800\n",
      "11/07/2019 23:23:48 - INFO - __main__ -     loss = 0.2141\n",
      "================================================================================\n",
      "loss 0.2486:  53%|███████████▋          | 15999/30000 [1:03:18<43:26,  5.37it/s]11/07/2019 23:26:03 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 23:26:03 - INFO - __main__ -     global_step = 4000\n",
      "11/07/2019 23:26:03 - INFO - __main__ -     train loss = 0.2486\n",
      "11/07/2019 23:26:28 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 23:26:28 - INFO - __main__ -     Num examples = 2938\n",
      "11/07/2019 23:26:28 - INFO - __main__ -     Batch size = 64\n",
      "11/07/2019 23:28:40 - INFO - __main__ -     eval_F1 = 0.8088638515036831\n",
      "11/07/2019 23:28:40 - INFO - __main__ -     eval_loss = 0.3535859866634659\n",
      "11/07/2019 23:28:40 - INFO - __main__ -     global_step = 4000\n",
      "11/07/2019 23:28:40 - INFO - __main__ -     loss = 0.2486\n",
      "================================================================================\n",
      "loss 0.2032:  56%|████████████▎         | 16799/30000 [1:08:09<40:57,  5.37it/s]11/07/2019 23:30:55 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 23:30:55 - INFO - __main__ -     global_step = 4200\n",
      "11/07/2019 23:30:55 - INFO - __main__ -     train loss = 0.2032\n",
      "11/07/2019 23:31:20 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 23:31:20 - INFO - __main__ -     Num examples = 2938\n",
      "11/07/2019 23:31:20 - INFO - __main__ -     Batch size = 64\n",
      "11/07/2019 23:33:32 - INFO - __main__ -     eval_F1 = 0.8048768091594248\n",
      "11/07/2019 23:33:32 - INFO - __main__ -     eval_loss = 0.3765031027243189\n",
      "11/07/2019 23:33:32 - INFO - __main__ -     global_step = 4200\n",
      "11/07/2019 23:33:32 - INFO - __main__ -     loss = 0.2032\n",
      "================================================================================\n",
      "loss 0.2275:  59%|████████████▉         | 17599/30000 [1:13:01<38:04,  5.43it/s]11/07/2019 23:35:47 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 23:35:47 - INFO - __main__ -     global_step = 4400\n",
      "11/07/2019 23:35:47 - INFO - __main__ -     train loss = 0.2275\n",
      "11/07/2019 23:36:12 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 23:36:12 - INFO - __main__ -     Num examples = 2938\n",
      "11/07/2019 23:36:12 - INFO - __main__ -     Batch size = 64\n",
      "11/07/2019 23:38:24 - INFO - __main__ -     eval_F1 = 0.7945480538334143\n",
      "11/07/2019 23:38:24 - INFO - __main__ -     eval_loss = 0.38919710910514643\n",
      "11/07/2019 23:38:24 - INFO - __main__ -     global_step = 4400\n",
      "11/07/2019 23:38:24 - INFO - __main__ -     loss = 0.2275\n",
      "================================================================================\n",
      "loss 0.2189:  61%|█████████████▍        | 18399/30000 [1:17:53<35:45,  5.41it/s]11/07/2019 23:40:39 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 23:40:39 - INFO - __main__ -     global_step = 4600\n",
      "11/07/2019 23:40:39 - INFO - __main__ -     train loss = 0.2189\n",
      "11/07/2019 23:41:04 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 23:41:04 - INFO - __main__ -     Num examples = 2938\n",
      "11/07/2019 23:41:04 - INFO - __main__ -     Batch size = 64\n",
      "11/07/2019 23:43:16 - INFO - __main__ -     eval_F1 = 0.7949377633841562\n",
      "11/07/2019 23:43:16 - INFO - __main__ -     eval_loss = 0.39724611766312434\n",
      "11/07/2019 23:43:16 - INFO - __main__ -     global_step = 4600\n",
      "11/07/2019 23:43:16 - INFO - __main__ -     loss = 0.2189\n",
      "================================================================================\n",
      "loss 0.2127:  64%|██████████████        | 19199/30000 [1:22:46<32:58,  5.46it/s]11/07/2019 23:45:31 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 23:45:31 - INFO - __main__ -     global_step = 4800\n",
      "11/07/2019 23:45:31 - INFO - __main__ -     train loss = 0.2127\n",
      "11/07/2019 23:45:56 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 23:45:56 - INFO - __main__ -     Num examples = 2938\n",
      "11/07/2019 23:45:56 - INFO - __main__ -     Batch size = 64\n",
      "11/07/2019 23:48:08 - INFO - __main__ -     eval_F1 = 0.8117299098488466\n",
      "11/07/2019 23:48:08 - INFO - __main__ -     eval_loss = 0.35757720600003784\n",
      "11/07/2019 23:48:08 - INFO - __main__ -     global_step = 4800\n",
      "11/07/2019 23:48:08 - INFO - __main__ -     loss = 0.2127\n",
      "================================================================================\n",
      "loss 0.1747:  67%|██████████████▋       | 19999/30000 [1:27:36<30:42,  5.43it/s]11/07/2019 23:50:22 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 23:50:22 - INFO - __main__ -     global_step = 5000\n",
      "11/07/2019 23:50:22 - INFO - __main__ -     train loss = 0.1747\n",
      "11/07/2019 23:50:45 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 23:50:45 - INFO - __main__ -     Num examples = 2938\n",
      "11/07/2019 23:50:45 - INFO - __main__ -     Batch size = 64\n",
      "11/07/2019 23:52:57 - INFO - __main__ -     eval_F1 = 0.8064739022103362\n",
      "11/07/2019 23:52:57 - INFO - __main__ -     eval_loss = 0.3864356872585157\n",
      "11/07/2019 23:52:57 - INFO - __main__ -     global_step = 5000\n",
      "11/07/2019 23:52:57 - INFO - __main__ -     loss = 0.1747\n",
      "================================================================================\n",
      "loss 0.2296:  69%|███████████████▎      | 20799/30000 [1:32:26<28:16,  5.42it/s]11/07/2019 23:55:11 - INFO - __main__ -   ***** Report result *****\n",
      "11/07/2019 23:55:11 - INFO - __main__ -     global_step = 5200\n",
      "11/07/2019 23:55:11 - INFO - __main__ -     train loss = 0.2296\n",
      "11/07/2019 23:55:35 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/07/2019 23:55:35 - INFO - __main__ -     Num examples = 2938\n",
      "11/07/2019 23:55:35 - INFO - __main__ -     Batch size = 64\n",
      "11/07/2019 23:57:47 - INFO - __main__ -     eval_F1 = 0.7979999467453643\n",
      "11/07/2019 23:57:47 - INFO - __main__ -     eval_loss = 0.3790832281760547\n",
      "11/07/2019 23:57:47 - INFO - __main__ -     global_step = 5200\n",
      "11/07/2019 23:57:47 - INFO - __main__ -     loss = 0.2296\n",
      "================================================================================\n",
      "loss 0.2296:  72%|███████████████▊      | 21599/30000 [1:37:16<25:56,  5.40it/s]11/08/2019 00:00:02 - INFO - __main__ -   ***** Report result *****\n",
      "11/08/2019 00:00:02 - INFO - __main__ -     global_step = 5400\n",
      "11/08/2019 00:00:02 - INFO - __main__ -     train loss = 0.2296\n",
      "11/08/2019 00:00:27 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/08/2019 00:00:27 - INFO - __main__ -     Num examples = 2938\n",
      "11/08/2019 00:00:27 - INFO - __main__ -     Batch size = 64\n",
      "11/08/2019 00:02:39 - INFO - __main__ -     eval_F1 = 0.8059589411335909\n",
      "11/08/2019 00:02:39 - INFO - __main__ -     eval_loss = 0.3583945572862159\n",
      "11/08/2019 00:02:39 - INFO - __main__ -     global_step = 5400\n",
      "11/08/2019 00:02:39 - INFO - __main__ -     loss = 0.2296\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.1811:  75%|████████████████▍     | 22399/30000 [1:42:09<23:53,  5.30it/s]11/08/2019 00:04:55 - INFO - __main__ -   ***** Report result *****\n",
      "11/08/2019 00:04:55 - INFO - __main__ -     global_step = 5600\n",
      "11/08/2019 00:04:55 - INFO - __main__ -     train loss = 0.1811\n",
      "11/08/2019 00:05:22 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/08/2019 00:05:22 - INFO - __main__ -     Num examples = 2938\n",
      "11/08/2019 00:05:22 - INFO - __main__ -     Batch size = 64\n",
      "11/08/2019 00:07:33 - INFO - __main__ -     eval_F1 = 0.8097181923034881\n",
      "11/08/2019 00:07:33 - INFO - __main__ -     eval_loss = 0.37678374001837295\n",
      "11/08/2019 00:07:33 - INFO - __main__ -     global_step = 5600\n",
      "11/08/2019 00:07:33 - INFO - __main__ -     loss = 0.1811\n",
      "================================================================================\n",
      "loss 0.211:  77%|█████████████████▊     | 23199/30000 [1:47:02<20:41,  5.48it/s]11/08/2019 00:09:48 - INFO - __main__ -   ***** Report result *****\n",
      "11/08/2019 00:09:48 - INFO - __main__ -     global_step = 5800\n",
      "11/08/2019 00:09:48 - INFO - __main__ -     train loss = 0.211\n",
      "11/08/2019 00:10:11 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/08/2019 00:10:11 - INFO - __main__ -     Num examples = 2938\n",
      "11/08/2019 00:10:11 - INFO - __main__ -     Batch size = 64\n",
      "11/08/2019 00:12:22 - INFO - __main__ -     eval_F1 = 0.793765672865101\n",
      "11/08/2019 00:12:22 - INFO - __main__ -     eval_loss = 0.3702479539358098\n",
      "11/08/2019 00:12:22 - INFO - __main__ -     global_step = 5800\n",
      "11/08/2019 00:12:22 - INFO - __main__ -     loss = 0.211\n",
      "================================================================================\n",
      "loss 0.2568:  80%|█████████████████▌    | 23999/30000 [1:51:51<18:15,  5.48it/s]11/08/2019 00:14:36 - INFO - __main__ -   ***** Report result *****\n",
      "11/08/2019 00:14:36 - INFO - __main__ -     global_step = 6000\n",
      "11/08/2019 00:14:36 - INFO - __main__ -     train loss = 0.2568\n",
      "11/08/2019 00:15:01 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/08/2019 00:15:01 - INFO - __main__ -     Num examples = 2938\n",
      "11/08/2019 00:15:01 - INFO - __main__ -     Batch size = 64\n",
      "11/08/2019 00:17:11 - INFO - __main__ -     eval_F1 = 0.8164957873323622\n",
      "11/08/2019 00:17:11 - INFO - __main__ -     eval_loss = 0.3395697186984446\n",
      "11/08/2019 00:17:11 - INFO - __main__ -     global_step = 6000\n",
      "11/08/2019 00:17:11 - INFO - __main__ -     loss = 0.2568\n",
      "================================================================================\n",
      "Best F1 0.8164957873323622\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.1371:  83%|██████████████████▏   | 24799/30000 [1:56:44<15:38,  5.54it/s]11/08/2019 00:19:29 - INFO - __main__ -   ***** Report result *****\n",
      "11/08/2019 00:19:29 - INFO - __main__ -     global_step = 6200\n",
      "11/08/2019 00:19:29 - INFO - __main__ -     train loss = 0.1371\n",
      "11/08/2019 00:19:53 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/08/2019 00:19:53 - INFO - __main__ -     Num examples = 2938\n",
      "11/08/2019 00:19:53 - INFO - __main__ -     Batch size = 64\n",
      "11/08/2019 00:22:03 - INFO - __main__ -     eval_F1 = 0.8131562313720044\n",
      "11/08/2019 00:22:03 - INFO - __main__ -     eval_loss = 0.35984818070479063\n",
      "11/08/2019 00:22:03 - INFO - __main__ -     global_step = 6200\n",
      "11/08/2019 00:22:03 - INFO - __main__ -     loss = 0.1371\n",
      "================================================================================\n",
      "loss 0.1235:  85%|██████████████████▊   | 25599/30000 [2:01:32<13:18,  5.51it/s]11/08/2019 00:24:17 - INFO - __main__ -   ***** Report result *****\n",
      "11/08/2019 00:24:17 - INFO - __main__ -     global_step = 6400\n",
      "11/08/2019 00:24:17 - INFO - __main__ -     train loss = 0.1235\n",
      "11/08/2019 00:24:40 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/08/2019 00:24:40 - INFO - __main__ -     Num examples = 2938\n",
      "11/08/2019 00:24:40 - INFO - __main__ -     Batch size = 64\n",
      "11/08/2019 00:26:51 - INFO - __main__ -     eval_F1 = 0.818203346980903\n",
      "11/08/2019 00:26:51 - INFO - __main__ -     eval_loss = 0.37481073974429263\n",
      "11/08/2019 00:26:51 - INFO - __main__ -     global_step = 6400\n",
      "11/08/2019 00:26:51 - INFO - __main__ -     loss = 0.1235\n",
      "================================================================================\n",
      "Best F1 0.818203346980903\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.1168:  88%|███████████████████▎  | 26399/30000 [2:06:22<11:03,  5.43it/s]11/08/2019 00:29:08 - INFO - __main__ -   ***** Report result *****\n",
      "11/08/2019 00:29:08 - INFO - __main__ -     global_step = 6600\n",
      "11/08/2019 00:29:08 - INFO - __main__ -     train loss = 0.1168\n",
      "11/08/2019 00:29:32 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/08/2019 00:29:32 - INFO - __main__ -     Num examples = 2938\n",
      "11/08/2019 00:29:32 - INFO - __main__ -     Batch size = 64\n",
      "11/08/2019 00:31:42 - INFO - __main__ -     eval_F1 = 0.81399015616578\n",
      "11/08/2019 00:31:42 - INFO - __main__ -     eval_loss = 0.37351485868187057\n",
      "11/08/2019 00:31:42 - INFO - __main__ -     global_step = 6600\n",
      "11/08/2019 00:31:42 - INFO - __main__ -     loss = 0.1168\n",
      "================================================================================\n",
      "loss 0.1444:  91%|███████████████████▉  | 27199/30000 [2:11:10<08:24,  5.55it/s]11/08/2019 00:33:55 - INFO - __main__ -   ***** Report result *****\n",
      "11/08/2019 00:33:55 - INFO - __main__ -     global_step = 6800\n",
      "11/08/2019 00:33:55 - INFO - __main__ -     train loss = 0.1444\n",
      "11/08/2019 00:34:20 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/08/2019 00:34:20 - INFO - __main__ -     Num examples = 2938\n",
      "11/08/2019 00:34:20 - INFO - __main__ -     Batch size = 64\n",
      "11/08/2019 00:36:30 - INFO - __main__ -     eval_F1 = 0.801447441225258\n",
      "11/08/2019 00:36:30 - INFO - __main__ -     eval_loss = 0.3785142760075953\n",
      "11/08/2019 00:36:30 - INFO - __main__ -     global_step = 6800\n",
      "11/08/2019 00:36:30 - INFO - __main__ -     loss = 0.1444\n",
      "================================================================================\n",
      "loss 0.1525:  93%|████████████████████▌ | 27999/30000 [2:15:58<06:04,  5.49it/s]11/08/2019 00:38:43 - INFO - __main__ -   ***** Report result *****\n",
      "11/08/2019 00:38:43 - INFO - __main__ -     global_step = 7000\n",
      "11/08/2019 00:38:43 - INFO - __main__ -     train loss = 0.1525\n",
      "11/08/2019 00:39:06 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/08/2019 00:39:06 - INFO - __main__ -     Num examples = 2938\n",
      "11/08/2019 00:39:06 - INFO - __main__ -     Batch size = 64\n",
      "11/08/2019 00:41:16 - INFO - __main__ -     eval_F1 = 0.8188338594825536\n",
      "11/08/2019 00:41:16 - INFO - __main__ -     eval_loss = 0.3726697847571062\n",
      "11/08/2019 00:41:16 - INFO - __main__ -     global_step = 7000\n",
      "11/08/2019 00:41:16 - INFO - __main__ -     loss = 0.1525\n",
      "================================================================================\n",
      "Best F1 0.8188338594825536\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.105:  96%|██████████████████████ | 28799/30000 [2:20:46<03:38,  5.49it/s]11/08/2019 00:43:32 - INFO - __main__ -   ***** Report result *****\n",
      "11/08/2019 00:43:32 - INFO - __main__ -     global_step = 7200\n",
      "11/08/2019 00:43:32 - INFO - __main__ -     train loss = 0.105\n",
      "11/08/2019 00:43:55 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/08/2019 00:43:55 - INFO - __main__ -     Num examples = 2938\n",
      "11/08/2019 00:43:55 - INFO - __main__ -     Batch size = 64\n",
      "11/08/2019 00:46:06 - INFO - __main__ -     eval_F1 = 0.8150757315716802\n",
      "11/08/2019 00:46:06 - INFO - __main__ -     eval_loss = 0.38121977343183494\n",
      "11/08/2019 00:46:06 - INFO - __main__ -     global_step = 7200\n",
      "11/08/2019 00:46:06 - INFO - __main__ -     loss = 0.105\n",
      "================================================================================\n",
      "loss 0.1473:  99%|█████████████████████▋| 29599/30000 [2:25:33<01:12,  5.52it/s]11/08/2019 00:48:19 - INFO - __main__ -   ***** Report result *****\n",
      "11/08/2019 00:48:19 - INFO - __main__ -     global_step = 7400\n",
      "11/08/2019 00:48:19 - INFO - __main__ -     train loss = 0.1473\n",
      "11/08/2019 00:48:42 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/08/2019 00:48:42 - INFO - __main__ -     Num examples = 2938\n",
      "11/08/2019 00:48:42 - INFO - __main__ -     Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/08/2019 00:50:52 - INFO - __main__ -     eval_F1 = 0.8178971826861908\n",
      "11/08/2019 00:50:52 - INFO - __main__ -     eval_loss = 0.38008762920356315\n",
      "11/08/2019 00:50:52 - INFO - __main__ -     global_step = 7400\n",
      "11/08/2019 00:50:52 - INFO - __main__ -     loss = 0.1473\n",
      "================================================================================\n",
      "loss 0.1145: 100%|██████████████████████| 30000/30000 [2:29:13<00:00,  6.06it/s]\n",
      "11/08/2019 00:51:59 - INFO - pytorch_transformers.modeling_utils -   loading weights file ./model/roberta_wwm_large_5121_42/roberta_wwm_large_5121_42_3/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "dev 0.8188338594825536\n",
      "Traceback (most recent call last):\n",
      "  File \"../BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/run_bert_2562.py\", line 841, in <module>\n",
      "    main()\n",
      "  File \"../BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/run_bert_2562.py\", line 757, in main\n",
      "    logits = model(input_ids=input_ids, token_type_ids=segment_ids, attention_mask=input_mask).detach().cpu().numpy()\n",
      "  File \"/root/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 547, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/root/code/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 1017, in forward\n",
      "    attention_mask=flat_attention_mask, head_mask=head_mask)\n",
      "  File \"/root/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 547, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/root/code/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 727, in forward\n",
      "    head_mask=head_mask)\n",
      "  File \"/root/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 547, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/root/code/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 440, in forward\n",
      "    layer_outputs = layer_module(hidden_states, attention_mask, head_mask[i])\n",
      "  File \"/root/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 547, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/root/code/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 420, in forward\n",
      "    intermediate_output = self.intermediate(attention_output)\n",
      "  File \"/root/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 547, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/root/code/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 392, in forward\n",
      "    hidden_states = self.intermediate_act_fn(hidden_states)\n",
      "  File \"/root/code/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 142, in gelu\n",
      "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 10.73 GiB total capacity; 8.04 GiB already allocated; 495.62 MiB free; 1.29 GiB cached)\n"
     ]
    }
   ],
   "source": [
    "!python run_bert_2562.py \\\n",
    "--model_type bert \\\n",
    "--model_name_or_path ../model/chinese_roberta_wwm_large_ext_pytorch  \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_test \\\n",
    "--data_dir ../data/data_StratifiedKFold_42/data_origin_3 \\\n",
    "--output_dir ../model/roberta_wwm_large_5121_42/roberta_wwm_large_5121_42_3 \\\n",
    "--max_seq_length 512 \\\n",
    "--split_num 1 \\\n",
    "--lstm_hidden_size 512 \\\n",
    "--lstm_layers 1 \\\n",
    "--lstm_dropout 0.1 \\\n",
    "--eval_steps 200 \\\n",
    "--per_gpu_train_batch_size 4 \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--warmup_steps 0 \\\n",
    "--per_gpu_eval_batch_size 64 \\\n",
    "--learning_rate 1e-5 \\\n",
    "--adam_epsilon 1e-6 \\\n",
    "--weight_decay 0 \\\n",
    "--train_steps 30000 \\\n",
    "--freeze 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/08/2019 00:55:39 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "11/08/2019 00:55:39 - INFO - pytorch_transformers.tokenization_utils -   Model name '../BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/chinese_roberta_wwm_large_ext_pytorch' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming '../BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/chinese_roberta_wwm_large_ext_pytorch' is a path or url to a directory containing tokenizer files.\n",
      "11/08/2019 00:55:39 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/chinese_roberta_wwm_large_ext_pytorch/added_tokens.json. We won't load it.\n",
      "11/08/2019 00:55:39 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/chinese_roberta_wwm_large_ext_pytorch/special_tokens_map.json. We won't load it.\n",
      "11/08/2019 00:55:39 - INFO - pytorch_transformers.tokenization_utils -   loading file ../BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/chinese_roberta_wwm_large_ext_pytorch/vocab.txt\n",
      "11/08/2019 00:55:39 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/08/2019 00:55:39 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/08/2019 00:55:39 - INFO - pytorch_transformers.modeling_utils -   loading configuration file ../BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/chinese_roberta_wwm_large_ext_pytorch/config.json\n",
      "11/08/2019 00:55:39 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 3,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "11/08/2019 00:55:39 - INFO - pytorch_transformers.modeling_utils -   loading weights file ../BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/chinese_roberta_wwm_large_ext_pytorch/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "11/08/2019 00:55:54 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'W.0.weight', 'W.0.bias', 'gru.0.weight_ih_l0', 'gru.0.weight_hh_l0', 'gru.0.bias_ih_l0', 'gru.0.bias_hh_l0', 'gru.0.weight_ih_l0_reverse', 'gru.0.weight_hh_l0_reverse', 'gru.0.bias_ih_l0_reverse', 'gru.0.bias_hh_l0_reverse']\n",
      "11/08/2019 00:55:54 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "11/08/2019 00:55:55 - INFO - __main__ -   *** Example ***\n",
      "11/08/2019 00:55:55 - INFO - __main__ -   idx: 0\n",
      "11/08/2019 00:55:55 - INFO - __main__ -   guid: 7a3dd79f90ee419da87190cff60f7a86\n",
      "11/08/2019 00:55:55 - INFO - __main__ -   tokens: [CLS] 问 责 领 导 ( 上 黄 镇 党 委 书 记 张 涛 ， 宣 国 才 真 能 一 手 遮 天 吗 ？ ) [SEP] 这 几 天 看 了 有 人 举 报 施 某 某 的 贴 子 ， 经 与 举 报 人 联 系 证 实 ， 是 宣 某 当 天 中 午 请 举 报 人 和 枪 手 喝 酒 后 ， 晚 上 才 发 的 贴 子 ！ 本 人 不 去 讨 论 前 二 天 的 举 报 ， 相 信 总 归 会 有 说 法 的 ！ 今 天 一 看 施 全 军 2017 年 1 月 2 日 实 名 举 报 上 黄 镇 宣 国 才 的 贴 子 （ 仍 被 锁 定 禁 止 评 论 ） 已 经 正 好 一 整 年 了 = 750 ) window . open ( ' http : / / img . js ##ly ##001 . com / at ##ta ##ch ##ment / mon _ 180 ##1 / 4 _ 291 ##08 ##5 _ c ##79 ##6 ##a ##6 ##a ##86 ##e ##17 ##12 ##1 . jpg ? 123 ' ) ; \" on ##load = \" if ( this . off ##set ##wi ##dt ##h > ' 750 ' ) this . wi ##dt ##h = ' 750 ' ; \" sr ##c = \" http : / / img . js ##ly ##001 . com / at ##ta ##ch ##ment / mon _ 180 ##1 / 4 _ 291 ##08 ##5 _ c ##79 ##6 ##a ##6 ##a ##86 ##e ##17 ##12 ##1 . jpg ? 123 \" style = \" max - wi ##dt ##h : 750 ##px ; \" / > 图 片 : / home / al ##ida ##ta / www / data / tm ##p / q ##fu ##pl ##oa ##d / 4 _ 291 ##08 ##5 _ 151 ##49 ##81 ##47 ##14 ##78 ##95 ##2 . jpg 施 全 军 实 名 举 报 50 天 后 ， 上 黄 镇 党 委 政 府 回 复 如 下 图 ： = 750 ) window . open ( ' http : / / img . js ##ly ##001 . com / at ##ta ##ch ##ment / mon _ 180 ##1 / 4 _ 291 ##08 ##5 _ a9 ##b ##11 ##b ##7 ##ea ##2 ##b ##1 ##ce ##9 . jpg ? 90 ' ) ; \" on ##load = \" if ( this . off ##set ##wi ##dt ##h > ' 750 ' ) this . wi ##dt ##h = ' 750 ' ; \" sr ##c = \" http : / / img . js ##ly ##001 . com / at ##ta ##ch ##ment / mon _ 180 ##1 / 4 _ 291 ##08 ##5 _ a9 ##b ##11 ##b ##7 ##ea ##2 ##b ##1 ##ce ##9 . jpg ? 90 \" style = \" max - wi ##dt ##h : 750 ##px ; \" / > 图 片 : / home / [SEP]\n",
      "11/08/2019 00:55:55 - INFO - __main__ -   input_ids: 101 7309 6569 7566 2193 113 677 7942 7252 1054 1999 741 6381 2476 3875 8024 2146 1744 2798 4696 5543 671 2797 6902 1921 1408 8043 114 102 6821 1126 1921 4692 749 3300 782 715 2845 3177 3378 3378 4638 6585 2094 8024 5307 680 715 2845 782 5468 5143 6395 2141 8024 3221 2146 3378 2496 1921 704 1286 6435 715 2845 782 1469 3366 2797 1600 6983 1400 8024 3241 677 2798 1355 4638 6585 2094 8013 3315 782 679 1343 6374 6389 1184 753 1921 4638 715 2845 8024 4685 928 2600 2495 833 3300 6432 3791 4638 8013 791 1921 671 4692 3177 1059 1092 8109 2399 122 3299 123 3189 2141 1399 715 2845 677 7942 7252 2146 1744 2798 4638 6585 2094 8020 793 6158 7219 2137 4881 3632 6397 6389 8021 2347 5307 3633 1962 671 3146 2399 749 134 9180 114 12158 119 8893 113 112 8184 131 120 120 11412 119 9016 8436 9942 119 8134 120 8243 8383 8370 8631 120 8556 142 8420 8148 120 125 142 11777 9153 8157 142 145 9495 8158 8139 8158 8139 9219 8154 8408 8455 8148 119 9248 136 8604 112 114 132 107 8281 11713 134 107 8898 113 8554 119 9594 9852 10958 12672 8199 135 112 9180 112 114 8554 119 8541 12672 8199 134 112 9180 112 132 107 12109 8177 134 107 8184 131 120 120 11412 119 9016 8436 9942 119 8134 120 8243 8383 8370 8631 120 8556 142 8420 8148 120 125 142 11777 9153 8157 142 145 9495 8158 8139 8158 8139 9219 8154 8408 8455 8148 119 9248 136 8604 107 8969 134 107 8621 118 8541 12672 8199 131 9180 10605 132 107 120 135 1745 4275 131 120 8563 120 9266 12708 8383 120 8173 120 9000 120 9908 8187 120 159 12043 12569 11355 8168 120 125 142 11777 9153 8157 142 9564 9500 9313 9050 8717 9136 9102 8144 119 9248 3177 1059 1092 2141 1399 715 2845 8145 1921 1400 8024 677 7942 7252 1054 1999 3124 2424 1726 1908 1963 678 1745 8038 134 9180 114 12158 119 8893 113 112 8184 131 120 120 11412 119 9016 8436 9942 119 8134 120 8243 8383 8370 8631 120 8556 142 8420 8148 120 125 142 11777 9153 8157 142 11094 8204 8452 8204 8161 10073 8144 8204 8148 8328 8160 119 9248 136 8192 112 114 132 107 8281 11713 134 107 8898 113 8554 119 9594 9852 10958 12672 8199 135 112 9180 112 114 8554 119 8541 12672 8199 134 112 9180 112 132 107 12109 8177 134 107 8184 131 120 120 11412 119 9016 8436 9942 119 8134 120 8243 8383 8370 8631 120 8556 142 8420 8148 120 125 142 11777 9153 8157 142 11094 8204 8452 8204 8161 10073 8144 8204 8148 8328 8160 119 9248 136 8192 107 8969 134 107 8621 118 8541 12672 8199 131 9180 10605 132 107 120 135 1745 4275 131 120 8563 120 102\n",
      "11/08/2019 00:55:55 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/08/2019 00:55:55 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/08/2019 00:55:55 - INFO - __main__ -   label: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/08/2019 00:57:24 - INFO - __main__ -   ***** Running training *****\n",
      "11/08/2019 00:57:24 - INFO - __main__ -     Num examples = 11758\n",
      "11/08/2019 00:57:24 - INFO - __main__ -     Batch size = 4\n",
      "11/08/2019 00:57:24 - INFO - __main__ -     Num steps = 30000\n",
      "  0%|                                                 | 0/30000 [00:00<?, ?it/s]11/08/2019 00:57:47 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/08/2019 00:57:47 - INFO - __main__ -     Num examples = 2938\n",
      "11/08/2019 00:57:47 - INFO - __main__ -     Batch size = 64\n",
      "/root/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "11/08/2019 00:59:56 - INFO - __main__ -     eval_F1 = 0.06311785211058317\n",
      "11/08/2019 00:59:56 - INFO - __main__ -     eval_loss = 1.1610753121583357\n",
      "11/08/2019 00:59:56 - INFO - __main__ -     global_step = 0\n",
      "================================================================================\n",
      "Best F1 0.06311785211058317\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.687:   3%|▋                        | 799/30000 [04:45<1:27:17,  5.58it/s]11/08/2019 01:02:10 - INFO - __main__ -   ***** Report result *****\n",
      "11/08/2019 01:02:10 - INFO - __main__ -     global_step = 200\n",
      "11/08/2019 01:02:10 - INFO - __main__ -     train loss = 0.687\n",
      "loss 0.5148:   5%|█▏                     | 1599/30000 [06:57<1:25:54,  5.51it/s]11/08/2019 01:04:22 - INFO - __main__ -   ***** Report result *****\n",
      "11/08/2019 01:04:22 - INFO - __main__ -     global_step = 400\n",
      "11/08/2019 01:04:22 - INFO - __main__ -     train loss = 0.5148\n",
      "loss 0.4785:   8%|█▊                     | 2399/30000 [09:10<1:22:35,  5.57it/s]11/08/2019 01:06:34 - INFO - __main__ -   ***** Report result *****\n",
      "11/08/2019 01:06:34 - INFO - __main__ -     global_step = 600\n",
      "11/08/2019 01:06:34 - INFO - __main__ -     train loss = 0.4785\n",
      "loss 0.4273:  11%|██▍                    | 3199/30000 [11:21<1:20:19,  5.56it/s]11/08/2019 01:08:46 - INFO - __main__ -   ***** Report result *****\n",
      "11/08/2019 01:08:46 - INFO - __main__ -     global_step = 800\n",
      "11/08/2019 01:08:46 - INFO - __main__ -     train loss = 0.4273\n",
      "loss 0.4212:  13%|███                    | 3999/30000 [13:34<1:18:20,  5.53it/s]11/08/2019 01:10:58 - INFO - __main__ -   ***** Report result *****\n",
      "11/08/2019 01:10:58 - INFO - __main__ -     global_step = 1000\n",
      "11/08/2019 01:10:58 - INFO - __main__ -     train loss = 0.4212\n",
      "loss 0.4002:  16%|███▋                   | 4799/30000 [15:46<1:16:06,  5.52it/s]11/08/2019 01:13:10 - INFO - __main__ -   ***** Report result *****\n",
      "11/08/2019 01:13:10 - INFO - __main__ -     global_step = 1200\n",
      "11/08/2019 01:13:10 - INFO - __main__ -     train loss = 0.4002\n",
      "loss 0.3852:  19%|████▎                  | 5599/30000 [17:57<1:13:06,  5.56it/s]11/08/2019 01:15:22 - INFO - __main__ -   ***** Report result *****\n",
      "11/08/2019 01:15:22 - INFO - __main__ -     global_step = 1400\n",
      "11/08/2019 01:15:22 - INFO - __main__ -     train loss = 0.3852\n",
      "loss 0.393:  21%|█████                   | 6399/30000 [20:09<1:12:01,  5.46it/s]11/08/2019 01:17:33 - INFO - __main__ -   ***** Report result *****\n",
      "11/08/2019 01:17:33 - INFO - __main__ -     global_step = 1600\n",
      "11/08/2019 01:17:33 - INFO - __main__ -     train loss = 0.393\n",
      "loss 0.4009:  24%|█████▌                 | 7199/30000 [22:20<1:08:22,  5.56it/s]11/08/2019 01:19:45 - INFO - __main__ -   ***** Report result *****\n",
      "11/08/2019 01:19:45 - INFO - __main__ -     global_step = 1800\n",
      "11/08/2019 01:19:45 - INFO - __main__ -     train loss = 0.4009\n",
      "loss 0.3737:  27%|██████▏                | 7999/30000 [24:32<1:06:13,  5.54it/s]11/08/2019 01:21:56 - INFO - __main__ -   ***** Report result *****\n",
      "11/08/2019 01:21:56 - INFO - __main__ -     global_step = 2000\n",
      "11/08/2019 01:21:56 - INFO - __main__ -     train loss = 0.3737\n",
      "loss 0.3974:  29%|██████▋                | 8799/30000 [26:43<1:03:41,  5.55it/s]11/08/2019 01:24:08 - INFO - __main__ -   ***** Report result *****\n",
      "11/08/2019 01:24:08 - INFO - __main__ -     global_step = 2200\n",
      "11/08/2019 01:24:08 - INFO - __main__ -     train loss = 0.3974\n",
      "loss 0.3945:  32%|███████▎               | 9599/30000 [28:55<1:00:56,  5.58it/s]11/08/2019 01:26:20 - INFO - __main__ -   ***** Report result *****\n",
      "11/08/2019 01:26:20 - INFO - __main__ -     global_step = 2400\n",
      "11/08/2019 01:26:20 - INFO - __main__ -     train loss = 0.3945\n",
      "loss 0.3395:  35%|████████▎               | 10399/30000 [31:07<58:54,  5.55it/s]11/08/2019 01:28:32 - INFO - __main__ -   ***** Report result *****\n",
      "11/08/2019 01:28:32 - INFO - __main__ -     global_step = 2600\n",
      "11/08/2019 01:28:32 - INFO - __main__ -     train loss = 0.3395\n",
      "loss 0.3565:  37%|████████▉               | 11199/30000 [33:20<57:06,  5.49it/s]11/08/2019 01:30:44 - INFO - __main__ -   ***** Report result *****\n",
      "11/08/2019 01:30:44 - INFO - __main__ -     global_step = 2800\n",
      "11/08/2019 01:30:44 - INFO - __main__ -     train loss = 0.3565\n",
      "11/08/2019 01:31:09 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/08/2019 01:31:09 - INFO - __main__ -     Num examples = 2938\n",
      "11/08/2019 01:31:09 - INFO - __main__ -     Batch size = 64\n",
      "11/08/2019 01:33:20 - INFO - __main__ -     eval_F1 = 0.7939991246146691\n",
      "11/08/2019 01:33:20 - INFO - __main__ -     eval_loss = 0.36090998092423315\n",
      "11/08/2019 01:33:20 - INFO - __main__ -     global_step = 2800\n",
      "11/08/2019 01:33:20 - INFO - __main__ -     loss = 0.3565\n",
      "================================================================================\n",
      "Best F1 0.7939991246146691\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.3581:  40%|█████████▌              | 11999/30000 [38:11<54:04,  5.55it/s]11/08/2019 01:35:35 - INFO - __main__ -   ***** Report result *****\n",
      "11/08/2019 01:35:35 - INFO - __main__ -     global_step = 3000\n",
      "11/08/2019 01:35:35 - INFO - __main__ -     train loss = 0.3581\n",
      "11/08/2019 01:35:58 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/08/2019 01:35:58 - INFO - __main__ -     Num examples = 2938\n",
      "11/08/2019 01:35:58 - INFO - __main__ -     Batch size = 64\n",
      "11/08/2019 01:38:08 - INFO - __main__ -     eval_F1 = 0.7699107265214415\n",
      "11/08/2019 01:38:08 - INFO - __main__ -     eval_loss = 0.3796065508023552\n",
      "11/08/2019 01:38:08 - INFO - __main__ -     global_step = 3000\n",
      "11/08/2019 01:38:08 - INFO - __main__ -     loss = 0.3581\n",
      "================================================================================\n",
      "loss 0.2664:  43%|██████████▏             | 12799/30000 [42:56<51:58,  5.52it/s]11/08/2019 01:40:21 - INFO - __main__ -   ***** Report result *****\n",
      "11/08/2019 01:40:21 - INFO - __main__ -     global_step = 3200\n",
      "11/08/2019 01:40:21 - INFO - __main__ -     train loss = 0.2664\n",
      "11/08/2019 01:40:43 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/08/2019 01:40:43 - INFO - __main__ -     Num examples = 2938\n",
      "11/08/2019 01:40:43 - INFO - __main__ -     Batch size = 64\n",
      "11/08/2019 01:42:53 - INFO - __main__ -     eval_F1 = 0.7945325544683614\n",
      "11/08/2019 01:42:53 - INFO - __main__ -     eval_loss = 0.36293822316371877\n",
      "11/08/2019 01:42:53 - INFO - __main__ -     global_step = 3200\n",
      "11/08/2019 01:42:53 - INFO - __main__ -     loss = 0.2664\n",
      "================================================================================\n",
      "Best F1 0.7945325544683614\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.2402:  45%|██████████▉             | 13599/30000 [47:44<49:25,  5.53it/s]11/08/2019 01:45:09 - INFO - __main__ -   ***** Report result *****\n",
      "11/08/2019 01:45:09 - INFO - __main__ -     global_step = 3400\n",
      "11/08/2019 01:45:09 - INFO - __main__ -     train loss = 0.2402\n",
      "11/08/2019 01:45:32 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/08/2019 01:45:32 - INFO - __main__ -     Num examples = 2938\n",
      "11/08/2019 01:45:32 - INFO - __main__ -     Batch size = 64\n",
      "11/08/2019 01:47:42 - INFO - __main__ -     eval_F1 = 0.7987585350501712\n",
      "11/08/2019 01:47:42 - INFO - __main__ -     eval_loss = 0.3813633930100047\n",
      "11/08/2019 01:47:42 - INFO - __main__ -     global_step = 3400\n",
      "11/08/2019 01:47:42 - INFO - __main__ -     loss = 0.2402\n",
      "================================================================================\n",
      "Best F1 0.7987585350501712\n",
      "Saving Model......\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "loss 0.2707:  48%|███████████▌            | 14399/30000 [52:34<47:03,  5.53it/s]11/08/2019 01:49:58 - INFO - __main__ -   ***** Report result *****\n",
      "11/08/2019 01:49:58 - INFO - __main__ -     global_step = 3600\n",
      "11/08/2019 01:49:58 - INFO - __main__ -     train loss = 0.2707\n",
      "11/08/2019 01:50:22 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/08/2019 01:50:22 - INFO - __main__ -     Num examples = 2938\n",
      "11/08/2019 01:50:22 - INFO - __main__ -     Batch size = 64\n",
      "11/08/2019 01:52:34 - INFO - __main__ -     eval_F1 = 0.7891156746864469\n",
      "11/08/2019 01:52:34 - INFO - __main__ -     eval_loss = 0.3716871539211791\n",
      "11/08/2019 01:52:34 - INFO - __main__ -     global_step = 3600\n",
      "11/08/2019 01:52:34 - INFO - __main__ -     loss = 0.2707\n",
      "================================================================================\n",
      "loss 0.245:  51%|████████████▋            | 15199/30000 [57:24<45:16,  5.45it/s]11/08/2019 01:54:48 - INFO - __main__ -   ***** Report result *****\n",
      "11/08/2019 01:54:48 - INFO - __main__ -     global_step = 3800\n",
      "11/08/2019 01:54:48 - INFO - __main__ -     train loss = 0.245\n",
      "11/08/2019 01:55:13 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/08/2019 01:55:13 - INFO - __main__ -     Num examples = 2938\n",
      "11/08/2019 01:55:13 - INFO - __main__ -     Batch size = 64\n",
      "11/08/2019 01:57:24 - INFO - __main__ -     eval_F1 = 0.7548753192335572\n",
      "11/08/2019 01:57:24 - INFO - __main__ -     eval_loss = 0.47087953790374426\n",
      "11/08/2019 01:57:24 - INFO - __main__ -     global_step = 3800\n",
      "11/08/2019 01:57:24 - INFO - __main__ -     loss = 0.245\n",
      "================================================================================\n",
      "loss 0.2372:  53%|███████████▋          | 15999/30000 [1:02:15<42:55,  5.44it/s]11/08/2019 01:59:39 - INFO - __main__ -   ***** Report result *****\n",
      "11/08/2019 01:59:39 - INFO - __main__ -     global_step = 4000\n",
      "11/08/2019 01:59:39 - INFO - __main__ -     train loss = 0.2372\n",
      "11/08/2019 02:00:02 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/08/2019 02:00:02 - INFO - __main__ -     Num examples = 2938\n",
      "11/08/2019 02:00:02 - INFO - __main__ -     Batch size = 64\n",
      "11/08/2019 02:02:14 - INFO - __main__ -     eval_F1 = 0.7712089567352726\n",
      "11/08/2019 02:02:14 - INFO - __main__ -     eval_loss = 0.4077981883740943\n",
      "11/08/2019 02:02:14 - INFO - __main__ -     global_step = 4000\n",
      "11/08/2019 02:02:14 - INFO - __main__ -     loss = 0.2372\n",
      "================================================================================\n",
      "loss 0.2028:  56%|████████████▎         | 16799/30000 [1:07:03<40:24,  5.45it/s]11/08/2019 02:04:28 - INFO - __main__ -   ***** Report result *****\n",
      "11/08/2019 02:04:28 - INFO - __main__ -     global_step = 4200\n",
      "11/08/2019 02:04:28 - INFO - __main__ -     train loss = 0.2028\n",
      "11/08/2019 02:04:51 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/08/2019 02:04:51 - INFO - __main__ -     Num examples = 2938\n",
      "11/08/2019 02:04:51 - INFO - __main__ -     Batch size = 64\n",
      "11/08/2019 02:07:02 - INFO - __main__ -     eval_F1 = 0.7846810952968637\n",
      "11/08/2019 02:07:02 - INFO - __main__ -     eval_loss = 0.40818117818106775\n",
      "11/08/2019 02:07:02 - INFO - __main__ -     global_step = 4200\n",
      "11/08/2019 02:07:02 - INFO - __main__ -     loss = 0.2028\n",
      "================================================================================\n",
      "loss 0.2294:  59%|████████████▉         | 17599/30000 [1:11:52<37:56,  5.45it/s]11/08/2019 02:09:16 - INFO - __main__ -   ***** Report result *****\n",
      "11/08/2019 02:09:16 - INFO - __main__ -     global_step = 4400\n",
      "11/08/2019 02:09:16 - INFO - __main__ -     train loss = 0.2294\n",
      "11/08/2019 02:09:40 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/08/2019 02:09:40 - INFO - __main__ -     Num examples = 2938\n",
      "11/08/2019 02:09:40 - INFO - __main__ -     Batch size = 64\n",
      "11/08/2019 02:11:52 - INFO - __main__ -     eval_F1 = 0.7815988143550125\n",
      "11/08/2019 02:11:52 - INFO - __main__ -     eval_loss = 0.3757830216832783\n",
      "11/08/2019 02:11:52 - INFO - __main__ -     global_step = 4400\n",
      "11/08/2019 02:11:52 - INFO - __main__ -     loss = 0.2294\n",
      "================================================================================\n",
      "loss 0.1887:  61%|█████████████▍        | 18399/30000 [1:16:41<35:24,  5.46it/s]11/08/2019 02:14:06 - INFO - __main__ -   ***** Report result *****\n",
      "11/08/2019 02:14:06 - INFO - __main__ -     global_step = 4600\n",
      "11/08/2019 02:14:06 - INFO - __main__ -     train loss = 0.1887\n",
      "11/08/2019 02:14:28 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/08/2019 02:14:28 - INFO - __main__ -     Num examples = 2938\n",
      "11/08/2019 02:14:28 - INFO - __main__ -     Batch size = 64\n",
      "11/08/2019 02:16:40 - INFO - __main__ -     eval_F1 = 0.7913953319012501\n",
      "11/08/2019 02:16:40 - INFO - __main__ -     eval_loss = 0.3997830589828284\n",
      "11/08/2019 02:16:40 - INFO - __main__ -     global_step = 4600\n",
      "11/08/2019 02:16:40 - INFO - __main__ -     loss = 0.1887\n",
      "================================================================================\n",
      "loss 0.2704:  64%|██████████████        | 19199/30000 [1:21:29<32:54,  5.47it/s]11/08/2019 02:18:54 - INFO - __main__ -   ***** Report result *****\n",
      "11/08/2019 02:18:54 - INFO - __main__ -     global_step = 4800\n",
      "11/08/2019 02:18:54 - INFO - __main__ -     train loss = 0.2704\n",
      "11/08/2019 02:19:17 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/08/2019 02:19:17 - INFO - __main__ -     Num examples = 2938\n",
      "11/08/2019 02:19:17 - INFO - __main__ -     Batch size = 64\n",
      "11/08/2019 02:21:29 - INFO - __main__ -     eval_F1 = 0.7956770327966303\n",
      "11/08/2019 02:21:29 - INFO - __main__ -     eval_loss = 0.3690009938310022\n",
      "11/08/2019 02:21:29 - INFO - __main__ -     global_step = 4800\n",
      "11/08/2019 02:21:29 - INFO - __main__ -     loss = 0.2704\n",
      "================================================================================\n",
      "loss 0.1869:  67%|██████████████▋       | 19999/30000 [1:26:19<30:26,  5.48it/s]11/08/2019 02:23:43 - INFO - __main__ -   ***** Report result *****\n",
      "11/08/2019 02:23:43 - INFO - __main__ -     global_step = 5000\n",
      "11/08/2019 02:23:43 - INFO - __main__ -     train loss = 0.1869\n",
      "11/08/2019 02:24:08 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/08/2019 02:24:08 - INFO - __main__ -     Num examples = 2938\n",
      "11/08/2019 02:24:08 - INFO - __main__ -     Batch size = 64\n",
      "11/08/2019 02:26:19 - INFO - __main__ -     eval_F1 = 0.7900671590582725\n",
      "11/08/2019 02:26:19 - INFO - __main__ -     eval_loss = 0.3922702621506608\n",
      "11/08/2019 02:26:19 - INFO - __main__ -     global_step = 5000\n",
      "11/08/2019 02:26:19 - INFO - __main__ -     loss = 0.1869\n",
      "================================================================================\n",
      "loss 0.2186:  69%|███████████████▎      | 20799/30000 [1:31:09<28:09,  5.44it/s]11/08/2019 02:28:33 - INFO - __main__ -   ***** Report result *****\n",
      "11/08/2019 02:28:33 - INFO - __main__ -     global_step = 5200\n",
      "11/08/2019 02:28:33 - INFO - __main__ -     train loss = 0.2186\n",
      "11/08/2019 02:28:57 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/08/2019 02:28:57 - INFO - __main__ -     Num examples = 2938\n",
      "11/08/2019 02:28:57 - INFO - __main__ -     Batch size = 64\n",
      "11/08/2019 02:31:08 - INFO - __main__ -     eval_F1 = 0.7741460123021807\n",
      "11/08/2019 02:31:08 - INFO - __main__ -     eval_loss = 0.4053484935151494\n",
      "11/08/2019 02:31:08 - INFO - __main__ -     global_step = 5200\n",
      "11/08/2019 02:31:08 - INFO - __main__ -     loss = 0.2186\n",
      "================================================================================\n",
      "loss 0.1973:  72%|███████████████▊      | 21599/30000 [1:35:59<26:13,  5.34it/s]11/08/2019 02:33:23 - INFO - __main__ -   ***** Report result *****\n",
      "11/08/2019 02:33:23 - INFO - __main__ -     global_step = 5400\n",
      "11/08/2019 02:33:23 - INFO - __main__ -     train loss = 0.1973\n",
      "11/08/2019 02:33:47 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/08/2019 02:33:47 - INFO - __main__ -     Num examples = 2938\n",
      "11/08/2019 02:33:47 - INFO - __main__ -     Batch size = 64\n",
      "11/08/2019 02:35:58 - INFO - __main__ -     eval_F1 = 0.7901783582745926\n",
      "11/08/2019 02:35:58 - INFO - __main__ -     eval_loss = 0.39557201026574423\n",
      "11/08/2019 02:35:58 - INFO - __main__ -     global_step = 5400\n",
      "11/08/2019 02:35:58 - INFO - __main__ -     loss = 0.1973\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.1594:  75%|████████████████▍     | 22399/30000 [1:40:49<23:24,  5.41it/s]11/08/2019 02:38:14 - INFO - __main__ -   ***** Report result *****\n",
      "11/08/2019 02:38:14 - INFO - __main__ -     global_step = 5600\n",
      "11/08/2019 02:38:14 - INFO - __main__ -     train loss = 0.1594\n",
      "11/08/2019 02:38:38 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/08/2019 02:38:38 - INFO - __main__ -     Num examples = 2938\n",
      "11/08/2019 02:38:38 - INFO - __main__ -     Batch size = 64\n",
      "11/08/2019 02:40:50 - INFO - __main__ -     eval_F1 = 0.7943245637969009\n",
      "11/08/2019 02:40:50 - INFO - __main__ -     eval_loss = 0.41268107959109807\n",
      "11/08/2019 02:40:50 - INFO - __main__ -     global_step = 5600\n",
      "11/08/2019 02:40:50 - INFO - __main__ -     loss = 0.1594\n",
      "================================================================================\n",
      "loss 0.1945:  77%|█████████████████     | 23199/30000 [1:45:40<21:25,  5.29it/s]11/08/2019 02:43:05 - INFO - __main__ -   ***** Report result *****\n",
      "11/08/2019 02:43:05 - INFO - __main__ -     global_step = 5800\n",
      "11/08/2019 02:43:05 - INFO - __main__ -     train loss = 0.1945\n",
      "11/08/2019 02:43:28 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/08/2019 02:43:28 - INFO - __main__ -     Num examples = 2938\n",
      "11/08/2019 02:43:28 - INFO - __main__ -     Batch size = 64\n",
      "11/08/2019 02:45:40 - INFO - __main__ -     eval_F1 = 0.7836457373710862\n",
      "11/08/2019 02:45:40 - INFO - __main__ -     eval_loss = 0.4278224658059037\n",
      "11/08/2019 02:45:40 - INFO - __main__ -     global_step = 5800\n",
      "11/08/2019 02:45:40 - INFO - __main__ -     loss = 0.1945\n",
      "================================================================================\n",
      "loss 0.189:  80%|██████████████████▍    | 23999/30000 [1:50:29<18:17,  5.47it/s]11/08/2019 02:47:54 - INFO - __main__ -   ***** Report result *****\n",
      "11/08/2019 02:47:54 - INFO - __main__ -     global_step = 6000\n",
      "11/08/2019 02:47:54 - INFO - __main__ -     train loss = 0.189\n",
      "11/08/2019 02:48:17 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/08/2019 02:48:17 - INFO - __main__ -     Num examples = 2938\n",
      "11/08/2019 02:48:17 - INFO - __main__ -     Batch size = 64\n",
      "11/08/2019 02:50:29 - INFO - __main__ -     eval_F1 = 0.7928603227222496\n",
      "11/08/2019 02:50:29 - INFO - __main__ -     eval_loss = 0.4157326751105163\n",
      "11/08/2019 02:50:29 - INFO - __main__ -     global_step = 6000\n",
      "11/08/2019 02:50:29 - INFO - __main__ -     loss = 0.189\n",
      "================================================================================\n",
      "loss 0.1385:  83%|██████████████████▏   | 24799/30000 [1:55:19<15:55,  5.44it/s]11/08/2019 02:52:44 - INFO - __main__ -   ***** Report result *****\n",
      "11/08/2019 02:52:44 - INFO - __main__ -     global_step = 6200\n",
      "11/08/2019 02:52:44 - INFO - __main__ -     train loss = 0.1385\n",
      "11/08/2019 02:53:08 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/08/2019 02:53:08 - INFO - __main__ -     Num examples = 2938\n",
      "11/08/2019 02:53:08 - INFO - __main__ -     Batch size = 64\n",
      "11/08/2019 02:55:20 - INFO - __main__ -     eval_F1 = 0.7934716222672339\n",
      "11/08/2019 02:55:20 - INFO - __main__ -     eval_loss = 0.4241032642514809\n",
      "11/08/2019 02:55:20 - INFO - __main__ -     global_step = 6200\n",
      "11/08/2019 02:55:20 - INFO - __main__ -     loss = 0.1385\n",
      "================================================================================\n",
      "loss 0.121:  85%|███████████████████▋   | 25599/30000 [2:00:12<13:30,  5.43it/s]11/08/2019 02:57:37 - INFO - __main__ -   ***** Report result *****\n",
      "11/08/2019 02:57:37 - INFO - __main__ -     global_step = 6400\n",
      "11/08/2019 02:57:37 - INFO - __main__ -     train loss = 0.121\n",
      "11/08/2019 02:58:01 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/08/2019 02:58:01 - INFO - __main__ -     Num examples = 2938\n",
      "11/08/2019 02:58:01 - INFO - __main__ -     Batch size = 64\n",
      "11/08/2019 03:00:12 - INFO - __main__ -     eval_F1 = 0.7951822994454542\n",
      "11/08/2019 03:00:12 - INFO - __main__ -     eval_loss = 0.4358082271140555\n",
      "11/08/2019 03:00:12 - INFO - __main__ -     global_step = 6400\n",
      "11/08/2019 03:00:12 - INFO - __main__ -     loss = 0.121\n",
      "================================================================================\n",
      "loss 0.1192:  88%|███████████████████▎  | 26399/30000 [2:05:02<11:02,  5.43it/s]11/08/2019 03:02:27 - INFO - __main__ -   ***** Report result *****\n",
      "11/08/2019 03:02:27 - INFO - __main__ -     global_step = 6600\n",
      "11/08/2019 03:02:27 - INFO - __main__ -     train loss = 0.1192\n",
      "11/08/2019 03:02:51 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/08/2019 03:02:51 - INFO - __main__ -     Num examples = 2938\n",
      "11/08/2019 03:02:51 - INFO - __main__ -     Batch size = 64\n",
      "11/08/2019 03:05:02 - INFO - __main__ -     eval_F1 = 0.7961163206437333\n",
      "11/08/2019 03:05:02 - INFO - __main__ -     eval_loss = 0.4369875006377697\n",
      "11/08/2019 03:05:02 - INFO - __main__ -     global_step = 6600\n",
      "11/08/2019 03:05:02 - INFO - __main__ -     loss = 0.1192\n",
      "================================================================================\n",
      "loss 0.1084:  91%|███████████████████▉  | 27199/30000 [2:09:52<08:33,  5.45it/s]11/08/2019 03:07:16 - INFO - __main__ -   ***** Report result *****\n",
      "11/08/2019 03:07:16 - INFO - __main__ -     global_step = 6800\n",
      "11/08/2019 03:07:16 - INFO - __main__ -     train loss = 0.1084\n",
      "11/08/2019 03:07:40 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/08/2019 03:07:40 - INFO - __main__ -     Num examples = 2938\n",
      "11/08/2019 03:07:40 - INFO - __main__ -     Batch size = 64\n",
      "11/08/2019 03:09:51 - INFO - __main__ -     eval_F1 = 0.7889310510097608\n",
      "11/08/2019 03:09:51 - INFO - __main__ -     eval_loss = 0.4396986937069375\n",
      "11/08/2019 03:09:51 - INFO - __main__ -     global_step = 6800\n",
      "11/08/2019 03:09:51 - INFO - __main__ -     loss = 0.1084\n",
      "================================================================================\n",
      "loss 0.1598:  93%|████████████████████▌ | 27999/30000 [2:14:42<06:08,  5.43it/s]11/08/2019 03:12:06 - INFO - __main__ -   ***** Report result *****\n",
      "11/08/2019 03:12:06 - INFO - __main__ -     global_step = 7000\n",
      "11/08/2019 03:12:06 - INFO - __main__ -     train loss = 0.1598\n",
      "11/08/2019 03:12:30 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/08/2019 03:12:30 - INFO - __main__ -     Num examples = 2938\n",
      "11/08/2019 03:12:30 - INFO - __main__ -     Batch size = 64\n",
      "11/08/2019 03:14:42 - INFO - __main__ -     eval_F1 = 0.7911360923899133\n",
      "11/08/2019 03:14:42 - INFO - __main__ -     eval_loss = 0.42954017513472104\n",
      "11/08/2019 03:14:42 - INFO - __main__ -     global_step = 7000\n",
      "11/08/2019 03:14:42 - INFO - __main__ -     loss = 0.1598\n",
      "================================================================================\n",
      "loss 0.1051:  96%|█████████████████████ | 28799/30000 [2:19:32<03:40,  5.44it/s]11/08/2019 03:16:56 - INFO - __main__ -   ***** Report result *****\n",
      "11/08/2019 03:16:56 - INFO - __main__ -     global_step = 7200\n",
      "11/08/2019 03:16:56 - INFO - __main__ -     train loss = 0.1051\n",
      "11/08/2019 03:17:20 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/08/2019 03:17:20 - INFO - __main__ -     Num examples = 2938\n",
      "11/08/2019 03:17:20 - INFO - __main__ -     Batch size = 64\n",
      "11/08/2019 03:19:31 - INFO - __main__ -     eval_F1 = 0.7887230234983639\n",
      "11/08/2019 03:19:31 - INFO - __main__ -     eval_loss = 0.4345126289712346\n",
      "11/08/2019 03:19:31 - INFO - __main__ -     global_step = 7200\n",
      "11/08/2019 03:19:31 - INFO - __main__ -     loss = 0.1051\n",
      "================================================================================\n",
      "loss 0.1041:  99%|█████████████████████▋| 29599/30000 [2:24:21<01:13,  5.44it/s]11/08/2019 03:21:45 - INFO - __main__ -   ***** Report result *****\n",
      "11/08/2019 03:21:45 - INFO - __main__ -     global_step = 7400\n",
      "11/08/2019 03:21:45 - INFO - __main__ -     train loss = 0.1041\n",
      "11/08/2019 03:22:09 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/08/2019 03:22:09 - INFO - __main__ -     Num examples = 2938\n",
      "11/08/2019 03:22:09 - INFO - __main__ -     Batch size = 64\n",
      "11/08/2019 03:24:21 - INFO - __main__ -     eval_F1 = 0.7927399268998534\n",
      "11/08/2019 03:24:21 - INFO - __main__ -     eval_loss = 0.4290070640652076\n",
      "11/08/2019 03:24:21 - INFO - __main__ -     global_step = 7400\n",
      "11/08/2019 03:24:21 - INFO - __main__ -     loss = 0.1041\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.0943: 100%|██████████████████████| 30000/30000 [2:28:04<00:00,  5.97it/s]\n",
      "11/08/2019 03:25:28 - INFO - pytorch_transformers.modeling_utils -   loading weights file ./model/roberta_wwm_large_5121_42/roberta_wwm_large_5121_42_4/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "dev 0.7987585350501712\n",
      "Traceback (most recent call last):\n",
      "  File \"../BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/run_bert_2562.py\", line 841, in <module>\n",
      "    main()\n",
      "  File \"../BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/run_bert_2562.py\", line 757, in main\n",
      "    logits = model(input_ids=input_ids, token_type_ids=segment_ids, attention_mask=input_mask).detach().cpu().numpy()\n",
      "  File \"/root/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 547, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/root/code/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 1017, in forward\n",
      "    attention_mask=flat_attention_mask, head_mask=head_mask)\n",
      "  File \"/root/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 547, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/root/code/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 727, in forward\n",
      "    head_mask=head_mask)\n",
      "  File \"/root/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 547, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/root/code/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 440, in forward\n",
      "    layer_outputs = layer_module(hidden_states, attention_mask, head_mask[i])\n",
      "  File \"/root/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 547, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/root/code/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 420, in forward\n",
      "    intermediate_output = self.intermediate(attention_output)\n",
      "  File \"/root/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 547, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/root/code/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 392, in forward\n",
      "    hidden_states = self.intermediate_act_fn(hidden_states)\n",
      "  File \"/root/code/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 142, in gelu\n",
      "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 10.73 GiB total capacity; 8.04 GiB already allocated; 495.62 MiB free; 1.29 GiB cached)\n"
     ]
    }
   ],
   "source": [
    "!python run_bert_2562.py \\\n",
    "--model_type bert \\\n",
    "--model_name_or_path ../model/chinese_roberta_wwm_large_ext_pytorch  \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_test \\\n",
    "--data_dir ../data/data_StratifiedKFold_42/data_origin_4 \\\n",
    "--output_dir ../model/roberta_wwm_large_5121_42/roberta_wwm_large_5121_42_4 \\\n",
    "--max_seq_length 512 \\\n",
    "--split_num 1 \\\n",
    "--lstm_hidden_size 512 \\\n",
    "--lstm_layers 1 \\\n",
    "--lstm_dropout 0.1 \\\n",
    "--eval_steps 200 \\\n",
    "--per_gpu_train_batch_size 4 \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--warmup_steps 0 \\\n",
    "--per_gpu_eval_batch_size 64 \\\n",
    "--learning_rate 1e-5 \\\n",
    "--adam_epsilon 1e-6 \\\n",
    "--weight_decay 0 \\\n",
    "--train_steps 30000 \\\n",
    "--freeze 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/08/2019 10:32:36 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "11/08/2019 10:32:36 - INFO - pytorch_transformers.tokenization_utils -   Model name './model/roberta_wwm_large_5121_42/roberta_wwm_large_5121_42_0' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming './model/roberta_wwm_large_5121_42/roberta_wwm_large_5121_42_0' is a path or url to a directory containing tokenizer files.\n",
      "11/08/2019 10:32:36 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ./model/roberta_wwm_large_5121_42/roberta_wwm_large_5121_42_0/added_tokens.json. We won't load it.\n",
      "11/08/2019 10:32:36 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ./model/roberta_wwm_large_5121_42/roberta_wwm_large_5121_42_0/special_tokens_map.json. We won't load it.\n",
      "11/08/2019 10:32:36 - INFO - pytorch_transformers.tokenization_utils -   loading file ./model/roberta_wwm_large_5121_42/roberta_wwm_large_5121_42_0/vocab.txt\n",
      "11/08/2019 10:32:36 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/08/2019 10:32:36 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/08/2019 10:32:36 - INFO - pytorch_transformers.modeling_utils -   loading configuration file ./model/roberta_wwm_large_5121_42/roberta_wwm_large_5121_42_0/config.json\n",
      "11/08/2019 10:32:36 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 3,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "11/08/2019 10:32:36 - INFO - pytorch_transformers.modeling_utils -   loading weights file ./model/roberta_wwm_large_5121_42/roberta_wwm_large_5121_42_0/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "11/08/2019 10:32:51 - INFO - pytorch_transformers.modeling_utils -   loading weights file ./model/roberta_wwm_large_5121_42/roberta_wwm_large_5121_42_0/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "dev 0.8060991327205045\n",
      "/root/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "test 0.06238200771566937\n"
     ]
    }
   ],
   "source": [
    "# 5121 \n",
    "!python run_bert_2562.py \\\n",
    "--model_type bert \\\n",
    "--model_name_or_path ../model/chinese_roberta_wwm_large_ext_pytorch  \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_test \\\n",
    "--data_dir ../data/data_StratifiedKFold_42/data_origin_0 \\\n",
    "--output_dir ../model/roberta_wwm_large_5121_42/roberta_wwm_large_5121_42_0 \\\n",
    "--max_seq_length 512 \\\n",
    "--split_num 1 \\\n",
    "--lstm_hidden_size 512 \\\n",
    "--lstm_layers 1 \\\n",
    "--lstm_dropout 0.1 \\\n",
    "--eval_steps 200 \\\n",
    "--per_gpu_train_batch_size 4 \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--warmup_steps 0 \\\n",
    "--per_gpu_eval_batch_size 64 \\\n",
    "--learning_rate 1e-5 \\\n",
    "--adam_epsilon 1e-6 \\\n",
    "--weight_decay 0 \\\n",
    "--train_steps 30000 \\\n",
    "--freeze 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/08/2019 10:42:09 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "11/08/2019 10:42:09 - INFO - pytorch_transformers.tokenization_utils -   Model name './model/roberta_wwm_large_5121_42/roberta_wwm_large_5121_42_1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming './model/roberta_wwm_large_5121_42/roberta_wwm_large_5121_42_1' is a path or url to a directory containing tokenizer files.\n",
      "11/08/2019 10:42:09 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ./model/roberta_wwm_large_5121_42/roberta_wwm_large_5121_42_1/added_tokens.json. We won't load it.\n",
      "11/08/2019 10:42:09 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ./model/roberta_wwm_large_5121_42/roberta_wwm_large_5121_42_1/special_tokens_map.json. We won't load it.\n",
      "11/08/2019 10:42:09 - INFO - pytorch_transformers.tokenization_utils -   loading file ./model/roberta_wwm_large_5121_42/roberta_wwm_large_5121_42_1/vocab.txt\n",
      "11/08/2019 10:42:09 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/08/2019 10:42:09 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/08/2019 10:42:09 - INFO - pytorch_transformers.modeling_utils -   loading configuration file ./model/roberta_wwm_large_5121_42/roberta_wwm_large_5121_42_1/config.json\n",
      "11/08/2019 10:42:09 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 3,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "11/08/2019 10:42:09 - INFO - pytorch_transformers.modeling_utils -   loading weights file ./model/roberta_wwm_large_5121_42/roberta_wwm_large_5121_42_1/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "11/08/2019 10:42:26 - INFO - pytorch_transformers.modeling_utils -   loading weights file ./model/roberta_wwm_large_5121_42/roberta_wwm_large_5121_42_1/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "dev 0.80239286370484\n",
      "/root/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "test 0.07161332040090528\n"
     ]
    }
   ],
   "source": [
    "# 5121 \n",
    "!python run_bert_2562.py \\\n",
    "--model_type bert \\\n",
    "--model_name_or_path ../model/chinese_roberta_wwm_large_ext_pytorch  \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_test \\\n",
    "--data_dir ../data/data_StratifiedKFold_42/data_origin_1 \\\n",
    "--output_dir ../model/roberta_wwm_large_5121_42/roberta_wwm_large_5121_42_1 \\\n",
    "--max_seq_length 512 \\\n",
    "--split_num 1 \\\n",
    "--lstm_hidden_size 512 \\\n",
    "--lstm_layers 1 \\\n",
    "--lstm_dropout 0.1 \\\n",
    "--eval_steps 200 \\\n",
    "--per_gpu_train_batch_size 4 \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--warmup_steps 0 \\\n",
    "--per_gpu_eval_batch_size 64 \\\n",
    "--learning_rate 1e-5 \\\n",
    "--adam_epsilon 1e-6 \\\n",
    "--weight_decay 0 \\\n",
    "--train_steps 30000 \\\n",
    "--freeze 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/08/2019 10:51:42 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "11/08/2019 10:51:42 - INFO - pytorch_transformers.tokenization_utils -   Model name './model/roberta_wwm_large_5121_42/roberta_wwm_large_5121_42_2' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming './model/roberta_wwm_large_5121_42/roberta_wwm_large_5121_42_2' is a path or url to a directory containing tokenizer files.\n",
      "11/08/2019 10:51:42 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ./model/roberta_wwm_large_5121_42/roberta_wwm_large_5121_42_2/added_tokens.json. We won't load it.\n",
      "11/08/2019 10:51:42 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ./model/roberta_wwm_large_5121_42/roberta_wwm_large_5121_42_2/special_tokens_map.json. We won't load it.\n",
      "11/08/2019 10:51:42 - INFO - pytorch_transformers.tokenization_utils -   loading file ./model/roberta_wwm_large_5121_42/roberta_wwm_large_5121_42_2/vocab.txt\n",
      "11/08/2019 10:51:42 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/08/2019 10:51:42 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/08/2019 10:51:42 - INFO - pytorch_transformers.modeling_utils -   loading configuration file ./model/roberta_wwm_large_5121_42/roberta_wwm_large_5121_42_2/config.json\n",
      "11/08/2019 10:51:42 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 3,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "11/08/2019 10:51:42 - INFO - pytorch_transformers.modeling_utils -   loading weights file ./model/roberta_wwm_large_5121_42/roberta_wwm_large_5121_42_2/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "11/08/2019 10:51:59 - INFO - pytorch_transformers.modeling_utils -   loading weights file ./model/roberta_wwm_large_5121_42/roberta_wwm_large_5121_42_2/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "dev 0.8058172511623906\n",
      "/root/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "test 0.06512235976631123\n"
     ]
    }
   ],
   "source": [
    "# 5121 \n",
    "!python run_bert_2562.py \\\n",
    "--model_type bert \\\n",
    "--model_name_or_path ../model/chinese_roberta_wwm_large_ext_pytorch  \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_test \\\n",
    "--data_dir ../data/data_StratifiedKFold_42/data_origin_2 \\\n",
    "--output_dir ../model/roberta_wwm_large_5121_42/roberta_wwm_large_5121_42_2 \\\n",
    "--max_seq_length 512 \\\n",
    "--split_num 1 \\\n",
    "--lstm_hidden_size 512 \\\n",
    "--lstm_layers 1 \\\n",
    "--lstm_dropout 0.1 \\\n",
    "--eval_steps 200 \\\n",
    "--per_gpu_train_batch_size 4 \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--warmup_steps 0 \\\n",
    "--per_gpu_eval_batch_size 64 \\\n",
    "--learning_rate 1e-5 \\\n",
    "--adam_epsilon 1e-6 \\\n",
    "--weight_decay 0 \\\n",
    "--train_steps 30000 \\\n",
    "--freeze 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/08/2019 11:01:17 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "11/08/2019 11:01:17 - INFO - pytorch_transformers.tokenization_utils -   Model name './model/roberta_wwm_large_5121_42/roberta_wwm_large_5121_42_3' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming './model/roberta_wwm_large_5121_42/roberta_wwm_large_5121_42_3' is a path or url to a directory containing tokenizer files.\n",
      "11/08/2019 11:01:17 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ./model/roberta_wwm_large_5121_42/roberta_wwm_large_5121_42_3/added_tokens.json. We won't load it.\n",
      "11/08/2019 11:01:17 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ./model/roberta_wwm_large_5121_42/roberta_wwm_large_5121_42_3/special_tokens_map.json. We won't load it.\n",
      "11/08/2019 11:01:17 - INFO - pytorch_transformers.tokenization_utils -   loading file ./model/roberta_wwm_large_5121_42/roberta_wwm_large_5121_42_3/vocab.txt\n",
      "11/08/2019 11:01:17 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/08/2019 11:01:17 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/08/2019 11:01:17 - INFO - pytorch_transformers.modeling_utils -   loading configuration file ./model/roberta_wwm_large_5121_42/roberta_wwm_large_5121_42_3/config.json\n",
      "11/08/2019 11:01:17 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 3,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "11/08/2019 11:01:17 - INFO - pytorch_transformers.modeling_utils -   loading weights file ./model/roberta_wwm_large_5121_42/roberta_wwm_large_5121_42_3/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "11/08/2019 11:01:31 - INFO - pytorch_transformers.modeling_utils -   loading weights file ./model/roberta_wwm_large_5121_42/roberta_wwm_large_5121_42_3/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "dev 0.8188338594825536\n",
      "/root/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "test 0.06200977372592501\n"
     ]
    }
   ],
   "source": [
    "# 5121 \n",
    "!python run_bert_2562.py \\\n",
    "--model_type bert \\\n",
    "--model_name_or_path ../model/chinese_roberta_wwm_large_ext_pytorch  \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_test \\\n",
    "--data_dir ../data/data_StratifiedKFold_42/data_origin_3 \\\n",
    "--output_dir ../model/roberta_wwm_large_5121_42/roberta_wwm_large_5121_42_3 \\\n",
    "--max_seq_length 512 \\\n",
    "--split_num 1 \\\n",
    "--lstm_hidden_size 512 \\\n",
    "--lstm_layers 1 \\\n",
    "--lstm_dropout 0.1 \\\n",
    "--eval_steps 200 \\\n",
    "--per_gpu_train_batch_size 4 \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--warmup_steps 0 \\\n",
    "--per_gpu_eval_batch_size 64 \\\n",
    "--learning_rate 1e-5 \\\n",
    "--adam_epsilon 1e-6 \\\n",
    "--weight_decay 0 \\\n",
    "--train_steps 30000 \\\n",
    "--freeze 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/08/2019 11:10:43 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "11/08/2019 11:10:43 - INFO - pytorch_transformers.tokenization_utils -   Model name './model/roberta_wwm_large_5121_42/roberta_wwm_large_5121_42_4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming './model/roberta_wwm_large_5121_42/roberta_wwm_large_5121_42_4' is a path or url to a directory containing tokenizer files.\n",
      "11/08/2019 11:10:43 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ./model/roberta_wwm_large_5121_42/roberta_wwm_large_5121_42_4/added_tokens.json. We won't load it.\n",
      "11/08/2019 11:10:43 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ./model/roberta_wwm_large_5121_42/roberta_wwm_large_5121_42_4/special_tokens_map.json. We won't load it.\n",
      "11/08/2019 11:10:43 - INFO - pytorch_transformers.tokenization_utils -   loading file ./model/roberta_wwm_large_5121_42/roberta_wwm_large_5121_42_4/vocab.txt\n",
      "11/08/2019 11:10:43 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/08/2019 11:10:43 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/08/2019 11:10:44 - INFO - pytorch_transformers.modeling_utils -   loading configuration file ./model/roberta_wwm_large_5121_42/roberta_wwm_large_5121_42_4/config.json\n",
      "11/08/2019 11:10:44 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 3,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "11/08/2019 11:10:44 - INFO - pytorch_transformers.modeling_utils -   loading weights file ./model/roberta_wwm_large_5121_42/roberta_wwm_large_5121_42_4/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "11/08/2019 11:10:59 - INFO - pytorch_transformers.modeling_utils -   loading weights file ./model/roberta_wwm_large_5121_42/roberta_wwm_large_5121_42_4/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "dev 0.7987585350501712\n",
      "/root/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "test 0.07824801182911721\n"
     ]
    }
   ],
   "source": [
    "# 5121 \n",
    "!python run_bert_2562.py \\\n",
    "--model_type bert \\\n",
    "--model_name_or_path ../model/chinese_roberta_wwm_large_ext_pytorch  \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_test \\\n",
    "--data_dir ../data/data_StratifiedKFold_42/data_origin_4 \\\n",
    "--output_dir ../model/roberta_wwm_large_5121_42/roberta_wwm_large_5121_42_4 \\\n",
    "--max_seq_length 512 \\\n",
    "--split_num 1 \\\n",
    "--lstm_hidden_size 512 \\\n",
    "--lstm_layers 1 \\\n",
    "--lstm_dropout 0.1 \\\n",
    "--eval_steps 200 \\\n",
    "--per_gpu_train_batch_size 4 \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--warmup_steps 0 \\\n",
    "--per_gpu_eval_batch_size 64 \\\n",
    "--learning_rate 1e-5 \\\n",
    "--adam_epsilon 1e-6 \\\n",
    "--weight_decay 0 \\\n",
    "--train_steps 30000 \\\n",
    "--freeze 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1170787100526261\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df = pd.read_csv('../model/roberta_wwm_large_5121_42/roberta_wwm_large_5121_42_0/sub.csv')\n",
    "df = df[['id']]\n",
    "df['0'] = 0\n",
    "df['1'] = 0\n",
    "df['2'] = 0\n",
    "\n",
    "k=5\n",
    "for i in [0,1,2,3,4]:\n",
    "    temp=pd.read_csv('../model/roberta_wwm_large_5121_42/roberta_wwm_large_5121_42_{}/sub.csv'.format(i))\n",
    "    df['0']+=temp['label_0']/k\n",
    "    df['1']+=temp['label_1']/k\n",
    "    df['2']+=temp['label_2']/k\n",
    "print(df['0'].mean())\n",
    "\n",
    "df['label']=np.argmax(df[['0','1','2']].values,-1)\n",
    "df[['id','label']].to_csv('../model/roberta_wwm_large_5121_42/roberta_wwm_large_5121_42_sub.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
