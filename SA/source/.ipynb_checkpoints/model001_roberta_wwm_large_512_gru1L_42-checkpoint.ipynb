{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/09/2019 10:50:53 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "11/09/2019 10:50:53 - INFO - pytorch_transformers.tokenization_utils -   Model name '../CCF-BDCI-Sentiment-Analysis-Baseline/chinese_roberta_wwm_large_ext_pytorch' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming '../CCF-BDCI-Sentiment-Analysis-Baseline/chinese_roberta_wwm_large_ext_pytorch' is a path or url to a directory containing tokenizer files.\n",
      "11/09/2019 10:50:53 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../CCF-BDCI-Sentiment-Analysis-Baseline/chinese_roberta_wwm_large_ext_pytorch/added_tokens.json. We won't load it.\n",
      "11/09/2019 10:50:53 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../CCF-BDCI-Sentiment-Analysis-Baseline/chinese_roberta_wwm_large_ext_pytorch/special_tokens_map.json. We won't load it.\n",
      "11/09/2019 10:50:53 - INFO - pytorch_transformers.tokenization_utils -   loading file ../CCF-BDCI-Sentiment-Analysis-Baseline/chinese_roberta_wwm_large_ext_pytorch/vocab.txt\n",
      "11/09/2019 10:50:53 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/09/2019 10:50:53 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/09/2019 10:50:54 - INFO - pytorch_transformers.modeling_utils -   loading configuration file ../CCF-BDCI-Sentiment-Analysis-Baseline/chinese_roberta_wwm_large_ext_pytorch/config.json\n",
      "11/09/2019 10:50:54 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 3,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "11/09/2019 10:50:54 - INFO - pytorch_transformers.modeling_utils -   loading weights file ../CCF-BDCI-Sentiment-Analysis-Baseline/chinese_roberta_wwm_large_ext_pytorch/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "11/09/2019 10:51:11 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'W.0.weight', 'W.0.bias', 'W.1.weight', 'W.1.bias', 'gru.0.weight_ih_l0', 'gru.0.weight_hh_l0', 'gru.0.bias_ih_l0', 'gru.0.bias_hh_l0', 'gru.0.weight_ih_l0_reverse', 'gru.0.weight_hh_l0_reverse', 'gru.0.bias_ih_l0_reverse', 'gru.0.bias_hh_l0_reverse', 'gru.1.weight_ih_l0', 'gru.1.weight_hh_l0', 'gru.1.bias_ih_l0', 'gru.1.bias_hh_l0', 'gru.1.weight_ih_l0_reverse', 'gru.1.weight_hh_l0_reverse', 'gru.1.bias_ih_l0_reverse', 'gru.1.bias_hh_l0_reverse']\n",
      "11/09/2019 10:51:11 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "11/09/2019 10:51:12 - INFO - __main__ -   *** Example ***\n",
      "11/09/2019 10:51:12 - INFO - __main__ -   idx: 0\n",
      "11/09/2019 10:51:12 - INFO - __main__ -   guid: 7a3dd79f90ee419da87190cff60f7a86\n",
      "11/09/2019 10:51:12 - INFO - __main__ -   tokens: [CLS] 问 责 领 导 ( 上 黄 镇 党 委 书 记 张 涛 ， 宣 国 才 真 能 一 手 遮 天 吗 ？ ) [SEP] 这 几 天 看 了 有 人 举 报 施 某 某 的 贴 子 ， 经 与 举 报 人 联 系 证 实 ， 是 宣 某 当 天 中 午 请 举 报 人 和 枪 手 喝 酒 后 ， 晚 上 才 发 的 贴 子 ！ 本 人 不 去 讨 论 前 二 天 的 举 报 ， 相 信 总 归 会 有 说 法 的 ！ 今 天 一 看 施 全 军 2017 年 1 月 2 日 实 名 举 报 上 黄 镇 宣 国 才 的 贴 子 （ 仍 被 锁 定 禁 止 评 论 ） 已 经 正 好 一 整 年 了 = 750 ) window . open ( ' http : / / img . js ##ly ##001 . com / at ##ta ##ch ##ment / mon _ 180 ##1 / 4 _ 291 ##08 ##5 _ c ##79 ##6 ##a ##6 ##a ##86 ##e ##17 ##12 ##1 . jpg ? 123 ' ) ; \" on ##load = \" if ( this . off ##set ##wi ##dt ##h > ' 750 ' ) this . wi ##dt ##h = ' 750 ' ; \" sr ##c = \" http : / / img . js ##ly ##001 . com / at ##ta ##ch ##ment / mon _ 180 ##1 / 4 _ 291 ##08 ##5 _ c ##79 ##6 ##a ##6 ##a ##86 ##e ##17 ##12 ##1 . jpg ? 123 \" style = \" max - wi ##dt ##h : 750 ##px ; \" / > 图 片 : / home / al ##ida ##ta / www / data / tm ##p / q ##fu ##pl ##oa ##d / 4 _ 291 ##08 ##5 _ 151 ##49 ##81 ##47 ##14 ##78 ##95 ##2 . jpg 施 全 军 实 名 举 报 50 天 后 ， 上 黄 镇 党 委 政 府 回 复 如 下 图 ： = 750 ) window . open ( ' http : / / img . js ##ly ##001 . com / at ##ta ##ch ##ment / mon _ 180 ##1 / 4 _ 291 ##08 ##5 _ a9 ##b ##11 ##b ##7 ##ea ##2 ##b ##1 ##ce ##9 . jpg ? 90 ' ) ; \" on ##load = \" if ( this . off ##set ##wi ##dt ##h > ' 750 ' ) this . wi ##dt ##h = ' 750 ' ; \" sr ##c = \" http : / / img . js ##ly ##001 . com / at ##ta ##ch ##ment / mon _ 180 ##1 / 4 _ 291 ##08 ##5 _ a9 ##b ##11 ##b ##7 ##ea ##2 ##b ##1 ##ce ##9 . jpg ? 90 \" style = \" max - wi ##dt ##h : 750 ##px ; \" / > 图 片 : / home / [SEP]\n",
      "11/09/2019 10:51:12 - INFO - __main__ -   input_ids: 101 7309 6569 7566 2193 113 677 7942 7252 1054 1999 741 6381 2476 3875 8024 2146 1744 2798 4696 5543 671 2797 6902 1921 1408 8043 114 102 6821 1126 1921 4692 749 3300 782 715 2845 3177 3378 3378 4638 6585 2094 8024 5307 680 715 2845 782 5468 5143 6395 2141 8024 3221 2146 3378 2496 1921 704 1286 6435 715 2845 782 1469 3366 2797 1600 6983 1400 8024 3241 677 2798 1355 4638 6585 2094 8013 3315 782 679 1343 6374 6389 1184 753 1921 4638 715 2845 8024 4685 928 2600 2495 833 3300 6432 3791 4638 8013 791 1921 671 4692 3177 1059 1092 8109 2399 122 3299 123 3189 2141 1399 715 2845 677 7942 7252 2146 1744 2798 4638 6585 2094 8020 793 6158 7219 2137 4881 3632 6397 6389 8021 2347 5307 3633 1962 671 3146 2399 749 134 9180 114 12158 119 8893 113 112 8184 131 120 120 11412 119 9016 8436 9942 119 8134 120 8243 8383 8370 8631 120 8556 142 8420 8148 120 125 142 11777 9153 8157 142 145 9495 8158 8139 8158 8139 9219 8154 8408 8455 8148 119 9248 136 8604 112 114 132 107 8281 11713 134 107 8898 113 8554 119 9594 9852 10958 12672 8199 135 112 9180 112 114 8554 119 8541 12672 8199 134 112 9180 112 132 107 12109 8177 134 107 8184 131 120 120 11412 119 9016 8436 9942 119 8134 120 8243 8383 8370 8631 120 8556 142 8420 8148 120 125 142 11777 9153 8157 142 145 9495 8158 8139 8158 8139 9219 8154 8408 8455 8148 119 9248 136 8604 107 8969 134 107 8621 118 8541 12672 8199 131 9180 10605 132 107 120 135 1745 4275 131 120 8563 120 9266 12708 8383 120 8173 120 9000 120 9908 8187 120 159 12043 12569 11355 8168 120 125 142 11777 9153 8157 142 9564 9500 9313 9050 8717 9136 9102 8144 119 9248 3177 1059 1092 2141 1399 715 2845 8145 1921 1400 8024 677 7942 7252 1054 1999 3124 2424 1726 1908 1963 678 1745 8038 134 9180 114 12158 119 8893 113 112 8184 131 120 120 11412 119 9016 8436 9942 119 8134 120 8243 8383 8370 8631 120 8556 142 8420 8148 120 125 142 11777 9153 8157 142 11094 8204 8452 8204 8161 10073 8144 8204 8148 8328 8160 119 9248 136 8192 112 114 132 107 8281 11713 134 107 8898 113 8554 119 9594 9852 10958 12672 8199 135 112 9180 112 114 8554 119 8541 12672 8199 134 112 9180 112 132 107 12109 8177 134 107 8184 131 120 120 11412 119 9016 8436 9942 119 8134 120 8243 8383 8370 8631 120 8556 142 8420 8148 120 125 142 11777 9153 8157 142 11094 8204 8452 8204 8161 10073 8144 8204 8148 8328 8160 119 9248 136 8192 107 8969 134 107 8621 118 8541 12672 8199 131 9180 10605 132 107 120 135 1745 4275 131 120 8563 120 102\n",
      "11/09/2019 10:51:12 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/09/2019 10:51:12 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/09/2019 10:51:12 - INFO - __main__ -   label: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/09/2019 10:52:35 - INFO - __main__ -   ***** Running training *****\n",
      "11/09/2019 10:52:35 - INFO - __main__ -     Num examples = 11755\n",
      "11/09/2019 10:52:35 - INFO - __main__ -     Batch size = 4\n",
      "11/09/2019 10:52:35 - INFO - __main__ -     Num steps = 20000\n",
      "  0%|                                                 | 0/20000 [00:00<?, ?it/s]11/09/2019 10:52:56 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/09/2019 10:52:56 - INFO - __main__ -     Num examples = 2941\n",
      "11/09/2019 10:52:56 - INFO - __main__ -     Batch size = 32\n",
      "11/09/2019 10:55:04 - INFO - __main__ -     eval_F1 = 0.23479684663723754\n",
      "11/09/2019 10:55:04 - INFO - __main__ -     eval_loss = 1.0928309430246768\n",
      "11/09/2019 10:55:04 - INFO - __main__ -     global_step = 0\n",
      "================================================================================\n",
      "Best F1 0.23479684663723754\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.8157:   4%|█                         | 799/20000 [04:43<58:11,  5.50it/s]11/09/2019 10:57:18 - INFO - __main__ -   ***** Report result *****\n",
      "11/09/2019 10:57:18 - INFO - __main__ -     global_step = 200\n",
      "11/09/2019 10:57:18 - INFO - __main__ -     train loss = 0.8157\n",
      "loss 0.5446:   8%|█▉                       | 1599/20000 [06:56<55:54,  5.49it/s]11/09/2019 10:59:31 - INFO - __main__ -   ***** Report result *****\n",
      "11/09/2019 10:59:31 - INFO - __main__ -     global_step = 400\n",
      "11/09/2019 10:59:31 - INFO - __main__ -     train loss = 0.5446\n",
      "loss 0.4738:  12%|██▉                      | 2399/20000 [09:09<53:52,  5.44it/s]11/09/2019 11:01:44 - INFO - __main__ -   ***** Report result *****\n",
      "11/09/2019 11:01:44 - INFO - __main__ -     global_step = 600\n",
      "11/09/2019 11:01:44 - INFO - __main__ -     train loss = 0.4738\n",
      "loss 0.4446:  16%|███▉                     | 3199/20000 [11:27<54:08,  5.17it/s]11/09/2019 11:04:02 - INFO - __main__ -   ***** Report result *****\n",
      "11/09/2019 11:04:02 - INFO - __main__ -     global_step = 800\n",
      "11/09/2019 11:04:02 - INFO - __main__ -     train loss = 0.4446\n",
      "loss 0.3965:  20%|████▉                    | 3999/20000 [13:45<50:11,  5.31it/s]11/09/2019 11:06:20 - INFO - __main__ -   ***** Report result *****\n",
      "11/09/2019 11:06:20 - INFO - __main__ -     global_step = 1000\n",
      "11/09/2019 11:06:20 - INFO - __main__ -     train loss = 0.3965\n",
      "loss 0.4505:  24%|█████▉                   | 4799/20000 [16:03<47:59,  5.28it/s]11/09/2019 11:08:39 - INFO - __main__ -   ***** Report result *****\n",
      "11/09/2019 11:08:39 - INFO - __main__ -     global_step = 1200\n",
      "11/09/2019 11:08:39 - INFO - __main__ -     train loss = 0.4505\n",
      "loss 0.3901:  28%|██████▉                  | 5599/20000 [18:22<45:01,  5.33it/s]11/09/2019 11:10:57 - INFO - __main__ -   ***** Report result *****\n",
      "11/09/2019 11:10:57 - INFO - __main__ -     global_step = 1400\n",
      "11/09/2019 11:10:57 - INFO - __main__ -     train loss = 0.3901\n",
      "loss 0.3967:  32%|███████▉                 | 6399/20000 [20:39<42:55,  5.28it/s]11/09/2019 11:13:14 - INFO - __main__ -   ***** Report result *****\n",
      "11/09/2019 11:13:14 - INFO - __main__ -     global_step = 1600\n",
      "11/09/2019 11:13:14 - INFO - __main__ -     train loss = 0.3967\n",
      "loss 0.3963:  36%|████████▉                | 7199/20000 [22:51<38:46,  5.50it/s]11/09/2019 11:15:26 - INFO - __main__ -   ***** Report result *****\n",
      "11/09/2019 11:15:26 - INFO - __main__ -     global_step = 1800\n",
      "11/09/2019 11:15:26 - INFO - __main__ -     train loss = 0.3963\n",
      "11/09/2019 11:15:49 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/09/2019 11:15:49 - INFO - __main__ -     Num examples = 2941\n",
      "11/09/2019 11:15:49 - INFO - __main__ -     Batch size = 32\n",
      "11/09/2019 11:17:58 - INFO - __main__ -     eval_F1 = 0.7929674698228025\n",
      "11/09/2019 11:17:58 - INFO - __main__ -     eval_loss = 0.3772574842138135\n",
      "11/09/2019 11:17:58 - INFO - __main__ -     global_step = 1800\n",
      "11/09/2019 11:17:58 - INFO - __main__ -     loss = 0.3963\n",
      "================================================================================\n",
      "Best F1 0.7929674698228025\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.3433:  40%|█████████▉               | 7999/20000 [27:39<36:39,  5.46it/s]11/09/2019 11:20:15 - INFO - __main__ -   ***** Report result *****\n",
      "11/09/2019 11:20:15 - INFO - __main__ -     global_step = 2000\n",
      "11/09/2019 11:20:15 - INFO - __main__ -     train loss = 0.3433\n",
      "11/09/2019 11:20:38 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/09/2019 11:20:38 - INFO - __main__ -     Num examples = 2941\n",
      "11/09/2019 11:20:38 - INFO - __main__ -     Batch size = 32\n",
      "11/09/2019 11:22:47 - INFO - __main__ -     eval_F1 = 0.7679883275045131\n",
      "11/09/2019 11:22:47 - INFO - __main__ -     eval_loss = 0.36007601808270684\n",
      "11/09/2019 11:22:47 - INFO - __main__ -     global_step = 2000\n",
      "11/09/2019 11:22:47 - INFO - __main__ -     loss = 0.3433\n",
      "================================================================================\n",
      "loss 0.3744:  44%|██████████▉              | 8799/20000 [32:25<33:54,  5.51it/s]11/09/2019 11:25:00 - INFO - __main__ -   ***** Report result *****\n",
      "11/09/2019 11:25:00 - INFO - __main__ -     global_step = 2200\n",
      "11/09/2019 11:25:00 - INFO - __main__ -     train loss = 0.3744\n",
      "11/09/2019 11:25:23 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/09/2019 11:25:23 - INFO - __main__ -     Num examples = 2941\n",
      "11/09/2019 11:25:23 - INFO - __main__ -     Batch size = 32\n",
      "11/09/2019 11:27:32 - INFO - __main__ -     eval_F1 = 0.7927792942846322\n",
      "11/09/2019 11:27:32 - INFO - __main__ -     eval_loss = 0.35028713650029636\n",
      "11/09/2019 11:27:32 - INFO - __main__ -     global_step = 2200\n",
      "11/09/2019 11:27:32 - INFO - __main__ -     loss = 0.3744\n",
      "================================================================================\n",
      "loss 0.349:  48%|████████████▍             | 9599/20000 [37:09<31:15,  5.55it/s]11/09/2019 11:29:44 - INFO - __main__ -   ***** Report result *****\n",
      "11/09/2019 11:29:44 - INFO - __main__ -     global_step = 2400\n",
      "11/09/2019 11:29:44 - INFO - __main__ -     train loss = 0.349\n",
      "11/09/2019 11:30:07 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/09/2019 11:30:07 - INFO - __main__ -     Num examples = 2941\n",
      "11/09/2019 11:30:07 - INFO - __main__ -     Batch size = 32\n",
      "11/09/2019 11:32:16 - INFO - __main__ -     eval_F1 = 0.8098462255994542\n",
      "11/09/2019 11:32:16 - INFO - __main__ -     eval_loss = 0.34934671283902036\n",
      "11/09/2019 11:32:16 - INFO - __main__ -     global_step = 2400\n",
      "11/09/2019 11:32:16 - INFO - __main__ -     loss = 0.349\n",
      "================================================================================\n",
      "Best F1 0.8098462255994542\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.3738:  52%|████████████▍           | 10399/20000 [41:57<29:14,  5.47it/s]11/09/2019 11:34:33 - INFO - __main__ -   ***** Report result *****\n",
      "11/09/2019 11:34:33 - INFO - __main__ -     global_step = 2600\n",
      "11/09/2019 11:34:33 - INFO - __main__ -     train loss = 0.3738\n",
      "11/09/2019 11:34:55 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/09/2019 11:34:55 - INFO - __main__ -     Num examples = 2941\n",
      "11/09/2019 11:34:55 - INFO - __main__ -     Batch size = 32\n",
      "11/09/2019 11:37:05 - INFO - __main__ -     eval_F1 = 0.8025148464584827\n",
      "11/09/2019 11:37:05 - INFO - __main__ -     eval_loss = 0.34858476921268133\n",
      "11/09/2019 11:37:05 - INFO - __main__ -     global_step = 2600\n",
      "11/09/2019 11:37:05 - INFO - __main__ -     loss = 0.3738\n",
      "================================================================================\n",
      "loss 0.381:  56%|█████████████▉           | 11199/20000 [46:42<26:44,  5.49it/s]11/09/2019 11:39:17 - INFO - __main__ -   ***** Report result *****\n",
      "11/09/2019 11:39:17 - INFO - __main__ -     global_step = 2800\n",
      "11/09/2019 11:39:17 - INFO - __main__ -     train loss = 0.381\n",
      "11/09/2019 11:39:40 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/09/2019 11:39:40 - INFO - __main__ -     Num examples = 2941\n",
      "11/09/2019 11:39:40 - INFO - __main__ -     Batch size = 32\n",
      "11/09/2019 11:41:49 - INFO - __main__ -     eval_F1 = 0.8033415562290206\n",
      "11/09/2019 11:41:49 - INFO - __main__ -     eval_loss = 0.35871861061162275\n",
      "11/09/2019 11:41:49 - INFO - __main__ -     global_step = 2800\n",
      "11/09/2019 11:41:49 - INFO - __main__ -     loss = 0.381\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.3323:  60%|██████████████▍         | 11999/20000 [51:28<24:25,  5.46it/s]11/09/2019 11:44:03 - INFO - __main__ -   ***** Report result *****\n",
      "11/09/2019 11:44:03 - INFO - __main__ -     global_step = 3000\n",
      "11/09/2019 11:44:03 - INFO - __main__ -     train loss = 0.3323\n",
      "11/09/2019 11:44:25 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/09/2019 11:44:25 - INFO - __main__ -     Num examples = 2941\n",
      "11/09/2019 11:44:25 - INFO - __main__ -     Batch size = 32\n",
      "11/09/2019 11:46:35 - INFO - __main__ -     eval_F1 = 0.7983466470137479\n",
      "11/09/2019 11:46:35 - INFO - __main__ -     eval_loss = 0.3597585104972772\n",
      "11/09/2019 11:46:35 - INFO - __main__ -     global_step = 3000\n",
      "11/09/2019 11:46:35 - INFO - __main__ -     loss = 0.3323\n",
      "================================================================================\n",
      "loss 0.2986:  64%|███████████████▎        | 12799/20000 [56:13<21:58,  5.46it/s]11/09/2019 11:48:48 - INFO - __main__ -   ***** Report result *****\n",
      "11/09/2019 11:48:48 - INFO - __main__ -     global_step = 3200\n",
      "11/09/2019 11:48:48 - INFO - __main__ -     train loss = 0.2986\n",
      "11/09/2019 11:49:11 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/09/2019 11:49:11 - INFO - __main__ -     Num examples = 2941\n",
      "11/09/2019 11:49:11 - INFO - __main__ -     Batch size = 32\n",
      "11/09/2019 11:51:21 - INFO - __main__ -     eval_F1 = 0.7791319283406534\n",
      "11/09/2019 11:51:21 - INFO - __main__ -     eval_loss = 0.34533757190017594\n",
      "11/09/2019 11:51:21 - INFO - __main__ -     global_step = 3200\n",
      "11/09/2019 11:51:21 - INFO - __main__ -     loss = 0.2986\n",
      "================================================================================\n",
      "loss 0.2848:  68%|██████████████▉       | 13599/20000 [1:00:58<19:31,  5.46it/s]11/09/2019 11:53:33 - INFO - __main__ -   ***** Report result *****\n",
      "11/09/2019 11:53:33 - INFO - __main__ -     global_step = 3400\n",
      "11/09/2019 11:53:33 - INFO - __main__ -     train loss = 0.2848\n",
      "11/09/2019 11:53:56 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/09/2019 11:53:56 - INFO - __main__ -     Num examples = 2941\n",
      "11/09/2019 11:53:56 - INFO - __main__ -     Batch size = 32\n"
     ]
    }
   ],
   "source": [
    "# base:  首尾\n",
    "!python ./run_bert_2562.py \\\n",
    "--model_type bert \\\n",
    "--model_name_or_path ./chinese_roberta_wwm_large_ext_pytorch  \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_test \\\n",
    "--data_dir ../data/data_StratifiedKFold_42/data_origin_0 \\\n",
    "--output_dir ../model/roberta_wwm_large_512_gru1L_42/roberta_wwm_large_512_gru1L_42_0 \\\n",
    "--max_seq_length 512 \\\n",
    "--split_num 1 \\\n",
    "--lstm_hidden_size 512 \\\n",
    "--lstm_layers 2 \\\n",
    "--lstm_dropout 0.1 \\\n",
    "--eval_steps 200 \\\n",
    "--per_gpu_train_batch_size 4 \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--warmup_steps 0 \\\n",
    "--per_gpu_eval_batch_size 32 \\\n",
    "--learning_rate 5e-6 \\\n",
    "--adam_epsilon 1e-6 \\\n",
    "--weight_decay 0 \\\n",
    "--train_steps 20000 \\\n",
    "--freeze 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base:  首尾\n",
    "!python ../CCF-BDCI-Sentiment-Analysis-Baseline/run_bert_2562.py \\\n",
    "--model_type bert \\\n",
    "--model_name_or_path ../CCF-BDCI-Sentiment-Analysis-Baseline/chinese_roberta_wwm_large_ext_pytorch  \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_test \\\n",
    "--data_dir ./data/data_StratifiedKFold_42/data_origin_1 \\\n",
    "--output_dir ./model/roberta_wwm_large_512_gru1L_42/roberta_wwm_large_512_gru1L_42_1 \\\n",
    "--max_seq_length 512 \\\n",
    "--split_num 1 \\\n",
    "--lstm_hidden_size 512 \\\n",
    "--lstm_layers 2 \\\n",
    "--lstm_dropout 0.1 \\\n",
    "--eval_steps 200 \\\n",
    "--per_gpu_train_batch_size 4 \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--warmup_steps 0 \\\n",
    "--per_gpu_eval_batch_size 32 \\\n",
    "--learning_rate 5e-6 \\\n",
    "--adam_epsilon 1e-6 \\\n",
    "--weight_decay 0 \\\n",
    "--train_steps 20000 \\\n",
    "--freeze 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base:  首尾\n",
    "!python ../CCF-BDCI-Sentiment-Analysis-Baseline/run_bert_2562.py \\\n",
    "--model_type bert \\\n",
    "--model_name_or_path ../CCF-BDCI-Sentiment-Analysis-Baseline/chinese_roberta_wwm_large_ext_pytorch  \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_test \\\n",
    "--data_dir ./data/data_StratifiedKFold_42/data_origin_2 \\\n",
    "--output_dir ./model/roberta_wwm_large_512_gru1L_42/roberta_wwm_large_512_gru1L_42_2 \\\n",
    "--max_seq_length 512 \\\n",
    "--split_num 1 \\\n",
    "--lstm_hidden_size 512 \\\n",
    "--lstm_layers 2 \\\n",
    "--lstm_dropout 0.1 \\\n",
    "--eval_steps 200 \\\n",
    "--per_gpu_train_batch_size 4 \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--warmup_steps 0 \\\n",
    "--per_gpu_eval_batch_size 32 \\\n",
    "--learning_rate 5e-6 \\\n",
    "--adam_epsilon 1e-6 \\\n",
    "--weight_decay 0 \\\n",
    "--train_steps 20000 \\\n",
    "--freeze 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base:  首尾\n",
    "!python ../CCF-BDCI-Sentiment-Analysis-Baseline/run_bert_2562.py \\\n",
    "--model_type bert \\\n",
    "--model_name_or_path ../CCF-BDCI-Sentiment-Analysis-Baseline/chinese_roberta_wwm_large_ext_pytorch  \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_test \\\n",
    "--data_dir ./data/data_StratifiedKFold_42/data_origin_3 \\\n",
    "--output_dir ./model/roberta_wwm_large_512_gru1L_42/roberta_wwm_large_512_gru1L_42_3 \\\n",
    "--max_seq_length 512 \\\n",
    "--split_num 1 \\\n",
    "--lstm_hidden_size 512 \\\n",
    "--lstm_layers 2 \\\n",
    "--lstm_dropout 0.1 \\\n",
    "--eval_steps 200 \\\n",
    "--per_gpu_train_batch_size 4 \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--warmup_steps 0 \\\n",
    "--per_gpu_eval_batch_size 32 \\\n",
    "--learning_rate 5e-6 \\\n",
    "--adam_epsilon 1e-6 \\\n",
    "--weight_decay 0 \\\n",
    "--train_steps 20000 \\\n",
    "--freeze 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base:  首尾\n",
    "!python ../CCF-BDCI-Sentiment-Analysis-Baseline/run_bert_2562.py \\\n",
    "--model_type bert \\\n",
    "--model_name_or_path ../CCF-BDCI-Sentiment-Analysis-Baseline/chinese_roberta_wwm_large_ext_pytorch  \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_test \\\n",
    "--data_dir ./data/data_StratifiedKFold_42/data_origin_4 \\\n",
    "--output_dir ./model/roberta_wwm_large_512_gru1L_42/roberta_wwm_large_512_gru1L_42_4 \\\n",
    "--max_seq_length 512 \\\n",
    "--split_num 1 \\\n",
    "--lstm_hidden_size 512 \\\n",
    "--lstm_layers 2 \\\n",
    "--lstm_dropout 0.1 \\\n",
    "--eval_steps 200 \\\n",
    "--per_gpu_train_batch_size 4 \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--warmup_steps 0 \\\n",
    "--per_gpu_eval_batch_size 32 \\\n",
    "--learning_rate 5e-6 \\\n",
    "--adam_epsilon 1e-6 \\\n",
    "--weight_decay 0 \\\n",
    "--train_steps 20000 \\\n",
    "--freeze 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
