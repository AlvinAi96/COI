{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base:  首尾\n",
    "!python ./run_bert_2562.py \\\n",
    "--model_type bert \\\n",
    "--model_name_or_path ../model/chinese_roberta_wwm_large_ext_pytorch  \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_test \\\n",
    "--data_dir ../data/data_StratifiedKFold_42/data_origin_0 \\\n",
    "--output_dir ../model/roberta_wwm_large_3002_gru1_42/roberta_wwm_large_3002_gru1_0 \\\n",
    "--max_seq_length 300 \\\n",
    "--split_num 2 \\\n",
    "--lstm_hidden_size 512 \\\n",
    "--lstm_layers 1 \\\n",
    "    \n",
    "--lstm_dropout 0.1 \\\n",
    "--eval_steps 200 \\\n",
    "--per_gpu_train_batch_size 4 \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--warmup_steps 0 \\\n",
    "--per_gpu_eval_batch_size 32 \\\n",
    "--learning_rate 5e-6 \\\n",
    "--adam_epsilon 1e-6 \\\n",
    "--weight_decay 0 \\\n",
    "--train_steps 20000 \\\n",
    "--freeze 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/10/2019 20:41:43 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "11/10/2019 20:41:43 - INFO - pytorch_transformers.tokenization_utils -   Model name '../model/chinese_roberta_wwm_large_ext_pytorch' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming '../model/chinese_roberta_wwm_large_ext_pytorch' is a path or url to a directory containing tokenizer files.\n",
      "11/10/2019 20:41:43 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../model/chinese_roberta_wwm_large_ext_pytorch/added_tokens.json. We won't load it.\n",
      "11/10/2019 20:41:43 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../model/chinese_roberta_wwm_large_ext_pytorch/special_tokens_map.json. We won't load it.\n",
      "11/10/2019 20:41:43 - INFO - pytorch_transformers.tokenization_utils -   loading file ../model/chinese_roberta_wwm_large_ext_pytorch/vocab.txt\n",
      "11/10/2019 20:41:43 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/10/2019 20:41:43 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/10/2019 20:41:43 - INFO - pytorch_transformers.modeling_utils -   loading configuration file ../model/chinese_roberta_wwm_large_ext_pytorch/config.json\n",
      "11/10/2019 20:41:43 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 3,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "11/10/2019 20:41:43 - INFO - pytorch_transformers.modeling_utils -   loading weights file ../model/chinese_roberta_wwm_large_ext_pytorch/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "11/10/2019 20:41:57 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'W.0.weight', 'W.0.bias', 'gru.0.weight_ih_l0', 'gru.0.weight_hh_l0', 'gru.0.bias_ih_l0', 'gru.0.bias_hh_l0', 'gru.0.weight_ih_l0_reverse', 'gru.0.weight_hh_l0_reverse', 'gru.0.bias_ih_l0_reverse', 'gru.0.bias_hh_l0_reverse']\n",
      "11/10/2019 20:41:57 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "11/10/2019 20:41:58 - INFO - __main__ -   *** Example ***\n",
      "11/10/2019 20:41:58 - INFO - __main__ -   idx: 0\n",
      "11/10/2019 20:41:58 - INFO - __main__ -   guid: 7a3dd79f90ee419da87190cff60f7a86\n",
      "11/10/2019 20:41:58 - INFO - __main__ -   tokens: [CLS] 问 责 领 导 ( 上 黄 镇 党 委 书 记 张 涛 ， 宣 国 才 真 能 一 手 遮 天 吗 ？ ) [SEP] 这 几 天 看 了 有 人 举 报 施 某 某 的 贴 子 ， 经 与 举 报 人 联 系 证 实 ， 是 宣 某 当 天 中 午 请 举 报 人 和 枪 手 喝 酒 后 ， 晚 上 才 发 的 贴 子 ！ 本 人 不 去 讨 论 前 二 天 的 举 报 ， 相 信 总 归 会 有 说 法 的 ！ 今 天 一 看 施 全 军 2017 年 1 月 2 日 实 名 举 报 上 黄 镇 宣 国 才 的 贴 子 （ 仍 被 锁 定 禁 止 评 论 ） 已 经 正 好 一 整 年 了 = 750 ) window . open ( ' http : / / img . js ##ly ##001 . com / at ##ta ##ch ##ment / mon _ 180 ##1 / 4 _ 291 ##08 ##5 _ c ##79 ##6 ##a ##6 ##a ##86 ##e ##17 ##12 ##1 . jpg ? 123 ' ) ; \" on ##load = \" if ( this . off ##set ##wi ##dt ##h > ' 750 ' ) this . wi ##dt ##h = ' 750 ' ; \" sr ##c = \" http : / / img . js ##ly ##001 . com / at ##ta ##ch ##ment / mon _ 180 ##1 / 4 _ 291 ##08 ##5 _ c ##79 ##6 ##a ##6 ##a ##86 ##e ##17 ##12 ##1 . jpg ? 123 \" style = \" max - wi ##dt ##h : 750 ##px ; \" / > 图 片 : / [SEP]\n",
      "11/10/2019 20:41:58 - INFO - __main__ -   input_ids: 101 7309 6569 7566 2193 113 677 7942 7252 1054 1999 741 6381 2476 3875 8024 2146 1744 2798 4696 5543 671 2797 6902 1921 1408 8043 114 102 6821 1126 1921 4692 749 3300 782 715 2845 3177 3378 3378 4638 6585 2094 8024 5307 680 715 2845 782 5468 5143 6395 2141 8024 3221 2146 3378 2496 1921 704 1286 6435 715 2845 782 1469 3366 2797 1600 6983 1400 8024 3241 677 2798 1355 4638 6585 2094 8013 3315 782 679 1343 6374 6389 1184 753 1921 4638 715 2845 8024 4685 928 2600 2495 833 3300 6432 3791 4638 8013 791 1921 671 4692 3177 1059 1092 8109 2399 122 3299 123 3189 2141 1399 715 2845 677 7942 7252 2146 1744 2798 4638 6585 2094 8020 793 6158 7219 2137 4881 3632 6397 6389 8021 2347 5307 3633 1962 671 3146 2399 749 134 9180 114 12158 119 8893 113 112 8184 131 120 120 11412 119 9016 8436 9942 119 8134 120 8243 8383 8370 8631 120 8556 142 8420 8148 120 125 142 11777 9153 8157 142 145 9495 8158 8139 8158 8139 9219 8154 8408 8455 8148 119 9248 136 8604 112 114 132 107 8281 11713 134 107 8898 113 8554 119 9594 9852 10958 12672 8199 135 112 9180 112 114 8554 119 8541 12672 8199 134 112 9180 112 132 107 12109 8177 134 107 8184 131 120 120 11412 119 9016 8436 9942 119 8134 120 8243 8383 8370 8631 120 8556 142 8420 8148 120 125 142 11777 9153 8157 142 145 9495 8158 8139 8158 8139 9219 8154 8408 8455 8148 119 9248 136 8604 107 8969 134 107 8621 118 8541 12672 8199 131 9180 10605 132 107 120 135 1745 4275 131 120 102\n",
      "11/10/2019 20:41:58 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/10/2019 20:41:58 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/10/2019 20:41:58 - INFO - __main__ -   label: 2\n",
      "11/10/2019 20:41:58 - INFO - __main__ -   *** Example ***\n",
      "11/10/2019 20:41:58 - INFO - __main__ -   idx: 0\n",
      "11/10/2019 20:41:58 - INFO - __main__ -   guid: 7a3dd79f90ee419da87190cff60f7a86\n",
      "11/10/2019 20:41:58 - INFO - __main__ -   tokens: [CLS] 问 责 领 导 ( 上 黄 镇 党 委 书 记 张 涛 ， 宣 国 才 真 能 一 手 遮 天 吗 ？ ) [SEP] / www / data / tm ##p / q ##fu ##pl ##oa ##d / 4 _ 291 ##08 ##5 _ 151 ##49 ##81 ##47 ##23 ##53 ##07 ##5 . jpg 一 年 的 贴 子 ， 再 次 被 网 友 顶 起 来 后 ， 才 发 现 施 某 几 天 前 回 复 网 友 的 处 理 结 果 竟 如 下 图 ： = 750 ) window . open ( ' http : / / img . js ##ly ##001 . com / at ##ta ##ch ##ment / mon _ 180 ##1 / 4 _ 291 ##08 ##5 _ 9 ##d ##32 ##ee ##57 ##27 ##60 ##d ##85 . jpg ? 131 ' ) ; \" on ##load = \" if ( this . off ##set ##wi ##dt ##h > ' 750 ' ) this . wi ##dt ##h = ' 750 ' ; \" sr ##c = \" http : / / img . js ##ly ##001 . com / at ##ta ##ch ##ment / mon _ 180 ##1 / 4 _ 291 ##08 ##5 _ 9 ##d ##32 ##ee ##57 ##27 ##60 ##d ##85 . jpg ? 131 \" style = \" max - wi ##dt ##h : 750 ##px ; \" / > 图 片 : / home / al ##ida ##ta / www / data / tm ##p / q ##fu ##pl ##oa ##d / 4 _ 291 ##08 ##5 _ 151 ##49 ##81 ##47 ##35 ##47 ##17 ##2 . jpg 现 责 问 张 涛 书 记 ： 1 、 宣 国 才 被 举 报 这 么 多 问 [SEP]\n",
      "11/10/2019 20:41:58 - INFO - __main__ -   input_ids: 101 7309 6569 7566 2193 113 677 7942 7252 1054 1999 741 6381 2476 3875 8024 2146 1744 2798 4696 5543 671 2797 6902 1921 1408 8043 114 102 120 8173 120 9000 120 9908 8187 120 159 12043 12569 11355 8168 120 125 142 11777 9153 8157 142 9564 9500 9313 9050 8748 9310 9131 8157 119 9248 671 2399 4638 6585 2094 8024 1086 3613 6158 5381 1351 7553 6629 3341 1400 8024 2798 1355 4385 3177 3378 1126 1921 1184 1726 1908 5381 1351 4638 1905 4415 5310 3362 4994 1963 678 1745 8038 134 9180 114 12158 119 8893 113 112 8184 131 120 120 11412 119 9016 8436 9942 119 8134 120 8243 8383 8370 8631 120 8556 142 8420 8148 120 125 142 11777 9153 8157 142 130 8168 8709 8854 9647 8976 8581 8168 9169 119 9248 136 9403 112 114 132 107 8281 11713 134 107 8898 113 8554 119 9594 9852 10958 12672 8199 135 112 9180 112 114 8554 119 8541 12672 8199 134 112 9180 112 132 107 12109 8177 134 107 8184 131 120 120 11412 119 9016 8436 9942 119 8134 120 8243 8383 8370 8631 120 8556 142 8420 8148 120 125 142 11777 9153 8157 142 130 8168 8709 8854 9647 8976 8581 8168 9169 119 9248 136 9403 107 8969 134 107 8621 118 8541 12672 8199 131 9180 10605 132 107 120 135 1745 4275 131 120 8563 120 9266 12708 8383 120 8173 120 9000 120 9908 8187 120 159 12043 12569 11355 8168 120 125 142 11777 9153 8157 142 9564 9500 9313 9050 8852 9050 8408 8144 119 9248 4385 6569 7309 2476 3875 741 6381 8038 122 510 2146 1744 2798 6158 715 2845 6821 720 1914 7309 102\n",
      "11/10/2019 20:41:58 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/10/2019 20:41:58 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/10/2019 20:41:58 - INFO - __main__ -   label: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/10/2019 20:43:30 - INFO - __main__ -   ***** Running training *****\n",
      "11/10/2019 20:43:30 - INFO - __main__ -     Num examples = 11756\n",
      "11/10/2019 20:43:30 - INFO - __main__ -     Batch size = 4\n",
      "11/10/2019 20:43:30 - INFO - __main__ -     Num steps = 20000\n",
      "  0%|                                                 | 0/20000 [00:00<?, ?it/s]11/10/2019 20:43:51 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/10/2019 20:43:51 - INFO - __main__ -     Num examples = 2940\n",
      "11/10/2019 20:43:51 - INFO - __main__ -     Batch size = 32\n",
      "/root/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "11/10/2019 20:46:09 - INFO - __main__ -     eval_F1 = 0.06324671257734495\n",
      "11/10/2019 20:46:09 - INFO - __main__ -     eval_loss = 1.1868648684543113\n",
      "11/10/2019 20:46:09 - INFO - __main__ -     global_step = 0\n",
      "================================================================================\n",
      "Best F1 0.06324671257734495\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.7613:   4%|▉                       | 799/20000 [05:12<1:06:30,  4.81it/s]11/10/2019 20:48:42 - INFO - __main__ -   ***** Report result *****\n",
      "11/10/2019 20:48:42 - INFO - __main__ -     global_step = 200\n",
      "11/10/2019 20:48:42 - INFO - __main__ -     train loss = 0.7613\n",
      "loss 0.4764:   8%|█▊                     | 1599/20000 [07:45<1:04:02,  4.79it/s]11/10/2019 20:51:15 - INFO - __main__ -   ***** Report result *****\n",
      "11/10/2019 20:51:15 - INFO - __main__ -     global_step = 400\n",
      "11/10/2019 20:51:15 - INFO - __main__ -     train loss = 0.4764\n",
      "loss 0.4265:  12%|██▊                    | 2399/20000 [10:18<1:01:11,  4.79it/s]11/10/2019 20:53:48 - INFO - __main__ -   ***** Report result *****\n",
      "11/10/2019 20:53:48 - INFO - __main__ -     global_step = 600\n",
      "11/10/2019 20:53:48 - INFO - __main__ -     train loss = 0.4265\n",
      "loss 0.4134:  16%|███▉                     | 3199/20000 [12:51<58:44,  4.77it/s]11/10/2019 20:56:22 - INFO - __main__ -   ***** Report result *****\n",
      "11/10/2019 20:56:22 - INFO - __main__ -     global_step = 800\n",
      "11/10/2019 20:56:22 - INFO - __main__ -     train loss = 0.4134\n",
      "loss 0.4255:  20%|████▉                    | 3999/20000 [15:25<55:21,  4.82it/s]11/10/2019 20:58:55 - INFO - __main__ -   ***** Report result *****\n",
      "11/10/2019 20:58:55 - INFO - __main__ -     global_step = 1000\n",
      "11/10/2019 20:58:55 - INFO - __main__ -     train loss = 0.4255\n",
      "loss 0.4001:  24%|█████▉                   | 4799/20000 [17:58<52:38,  4.81it/s]11/10/2019 21:01:28 - INFO - __main__ -   ***** Report result *****\n",
      "11/10/2019 21:01:28 - INFO - __main__ -     global_step = 1200\n",
      "11/10/2019 21:01:28 - INFO - __main__ -     train loss = 0.4001\n",
      "loss 0.3892:  28%|██████▉                  | 5599/20000 [20:31<50:09,  4.78it/s]11/10/2019 21:04:01 - INFO - __main__ -   ***** Report result *****\n",
      "11/10/2019 21:04:01 - INFO - __main__ -     global_step = 1400\n",
      "11/10/2019 21:04:01 - INFO - __main__ -     train loss = 0.3892\n",
      "loss 0.3825:  32%|███████▉                 | 6399/20000 [23:04<47:12,  4.80it/s]11/10/2019 21:06:34 - INFO - __main__ -   ***** Report result *****\n",
      "11/10/2019 21:06:34 - INFO - __main__ -     global_step = 1600\n",
      "11/10/2019 21:06:34 - INFO - __main__ -     train loss = 0.3825\n",
      "loss 0.3543:  36%|████████▉                | 7199/20000 [25:37<44:31,  4.79it/s]11/10/2019 21:09:07 - INFO - __main__ -   ***** Report result *****\n",
      "11/10/2019 21:09:07 - INFO - __main__ -     global_step = 1800\n",
      "11/10/2019 21:09:07 - INFO - __main__ -     train loss = 0.3543\n",
      "11/10/2019 21:09:28 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/10/2019 21:09:28 - INFO - __main__ -     Num examples = 2940\n",
      "11/10/2019 21:09:28 - INFO - __main__ -     Batch size = 32\n",
      "11/10/2019 21:11:47 - INFO - __main__ -     eval_F1 = 0.7821142699625554\n",
      "11/10/2019 21:11:47 - INFO - __main__ -     eval_loss = 0.3572513594737519\n",
      "11/10/2019 21:11:47 - INFO - __main__ -     global_step = 1800\n",
      "11/10/2019 21:11:47 - INFO - __main__ -     loss = 0.3543\n",
      "================================================================================\n",
      "Best F1 0.7821142699625554\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.3797:  40%|█████████▉               | 7999/20000 [30:53<41:22,  4.83it/s]11/10/2019 21:14:24 - INFO - __main__ -   ***** Report result *****\n",
      "11/10/2019 21:14:24 - INFO - __main__ -     global_step = 2000\n",
      "11/10/2019 21:14:24 - INFO - __main__ -     train loss = 0.3797\n",
      "11/10/2019 21:14:43 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/10/2019 21:14:43 - INFO - __main__ -     Num examples = 2940\n",
      "11/10/2019 21:14:43 - INFO - __main__ -     Batch size = 32\n",
      "11/10/2019 21:17:03 - INFO - __main__ -     eval_F1 = 0.7817812435845655\n",
      "11/10/2019 21:17:03 - INFO - __main__ -     eval_loss = 0.3662859589347373\n",
      "11/10/2019 21:17:03 - INFO - __main__ -     global_step = 2000\n",
      "11/10/2019 21:17:03 - INFO - __main__ -     loss = 0.3797\n",
      "================================================================================\n",
      "loss 0.3484:  44%|██████████▉              | 8799/20000 [36:05<38:27,  4.85it/s]11/10/2019 21:19:36 - INFO - __main__ -   ***** Report result *****\n",
      "11/10/2019 21:19:36 - INFO - __main__ -     global_step = 2200\n",
      "11/10/2019 21:19:36 - INFO - __main__ -     train loss = 0.3484\n",
      "11/10/2019 21:19:56 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/10/2019 21:19:56 - INFO - __main__ -     Num examples = 2940\n",
      "11/10/2019 21:19:56 - INFO - __main__ -     Batch size = 32\n",
      "11/10/2019 21:22:15 - INFO - __main__ -     eval_F1 = 0.7327921696478397\n",
      "11/10/2019 21:22:15 - INFO - __main__ -     eval_loss = 0.37086561532772105\n",
      "11/10/2019 21:22:15 - INFO - __main__ -     global_step = 2200\n",
      "11/10/2019 21:22:15 - INFO - __main__ -     loss = 0.3484\n",
      "================================================================================\n",
      "loss 0.3911:  48%|███████████▉             | 9599/20000 [41:18<35:46,  4.85it/s]11/10/2019 21:24:48 - INFO - __main__ -   ***** Report result *****\n",
      "11/10/2019 21:24:48 - INFO - __main__ -     global_step = 2400\n",
      "11/10/2019 21:24:48 - INFO - __main__ -     train loss = 0.3911\n",
      "11/10/2019 21:25:07 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/10/2019 21:25:07 - INFO - __main__ -     Num examples = 2940\n",
      "11/10/2019 21:25:07 - INFO - __main__ -     Batch size = 32\n",
      "11/10/2019 21:27:27 - INFO - __main__ -     eval_F1 = 0.7965174910391318\n",
      "11/10/2019 21:27:27 - INFO - __main__ -     eval_loss = 0.3577576099449526\n",
      "11/10/2019 21:27:27 - INFO - __main__ -     global_step = 2400\n",
      "11/10/2019 21:27:27 - INFO - __main__ -     loss = 0.3911\n",
      "================================================================================\n",
      "Best F1 0.7965174910391318\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.337:  52%|████████████▉            | 10399/20000 [46:33<33:07,  4.83it/s]11/10/2019 21:30:03 - INFO - __main__ -   ***** Report result *****\n",
      "11/10/2019 21:30:03 - INFO - __main__ -     global_step = 2600\n",
      "11/10/2019 21:30:03 - INFO - __main__ -     train loss = 0.337\n",
      "11/10/2019 21:30:24 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/10/2019 21:30:24 - INFO - __main__ -     Num examples = 2940\n",
      "11/10/2019 21:30:24 - INFO - __main__ -     Batch size = 32\n",
      "11/10/2019 21:32:43 - INFO - __main__ -     eval_F1 = 0.7776206168756216\n",
      "11/10/2019 21:32:43 - INFO - __main__ -     eval_loss = 0.3453369735861602\n",
      "11/10/2019 21:32:43 - INFO - __main__ -     global_step = 2600\n",
      "11/10/2019 21:32:43 - INFO - __main__ -     loss = 0.337\n",
      "================================================================================\n",
      "loss 0.3146:  56%|█████████████▍          | 11199/20000 [51:46<30:21,  4.83it/s]11/10/2019 21:35:16 - INFO - __main__ -   ***** Report result *****\n",
      "11/10/2019 21:35:16 - INFO - __main__ -     global_step = 2800\n",
      "11/10/2019 21:35:16 - INFO - __main__ -     train loss = 0.3146\n",
      "11/10/2019 21:35:38 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/10/2019 21:35:38 - INFO - __main__ -     Num examples = 2940\n",
      "11/10/2019 21:35:38 - INFO - __main__ -     Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/10/2019 21:37:57 - INFO - __main__ -     eval_F1 = 0.7761975015766275\n",
      "11/10/2019 21:37:57 - INFO - __main__ -     eval_loss = 0.35057072924531024\n",
      "11/10/2019 21:37:57 - INFO - __main__ -     global_step = 2800\n",
      "11/10/2019 21:37:57 - INFO - __main__ -     loss = 0.3146\n",
      "================================================================================\n",
      "loss 0.343:  60%|██████████████▉          | 11999/20000 [57:00<27:41,  4.81it/s]11/10/2019 21:40:31 - INFO - __main__ -   ***** Report result *****\n",
      "11/10/2019 21:40:31 - INFO - __main__ -     global_step = 3000\n",
      "11/10/2019 21:40:31 - INFO - __main__ -     train loss = 0.343\n",
      "11/10/2019 21:40:50 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/10/2019 21:40:50 - INFO - __main__ -     Num examples = 2940\n",
      "11/10/2019 21:40:50 - INFO - __main__ -     Batch size = 32\n",
      "11/10/2019 21:43:10 - INFO - __main__ -     eval_F1 = 0.7938888854929603\n",
      "11/10/2019 21:43:10 - INFO - __main__ -     eval_loss = 0.3471315629456354\n",
      "11/10/2019 21:43:10 - INFO - __main__ -     global_step = 3000\n",
      "11/10/2019 21:43:10 - INFO - __main__ -     loss = 0.343\n",
      "================================================================================\n",
      "loss 0.2695:  64%|██████████████        | 12799/20000 [1:02:13<25:08,  4.77it/s]11/10/2019 21:45:44 - INFO - __main__ -   ***** Report result *****\n",
      "11/10/2019 21:45:44 - INFO - __main__ -     global_step = 3200\n",
      "11/10/2019 21:45:44 - INFO - __main__ -     train loss = 0.2695\n",
      "11/10/2019 21:46:03 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/10/2019 21:46:03 - INFO - __main__ -     Num examples = 2940\n",
      "11/10/2019 21:46:03 - INFO - __main__ -     Batch size = 32\n",
      "11/10/2019 21:48:23 - INFO - __main__ -     eval_F1 = 0.7790869744263834\n",
      "11/10/2019 21:48:23 - INFO - __main__ -     eval_loss = 0.34461004764813447\n",
      "11/10/2019 21:48:23 - INFO - __main__ -     global_step = 3200\n",
      "11/10/2019 21:48:23 - INFO - __main__ -     loss = 0.2695\n",
      "================================================================================\n",
      "loss 0.2625:  68%|██████████████▉       | 13599/20000 [1:07:26<22:20,  4.77it/s]11/10/2019 21:50:56 - INFO - __main__ -   ***** Report result *****\n",
      "11/10/2019 21:50:56 - INFO - __main__ -     global_step = 3400\n",
      "11/10/2019 21:50:56 - INFO - __main__ -     train loss = 0.2625\n",
      "11/10/2019 21:51:16 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/10/2019 21:51:16 - INFO - __main__ -     Num examples = 2940\n",
      "11/10/2019 21:51:16 - INFO - __main__ -     Batch size = 32\n",
      "11/10/2019 21:53:36 - INFO - __main__ -     eval_F1 = 0.7932651451884493\n",
      "11/10/2019 21:53:36 - INFO - __main__ -     eval_loss = 0.349013005990697\n",
      "11/10/2019 21:53:36 - INFO - __main__ -     global_step = 3400\n",
      "11/10/2019 21:53:36 - INFO - __main__ -     loss = 0.2625\n",
      "================================================================================\n",
      "loss 0.2309:  72%|███████████████▊      | 14399/20000 [1:12:39<19:30,  4.78it/s]11/10/2019 21:56:10 - INFO - __main__ -   ***** Report result *****\n",
      "11/10/2019 21:56:10 - INFO - __main__ -     global_step = 3600\n",
      "11/10/2019 21:56:10 - INFO - __main__ -     train loss = 0.2309\n",
      "11/10/2019 21:56:31 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/10/2019 21:56:31 - INFO - __main__ -     Num examples = 2940\n",
      "11/10/2019 21:56:31 - INFO - __main__ -     Batch size = 32\n",
      "11/10/2019 21:58:50 - INFO - __main__ -     eval_F1 = 0.7881615010086325\n",
      "11/10/2019 21:58:50 - INFO - __main__ -     eval_loss = 0.38411471093802346\n",
      "11/10/2019 21:58:50 - INFO - __main__ -     global_step = 3600\n",
      "11/10/2019 21:58:50 - INFO - __main__ -     loss = 0.2309\n",
      "================================================================================\n",
      "loss 0.2209:  76%|████████████████▋     | 15199/20000 [1:17:54<16:46,  4.77it/s]11/10/2019 22:01:24 - INFO - __main__ -   ***** Report result *****\n",
      "11/10/2019 22:01:24 - INFO - __main__ -     global_step = 3800\n",
      "11/10/2019 22:01:24 - INFO - __main__ -     train loss = 0.2209\n",
      "11/10/2019 22:01:43 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/10/2019 22:01:43 - INFO - __main__ -     Num examples = 2940\n",
      "11/10/2019 22:01:43 - INFO - __main__ -     Batch size = 32\n",
      "11/10/2019 22:04:03 - INFO - __main__ -     eval_F1 = 0.7907032221327124\n",
      "11/10/2019 22:04:03 - INFO - __main__ -     eval_loss = 0.36827044338797743\n",
      "11/10/2019 22:04:03 - INFO - __main__ -     global_step = 3800\n",
      "11/10/2019 22:04:03 - INFO - __main__ -     loss = 0.2209\n",
      "================================================================================\n",
      "loss 0.2374:  80%|█████████████████▌    | 15999/20000 [1:23:06<13:56,  4.79it/s]11/10/2019 22:06:36 - INFO - __main__ -   ***** Report result *****\n",
      "11/10/2019 22:06:36 - INFO - __main__ -     global_step = 4000\n",
      "11/10/2019 22:06:36 - INFO - __main__ -     train loss = 0.2374\n",
      "11/10/2019 22:06:56 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/10/2019 22:06:56 - INFO - __main__ -     Num examples = 2940\n",
      "11/10/2019 22:06:56 - INFO - __main__ -     Batch size = 32\n",
      "11/10/2019 22:09:16 - INFO - __main__ -     eval_F1 = 0.7973733474465073\n",
      "11/10/2019 22:09:16 - INFO - __main__ -     eval_loss = 0.3813551541255868\n",
      "11/10/2019 22:09:16 - INFO - __main__ -     global_step = 4000\n",
      "11/10/2019 22:09:16 - INFO - __main__ -     loss = 0.2374\n",
      "================================================================================\n",
      "Best F1 0.7973733474465073\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.2204:  84%|██████████████████▍   | 16799/20000 [1:28:22<11:07,  4.80it/s]11/10/2019 22:11:53 - INFO - __main__ -   ***** Report result *****\n",
      "11/10/2019 22:11:53 - INFO - __main__ -     global_step = 4200\n",
      "11/10/2019 22:11:53 - INFO - __main__ -     train loss = 0.2204\n",
      "11/10/2019 22:12:12 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/10/2019 22:12:12 - INFO - __main__ -     Num examples = 2940\n",
      "11/10/2019 22:12:12 - INFO - __main__ -     Batch size = 32\n",
      "11/10/2019 22:14:32 - INFO - __main__ -     eval_F1 = 0.7928418785079615\n",
      "11/10/2019 22:14:32 - INFO - __main__ -     eval_loss = 0.37273623458231275\n",
      "11/10/2019 22:14:32 - INFO - __main__ -     global_step = 4200\n",
      "11/10/2019 22:14:32 - INFO - __main__ -     loss = 0.2204\n",
      "================================================================================\n",
      "loss 0.2906:  88%|███████████████████▎  | 17599/20000 [1:33:35<08:24,  4.76it/s]11/10/2019 22:17:05 - INFO - __main__ -   ***** Report result *****\n",
      "11/10/2019 22:17:05 - INFO - __main__ -     global_step = 4400\n",
      "11/10/2019 22:17:05 - INFO - __main__ -     train loss = 0.2906\n",
      "11/10/2019 22:17:27 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/10/2019 22:17:27 - INFO - __main__ -     Num examples = 2940\n",
      "11/10/2019 22:17:27 - INFO - __main__ -     Batch size = 32\n",
      "11/10/2019 22:19:47 - INFO - __main__ -     eval_F1 = 0.8034204348197002\n",
      "11/10/2019 22:19:47 - INFO - __main__ -     eval_loss = 0.3577808006869062\n",
      "11/10/2019 22:19:47 - INFO - __main__ -     global_step = 4400\n",
      "11/10/2019 22:19:47 - INFO - __main__ -     loss = 0.2906\n",
      "================================================================================\n",
      "Best F1 0.8034204348197002\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.23:  92%|██████████████████████  | 18399/20000 [1:38:54<05:34,  4.79it/s]11/10/2019 22:22:24 - INFO - __main__ -   ***** Report result *****\n",
      "11/10/2019 22:22:24 - INFO - __main__ -     global_step = 4600\n",
      "11/10/2019 22:22:24 - INFO - __main__ -     train loss = 0.23\n",
      "11/10/2019 22:22:47 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/10/2019 22:22:47 - INFO - __main__ -     Num examples = 2940\n",
      "11/10/2019 22:22:47 - INFO - __main__ -     Batch size = 32\n",
      "11/10/2019 22:25:07 - INFO - __main__ -     eval_F1 = 0.794463322646187\n",
      "11/10/2019 22:25:07 - INFO - __main__ -     eval_loss = 0.3566210341194402\n",
      "11/10/2019 22:25:07 - INFO - __main__ -     global_step = 4600\n",
      "11/10/2019 22:25:07 - INFO - __main__ -     loss = 0.23\n",
      "================================================================================\n",
      "loss 0.2244:  96%|█████████████████████ | 19199/20000 [1:44:10<02:47,  4.78it/s]11/10/2019 22:27:40 - INFO - __main__ -   ***** Report result *****\n",
      "11/10/2019 22:27:40 - INFO - __main__ -     global_step = 4800\n",
      "11/10/2019 22:27:40 - INFO - __main__ -     train loss = 0.2244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/10/2019 22:28:00 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/10/2019 22:28:00 - INFO - __main__ -     Num examples = 2940\n",
      "11/10/2019 22:28:00 - INFO - __main__ -     Batch size = 32\n",
      "11/10/2019 22:30:19 - INFO - __main__ -     eval_F1 = 0.7949717693589172\n",
      "11/10/2019 22:30:19 - INFO - __main__ -     eval_loss = 0.355892576844148\n",
      "11/10/2019 22:30:19 - INFO - __main__ -     global_step = 4800\n",
      "11/10/2019 22:30:19 - INFO - __main__ -     loss = 0.2244\n",
      "================================================================================\n",
      "loss 0.2425: 100%|█████████████████████▉| 19999/20000 [1:49:22<00:00,  4.79it/s]11/10/2019 22:32:52 - INFO - __main__ -   ***** Report result *****\n",
      "11/10/2019 22:32:52 - INFO - __main__ -     global_step = 5000\n",
      "11/10/2019 22:32:52 - INFO - __main__ -     train loss = 0.2425\n",
      "11/10/2019 22:33:12 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/10/2019 22:33:12 - INFO - __main__ -     Num examples = 2940\n",
      "11/10/2019 22:33:12 - INFO - __main__ -     Batch size = 32\n",
      "11/10/2019 22:35:32 - INFO - __main__ -     eval_F1 = 0.8022968432139859\n",
      "11/10/2019 22:35:32 - INFO - __main__ -     eval_loss = 0.35648160415661073\n",
      "11/10/2019 22:35:32 - INFO - __main__ -     global_step = 5000\n",
      "11/10/2019 22:35:32 - INFO - __main__ -     loss = 0.2425\n",
      "================================================================================\n",
      "loss 0.2425: 100%|██████████████████████| 20000/20000 [1:52:01<00:00, 47.90s/it]\n",
      "11/10/2019 22:35:32 - INFO - pytorch_transformers.modeling_utils -   loading weights file ../model/roberta_wwm_large_3002_gru1_42/roberta_wwm_large_3002_gru1_1/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "dev 0.8034204348197002\n",
      "/root/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "test 0.06541712605659684\n"
     ]
    }
   ],
   "source": [
    "# base:  首尾\n",
    "!python ./run_bert_2562.py \\\n",
    "--model_type bert \\\n",
    "--model_name_or_path ../model/chinese_roberta_wwm_large_ext_pytorch  \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_test \\\n",
    "--data_dir ../data/data_StratifiedKFold_42/data_origin_1 \\\n",
    "--output_dir ../model/roberta_wwm_large_3002_gru1_42/roberta_wwm_large_3002_gru1_1 \\\n",
    "--max_seq_length 300 \\\n",
    "--split_num 2 \\\n",
    "--lstm_hidden_size 512 \\\n",
    "--lstm_layers 1 \\\n",
    "--lstm_dropout 0.1 \\\n",
    "--eval_steps 200 \\\n",
    "--per_gpu_train_batch_size 4 \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--warmup_steps 0 \\\n",
    "--per_gpu_eval_batch_size 32 \\\n",
    "--learning_rate 5e-6 \\\n",
    "--adam_epsilon 1e-6 \\\n",
    "--weight_decay 0 \\\n",
    "--train_steps 20000 \\\n",
    "--freeze 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/10/2019 22:45:04 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "11/10/2019 22:45:04 - INFO - pytorch_transformers.tokenization_utils -   Model name '../model/chinese_roberta_wwm_large_ext_pytorch' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming '../model/chinese_roberta_wwm_large_ext_pytorch' is a path or url to a directory containing tokenizer files.\n",
      "11/10/2019 22:45:04 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../model/chinese_roberta_wwm_large_ext_pytorch/added_tokens.json. We won't load it.\n",
      "11/10/2019 22:45:04 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../model/chinese_roberta_wwm_large_ext_pytorch/special_tokens_map.json. We won't load it.\n",
      "11/10/2019 22:45:04 - INFO - pytorch_transformers.tokenization_utils -   loading file ../model/chinese_roberta_wwm_large_ext_pytorch/vocab.txt\n",
      "11/10/2019 22:45:04 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/10/2019 22:45:04 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/10/2019 22:45:04 - INFO - pytorch_transformers.modeling_utils -   loading configuration file ../model/chinese_roberta_wwm_large_ext_pytorch/config.json\n",
      "11/10/2019 22:45:04 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 3,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "11/10/2019 22:45:04 - INFO - pytorch_transformers.modeling_utils -   loading weights file ../model/chinese_roberta_wwm_large_ext_pytorch/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "11/10/2019 22:45:17 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'W.0.weight', 'W.0.bias', 'gru.0.weight_ih_l0', 'gru.0.weight_hh_l0', 'gru.0.bias_ih_l0', 'gru.0.bias_hh_l0', 'gru.0.weight_ih_l0_reverse', 'gru.0.weight_hh_l0_reverse', 'gru.0.bias_ih_l0_reverse', 'gru.0.bias_hh_l0_reverse']\n",
      "11/10/2019 22:45:17 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "11/10/2019 22:45:17 - INFO - __main__ -   *** Example ***\n",
      "11/10/2019 22:45:17 - INFO - __main__ -   idx: 0\n",
      "11/10/2019 22:45:17 - INFO - __main__ -   guid: 7a3dd79f90ee419da87190cff60f7a86\n",
      "11/10/2019 22:45:17 - INFO - __main__ -   tokens: [CLS] 问 责 领 导 ( 上 黄 镇 党 委 书 记 张 涛 ， 宣 国 才 真 能 一 手 遮 天 吗 ？ ) [SEP] 这 几 天 看 了 有 人 举 报 施 某 某 的 贴 子 ， 经 与 举 报 人 联 系 证 实 ， 是 宣 某 当 天 中 午 请 举 报 人 和 枪 手 喝 酒 后 ， 晚 上 才 发 的 贴 子 ！ 本 人 不 去 讨 论 前 二 天 的 举 报 ， 相 信 总 归 会 有 说 法 的 ！ 今 天 一 看 施 全 军 2017 年 1 月 2 日 实 名 举 报 上 黄 镇 宣 国 才 的 贴 子 （ 仍 被 锁 定 禁 止 评 论 ） 已 经 正 好 一 整 年 了 = 750 ) window . open ( ' http : / / img . js ##ly ##001 . com / at ##ta ##ch ##ment / mon _ 180 ##1 / 4 _ 291 ##08 ##5 _ c ##79 ##6 ##a ##6 ##a ##86 ##e ##17 ##12 ##1 . jpg ? 123 ' ) ; \" on ##load = \" if ( this . off ##set ##wi ##dt ##h > ' 750 ' ) this . wi ##dt ##h = ' 750 ' ; \" sr ##c = \" http : / / img . js ##ly ##001 . com / at ##ta ##ch ##ment / mon _ 180 ##1 / 4 _ 291 ##08 ##5 _ c ##79 ##6 ##a ##6 ##a ##86 ##e ##17 ##12 ##1 . jpg ? 123 \" style = \" max - wi ##dt ##h : 750 ##px ; \" / > 图 片 : / [SEP]\n",
      "11/10/2019 22:45:17 - INFO - __main__ -   input_ids: 101 7309 6569 7566 2193 113 677 7942 7252 1054 1999 741 6381 2476 3875 8024 2146 1744 2798 4696 5543 671 2797 6902 1921 1408 8043 114 102 6821 1126 1921 4692 749 3300 782 715 2845 3177 3378 3378 4638 6585 2094 8024 5307 680 715 2845 782 5468 5143 6395 2141 8024 3221 2146 3378 2496 1921 704 1286 6435 715 2845 782 1469 3366 2797 1600 6983 1400 8024 3241 677 2798 1355 4638 6585 2094 8013 3315 782 679 1343 6374 6389 1184 753 1921 4638 715 2845 8024 4685 928 2600 2495 833 3300 6432 3791 4638 8013 791 1921 671 4692 3177 1059 1092 8109 2399 122 3299 123 3189 2141 1399 715 2845 677 7942 7252 2146 1744 2798 4638 6585 2094 8020 793 6158 7219 2137 4881 3632 6397 6389 8021 2347 5307 3633 1962 671 3146 2399 749 134 9180 114 12158 119 8893 113 112 8184 131 120 120 11412 119 9016 8436 9942 119 8134 120 8243 8383 8370 8631 120 8556 142 8420 8148 120 125 142 11777 9153 8157 142 145 9495 8158 8139 8158 8139 9219 8154 8408 8455 8148 119 9248 136 8604 112 114 132 107 8281 11713 134 107 8898 113 8554 119 9594 9852 10958 12672 8199 135 112 9180 112 114 8554 119 8541 12672 8199 134 112 9180 112 132 107 12109 8177 134 107 8184 131 120 120 11412 119 9016 8436 9942 119 8134 120 8243 8383 8370 8631 120 8556 142 8420 8148 120 125 142 11777 9153 8157 142 145 9495 8158 8139 8158 8139 9219 8154 8408 8455 8148 119 9248 136 8604 107 8969 134 107 8621 118 8541 12672 8199 131 9180 10605 132 107 120 135 1745 4275 131 120 102\n",
      "11/10/2019 22:45:17 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/10/2019 22:45:17 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/10/2019 22:45:17 - INFO - __main__ -   label: 2\n",
      "11/10/2019 22:45:17 - INFO - __main__ -   *** Example ***\n",
      "11/10/2019 22:45:17 - INFO - __main__ -   idx: 0\n",
      "11/10/2019 22:45:17 - INFO - __main__ -   guid: 7a3dd79f90ee419da87190cff60f7a86\n",
      "11/10/2019 22:45:17 - INFO - __main__ -   tokens: [CLS] 问 责 领 导 ( 上 黄 镇 党 委 书 记 张 涛 ， 宣 国 才 真 能 一 手 遮 天 吗 ？ ) [SEP] / www / data / tm ##p / q ##fu ##pl ##oa ##d / 4 _ 291 ##08 ##5 _ 151 ##49 ##81 ##47 ##23 ##53 ##07 ##5 . jpg 一 年 的 贴 子 ， 再 次 被 网 友 顶 起 来 后 ， 才 发 现 施 某 几 天 前 回 复 网 友 的 处 理 结 果 竟 如 下 图 ： = 750 ) window . open ( ' http : / / img . js ##ly ##001 . com / at ##ta ##ch ##ment / mon _ 180 ##1 / 4 _ 291 ##08 ##5 _ 9 ##d ##32 ##ee ##57 ##27 ##60 ##d ##85 . jpg ? 131 ' ) ; \" on ##load = \" if ( this . off ##set ##wi ##dt ##h > ' 750 ' ) this . wi ##dt ##h = ' 750 ' ; \" sr ##c = \" http : / / img . js ##ly ##001 . com / at ##ta ##ch ##ment / mon _ 180 ##1 / 4 _ 291 ##08 ##5 _ 9 ##d ##32 ##ee ##57 ##27 ##60 ##d ##85 . jpg ? 131 \" style = \" max - wi ##dt ##h : 750 ##px ; \" / > 图 片 : / home / al ##ida ##ta / www / data / tm ##p / q ##fu ##pl ##oa ##d / 4 _ 291 ##08 ##5 _ 151 ##49 ##81 ##47 ##35 ##47 ##17 ##2 . jpg 现 责 问 张 涛 书 记 ： 1 、 宣 国 才 被 举 报 这 么 多 问 [SEP]\n",
      "11/10/2019 22:45:17 - INFO - __main__ -   input_ids: 101 7309 6569 7566 2193 113 677 7942 7252 1054 1999 741 6381 2476 3875 8024 2146 1744 2798 4696 5543 671 2797 6902 1921 1408 8043 114 102 120 8173 120 9000 120 9908 8187 120 159 12043 12569 11355 8168 120 125 142 11777 9153 8157 142 9564 9500 9313 9050 8748 9310 9131 8157 119 9248 671 2399 4638 6585 2094 8024 1086 3613 6158 5381 1351 7553 6629 3341 1400 8024 2798 1355 4385 3177 3378 1126 1921 1184 1726 1908 5381 1351 4638 1905 4415 5310 3362 4994 1963 678 1745 8038 134 9180 114 12158 119 8893 113 112 8184 131 120 120 11412 119 9016 8436 9942 119 8134 120 8243 8383 8370 8631 120 8556 142 8420 8148 120 125 142 11777 9153 8157 142 130 8168 8709 8854 9647 8976 8581 8168 9169 119 9248 136 9403 112 114 132 107 8281 11713 134 107 8898 113 8554 119 9594 9852 10958 12672 8199 135 112 9180 112 114 8554 119 8541 12672 8199 134 112 9180 112 132 107 12109 8177 134 107 8184 131 120 120 11412 119 9016 8436 9942 119 8134 120 8243 8383 8370 8631 120 8556 142 8420 8148 120 125 142 11777 9153 8157 142 130 8168 8709 8854 9647 8976 8581 8168 9169 119 9248 136 9403 107 8969 134 107 8621 118 8541 12672 8199 131 9180 10605 132 107 120 135 1745 4275 131 120 8563 120 9266 12708 8383 120 8173 120 9000 120 9908 8187 120 159 12043 12569 11355 8168 120 125 142 11777 9153 8157 142 9564 9500 9313 9050 8852 9050 8408 8144 119 9248 4385 6569 7309 2476 3875 741 6381 8038 122 510 2146 1744 2798 6158 715 2845 6821 720 1914 7309 102\n",
      "11/10/2019 22:45:17 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/10/2019 22:45:17 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/10/2019 22:45:17 - INFO - __main__ -   label: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/10/2019 22:46:43 - INFO - __main__ -   ***** Running training *****\n",
      "11/10/2019 22:46:43 - INFO - __main__ -     Num examples = 11757\n",
      "11/10/2019 22:46:43 - INFO - __main__ -     Batch size = 4\n",
      "11/10/2019 22:46:43 - INFO - __main__ -     Num steps = 20000\n",
      "  0%|                                                 | 0/20000 [00:00<?, ?it/s]11/10/2019 22:47:05 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/10/2019 22:47:05 - INFO - __main__ -     Num examples = 2939\n",
      "11/10/2019 22:47:05 - INFO - __main__ -     Batch size = 32\n",
      "/root/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "11/10/2019 22:49:23 - INFO - __main__ -     eval_F1 = 0.062493575907081926\n",
      "11/10/2019 22:49:23 - INFO - __main__ -     eval_loss = 1.187313075946725\n",
      "11/10/2019 22:49:23 - INFO - __main__ -     global_step = 0\n",
      "================================================================================\n",
      "Best F1 0.062493575907081926\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.7669:   4%|▉                       | 799/20000 [05:12<1:06:29,  4.81it/s]11/10/2019 22:51:55 - INFO - __main__ -   ***** Report result *****\n",
      "11/10/2019 22:51:55 - INFO - __main__ -     global_step = 200\n",
      "11/10/2019 22:51:55 - INFO - __main__ -     train loss = 0.7669\n",
      "loss 0.4728:   8%|█▊                     | 1599/20000 [07:44<1:03:14,  4.85it/s]11/10/2019 22:54:27 - INFO - __main__ -   ***** Report result *****\n",
      "11/10/2019 22:54:27 - INFO - __main__ -     global_step = 400\n",
      "11/10/2019 22:54:27 - INFO - __main__ -     train loss = 0.4728\n",
      "loss 0.4365:  12%|██▊                    | 2399/20000 [10:16<1:00:45,  4.83it/s]11/10/2019 22:57:00 - INFO - __main__ -   ***** Report result *****\n",
      "11/10/2019 22:57:00 - INFO - __main__ -     global_step = 600\n",
      "11/10/2019 22:57:00 - INFO - __main__ -     train loss = 0.4365\n",
      "loss 0.4206:  16%|███▉                     | 3199/20000 [12:49<58:19,  4.80it/s]11/10/2019 22:59:33 - INFO - __main__ -   ***** Report result *****\n",
      "11/10/2019 22:59:33 - INFO - __main__ -     global_step = 800\n",
      "11/10/2019 22:59:33 - INFO - __main__ -     train loss = 0.4206\n",
      "loss 0.3499:  20%|████▉                    | 3999/20000 [15:22<55:07,  4.84it/s]11/10/2019 23:02:06 - INFO - __main__ -   ***** Report result *****\n",
      "11/10/2019 23:02:06 - INFO - __main__ -     global_step = 1000\n",
      "11/10/2019 23:02:06 - INFO - __main__ -     train loss = 0.3499\n",
      "loss 0.4019:  24%|█████▉                   | 4799/20000 [17:56<52:50,  4.79it/s]11/10/2019 23:04:39 - INFO - __main__ -   ***** Report result *****\n",
      "11/10/2019 23:04:39 - INFO - __main__ -     global_step = 1200\n",
      "11/10/2019 23:04:39 - INFO - __main__ -     train loss = 0.4019\n",
      "loss 0.3898:  28%|██████▉                  | 5599/20000 [20:29<49:38,  4.84it/s]11/10/2019 23:07:12 - INFO - __main__ -   ***** Report result *****\n",
      "11/10/2019 23:07:12 - INFO - __main__ -     global_step = 1400\n",
      "11/10/2019 23:07:12 - INFO - __main__ -     train loss = 0.3898\n",
      "loss 0.3592:  32%|███████▉                 | 6399/20000 [23:01<46:59,  4.82it/s]11/10/2019 23:09:44 - INFO - __main__ -   ***** Report result *****\n",
      "11/10/2019 23:09:44 - INFO - __main__ -     global_step = 1600\n",
      "11/10/2019 23:09:44 - INFO - __main__ -     train loss = 0.3592\n",
      "loss 0.3519:  36%|████████▉                | 7199/20000 [25:33<44:17,  4.82it/s]11/10/2019 23:12:17 - INFO - __main__ -   ***** Report result *****\n",
      "11/10/2019 23:12:17 - INFO - __main__ -     global_step = 1800\n",
      "11/10/2019 23:12:17 - INFO - __main__ -     train loss = 0.3519\n",
      "11/10/2019 23:12:39 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/10/2019 23:12:39 - INFO - __main__ -     Num examples = 2939\n",
      "11/10/2019 23:12:39 - INFO - __main__ -     Batch size = 32\n",
      "11/10/2019 23:14:58 - INFO - __main__ -     eval_F1 = 0.720484736057795\n",
      "11/10/2019 23:14:58 - INFO - __main__ -     eval_loss = 0.3950917477840963\n",
      "11/10/2019 23:14:58 - INFO - __main__ -     global_step = 1800\n",
      "11/10/2019 23:14:58 - INFO - __main__ -     loss = 0.3519\n",
      "================================================================================\n",
      "Best F1 0.720484736057795\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.3487:  40%|█████████▉               | 7999/20000 [30:50<41:21,  4.84it/s]11/10/2019 23:17:34 - INFO - __main__ -   ***** Report result *****\n",
      "11/10/2019 23:17:34 - INFO - __main__ -     global_step = 2000\n",
      "11/10/2019 23:17:34 - INFO - __main__ -     train loss = 0.3487\n",
      "11/10/2019 23:17:56 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/10/2019 23:17:56 - INFO - __main__ -     Num examples = 2939\n",
      "11/10/2019 23:17:56 - INFO - __main__ -     Batch size = 32\n",
      "11/10/2019 23:20:15 - INFO - __main__ -     eval_F1 = 0.7975088007035113\n",
      "11/10/2019 23:20:15 - INFO - __main__ -     eval_loss = 0.3451566738683892\n",
      "11/10/2019 23:20:15 - INFO - __main__ -     global_step = 2000\n",
      "11/10/2019 23:20:15 - INFO - __main__ -     loss = 0.3487\n",
      "================================================================================\n",
      "Best F1 0.7975088007035113\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.3995:  44%|██████████▉              | 8799/20000 [36:07<38:51,  4.81it/s]11/10/2019 23:22:50 - INFO - __main__ -   ***** Report result *****\n",
      "11/10/2019 23:22:50 - INFO - __main__ -     global_step = 2200\n",
      "11/10/2019 23:22:50 - INFO - __main__ -     train loss = 0.3995\n",
      "11/10/2019 23:23:12 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/10/2019 23:23:12 - INFO - __main__ -     Num examples = 2939\n",
      "11/10/2019 23:23:12 - INFO - __main__ -     Batch size = 32\n",
      "11/10/2019 23:25:31 - INFO - __main__ -     eval_F1 = 0.7675550494399156\n",
      "11/10/2019 23:25:31 - INFO - __main__ -     eval_loss = 0.3453785964571264\n",
      "11/10/2019 23:25:31 - INFO - __main__ -     global_step = 2200\n",
      "11/10/2019 23:25:31 - INFO - __main__ -     loss = 0.3995\n",
      "================================================================================\n",
      "loss 0.3393:  48%|███████████▉             | 9599/20000 [41:21<36:02,  4.81it/s]11/10/2019 23:28:04 - INFO - __main__ -   ***** Report result *****\n",
      "11/10/2019 23:28:04 - INFO - __main__ -     global_step = 2400\n",
      "11/10/2019 23:28:04 - INFO - __main__ -     train loss = 0.3393\n",
      "11/10/2019 23:28:27 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/10/2019 23:28:27 - INFO - __main__ -     Num examples = 2939\n",
      "11/10/2019 23:28:27 - INFO - __main__ -     Batch size = 32\n",
      "11/10/2019 23:30:46 - INFO - __main__ -     eval_F1 = 0.7693496227538376\n",
      "11/10/2019 23:30:46 - INFO - __main__ -     eval_loss = 0.3817307832043456\n",
      "11/10/2019 23:30:46 - INFO - __main__ -     global_step = 2400\n",
      "11/10/2019 23:30:46 - INFO - __main__ -     loss = 0.3393\n",
      "================================================================================\n",
      "loss 0.3947:  52%|████████████▍           | 10399/20000 [46:35<33:20,  4.80it/s]11/10/2019 23:33:19 - INFO - __main__ -   ***** Report result *****\n",
      "11/10/2019 23:33:19 - INFO - __main__ -     global_step = 2600\n",
      "11/10/2019 23:33:19 - INFO - __main__ -     train loss = 0.3947\n",
      "11/10/2019 23:33:41 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/10/2019 23:33:41 - INFO - __main__ -     Num examples = 2939\n",
      "11/10/2019 23:33:41 - INFO - __main__ -     Batch size = 32\n",
      "11/10/2019 23:36:00 - INFO - __main__ -     eval_F1 = 0.7980285916475233\n",
      "11/10/2019 23:36:00 - INFO - __main__ -     eval_loss = 0.3527652464159157\n",
      "11/10/2019 23:36:00 - INFO - __main__ -     global_step = 2600\n",
      "11/10/2019 23:36:00 - INFO - __main__ -     loss = 0.3947\n",
      "================================================================================\n",
      "Best F1 0.7980285916475233\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.3434:  56%|█████████████▍          | 11199/20000 [51:54<30:27,  4.82it/s]11/10/2019 23:38:37 - INFO - __main__ -   ***** Report result *****\n",
      "11/10/2019 23:38:37 - INFO - __main__ -     global_step = 2800\n",
      "11/10/2019 23:38:37 - INFO - __main__ -     train loss = 0.3434\n",
      "11/10/2019 23:38:58 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/10/2019 23:38:58 - INFO - __main__ -     Num examples = 2939\n",
      "11/10/2019 23:38:58 - INFO - __main__ -     Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/10/2019 23:41:17 - INFO - __main__ -     eval_F1 = 0.7772176047259771\n",
      "11/10/2019 23:41:17 - INFO - __main__ -     eval_loss = 0.3394587717788375\n",
      "11/10/2019 23:41:17 - INFO - __main__ -     global_step = 2800\n",
      "11/10/2019 23:41:17 - INFO - __main__ -     loss = 0.3434\n",
      "================================================================================\n",
      "loss 0.314:  60%|██████████████▉          | 11999/20000 [57:07<27:57,  4.77it/s]11/10/2019 23:43:50 - INFO - __main__ -   ***** Report result *****\n",
      "11/10/2019 23:43:50 - INFO - __main__ -     global_step = 3000\n",
      "11/10/2019 23:43:50 - INFO - __main__ -     train loss = 0.314\n",
      "11/10/2019 23:44:13 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/10/2019 23:44:13 - INFO - __main__ -     Num examples = 2939\n",
      "11/10/2019 23:44:13 - INFO - __main__ -     Batch size = 32\n",
      "11/10/2019 23:46:32 - INFO - __main__ -     eval_F1 = 0.7809756924051477\n",
      "11/10/2019 23:46:32 - INFO - __main__ -     eval_loss = 0.33984982635339966\n",
      "11/10/2019 23:46:32 - INFO - __main__ -     global_step = 3000\n",
      "11/10/2019 23:46:32 - INFO - __main__ -     loss = 0.314\n",
      "================================================================================\n",
      "loss 0.292:  64%|██████████████▋        | 12799/20000 [1:02:23<25:12,  4.76it/s]11/10/2019 23:49:06 - INFO - __main__ -   ***** Report result *****\n",
      "11/10/2019 23:49:06 - INFO - __main__ -     global_step = 3200\n",
      "11/10/2019 23:49:06 - INFO - __main__ -     train loss = 0.292\n",
      "11/10/2019 23:49:28 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/10/2019 23:49:28 - INFO - __main__ -     Num examples = 2939\n",
      "11/10/2019 23:49:28 - INFO - __main__ -     Batch size = 32\n",
      "11/10/2019 23:51:47 - INFO - __main__ -     eval_F1 = 0.7905515296981699\n",
      "11/10/2019 23:51:47 - INFO - __main__ -     eval_loss = 0.3435441901421417\n",
      "11/10/2019 23:51:47 - INFO - __main__ -     global_step = 3200\n",
      "11/10/2019 23:51:47 - INFO - __main__ -     loss = 0.292\n",
      "================================================================================\n",
      "loss 0.2659:  68%|██████████████▉       | 13599/20000 [1:07:37<22:16,  4.79it/s]11/10/2019 23:54:21 - INFO - __main__ -   ***** Report result *****\n",
      "11/10/2019 23:54:21 - INFO - __main__ -     global_step = 3400\n",
      "11/10/2019 23:54:21 - INFO - __main__ -     train loss = 0.2659\n",
      "11/10/2019 23:54:42 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/10/2019 23:54:42 - INFO - __main__ -     Num examples = 2939\n",
      "11/10/2019 23:54:42 - INFO - __main__ -     Batch size = 32\n",
      "11/10/2019 23:57:01 - INFO - __main__ -     eval_F1 = 0.7938813388548177\n",
      "11/10/2019 23:57:01 - INFO - __main__ -     eval_loss = 0.34513512614142633\n",
      "11/10/2019 23:57:01 - INFO - __main__ -     global_step = 3400\n",
      "11/10/2019 23:57:01 - INFO - __main__ -     loss = 0.2659\n",
      "================================================================================\n",
      "loss 0.2457:  72%|███████████████▊      | 14399/20000 [1:12:51<19:26,  4.80it/s]11/10/2019 23:59:35 - INFO - __main__ -   ***** Report result *****\n",
      "11/10/2019 23:59:35 - INFO - __main__ -     global_step = 3600\n",
      "11/10/2019 23:59:35 - INFO - __main__ -     train loss = 0.2457\n",
      "11/10/2019 23:59:56 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/10/2019 23:59:56 - INFO - __main__ -     Num examples = 2939\n",
      "11/10/2019 23:59:56 - INFO - __main__ -     Batch size = 32\n",
      "11/11/2019 00:02:15 - INFO - __main__ -     eval_F1 = 0.8101349464136701\n",
      "11/11/2019 00:02:15 - INFO - __main__ -     eval_loss = 0.34153096009369777\n",
      "11/11/2019 00:02:15 - INFO - __main__ -     global_step = 3600\n",
      "11/11/2019 00:02:15 - INFO - __main__ -     loss = 0.2457\n",
      "================================================================================\n",
      "Best F1 0.8101349464136701\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.2307:  76%|████████████████▋     | 15199/20000 [1:18:09<16:39,  4.80it/s]11/11/2019 00:04:52 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 00:04:52 - INFO - __main__ -     global_step = 3800\n",
      "11/11/2019 00:04:52 - INFO - __main__ -     train loss = 0.2307\n",
      "11/11/2019 00:05:14 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 00:05:14 - INFO - __main__ -     Num examples = 2939\n",
      "11/11/2019 00:05:14 - INFO - __main__ -     Batch size = 32\n",
      "11/11/2019 00:07:33 - INFO - __main__ -     eval_F1 = 0.8107578289696115\n",
      "11/11/2019 00:07:33 - INFO - __main__ -     eval_loss = 0.35644146694761253\n",
      "11/11/2019 00:07:33 - INFO - __main__ -     global_step = 3800\n",
      "11/11/2019 00:07:33 - INFO - __main__ -     loss = 0.2307\n",
      "================================================================================\n",
      "Best F1 0.8107578289696115\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.1781:  80%|█████████████████▌    | 15999/20000 [1:23:27<13:55,  4.79it/s]11/11/2019 00:10:10 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 00:10:10 - INFO - __main__ -     global_step = 4000\n",
      "11/11/2019 00:10:10 - INFO - __main__ -     train loss = 0.1781\n",
      "11/11/2019 00:10:33 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 00:10:33 - INFO - __main__ -     Num examples = 2939\n",
      "11/11/2019 00:10:33 - INFO - __main__ -     Batch size = 32\n",
      "11/11/2019 00:12:52 - INFO - __main__ -     eval_F1 = 0.8146164514738801\n",
      "11/11/2019 00:12:52 - INFO - __main__ -     eval_loss = 0.3673504367470741\n",
      "11/11/2019 00:12:52 - INFO - __main__ -     global_step = 4000\n",
      "11/11/2019 00:12:52 - INFO - __main__ -     loss = 0.1781\n",
      "================================================================================\n",
      "Best F1 0.8146164514738801\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.2318:  84%|██████████████████▍   | 16799/20000 [1:28:46<11:13,  4.75it/s]11/11/2019 00:15:29 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 00:15:29 - INFO - __main__ -     global_step = 4200\n",
      "11/11/2019 00:15:29 - INFO - __main__ -     train loss = 0.2318\n",
      "11/11/2019 00:15:52 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 00:15:52 - INFO - __main__ -     Num examples = 2939\n",
      "11/11/2019 00:15:52 - INFO - __main__ -     Batch size = 32\n",
      "11/11/2019 00:18:11 - INFO - __main__ -     eval_F1 = 0.8071517502710158\n",
      "11/11/2019 00:18:11 - INFO - __main__ -     eval_loss = 0.3701526507654268\n",
      "11/11/2019 00:18:11 - INFO - __main__ -     global_step = 4200\n",
      "11/11/2019 00:18:11 - INFO - __main__ -     loss = 0.2318\n",
      "================================================================================\n",
      "loss 0.2593:  88%|███████████████████▎  | 17599/20000 [1:34:01<08:18,  4.82it/s]11/11/2019 00:20:45 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 00:20:45 - INFO - __main__ -     global_step = 4400\n",
      "11/11/2019 00:20:45 - INFO - __main__ -     train loss = 0.2593\n",
      "11/11/2019 00:21:07 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 00:21:07 - INFO - __main__ -     Num examples = 2939\n",
      "11/11/2019 00:21:07 - INFO - __main__ -     Batch size = 32\n",
      "11/11/2019 00:23:26 - INFO - __main__ -     eval_F1 = 0.8119776956655153\n",
      "11/11/2019 00:23:26 - INFO - __main__ -     eval_loss = 0.3536292001119126\n",
      "11/11/2019 00:23:26 - INFO - __main__ -     global_step = 4400\n",
      "11/11/2019 00:23:26 - INFO - __main__ -     loss = 0.2593\n",
      "================================================================================\n",
      "loss 0.2042:  92%|████████████████████▏ | 18399/20000 [1:39:16<05:34,  4.79it/s]11/11/2019 00:26:00 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 00:26:00 - INFO - __main__ -     global_step = 4600\n",
      "11/11/2019 00:26:00 - INFO - __main__ -     train loss = 0.2042\n",
      "11/11/2019 00:26:22 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 00:26:22 - INFO - __main__ -     Num examples = 2939\n",
      "11/11/2019 00:26:22 - INFO - __main__ -     Batch size = 32\n",
      "11/11/2019 00:28:41 - INFO - __main__ -     eval_F1 = 0.8109745273243755\n",
      "11/11/2019 00:28:41 - INFO - __main__ -     eval_loss = 0.35637274886602943\n",
      "11/11/2019 00:28:41 - INFO - __main__ -     global_step = 4600\n",
      "11/11/2019 00:28:41 - INFO - __main__ -     loss = 0.2042\n",
      "================================================================================\n",
      "loss 0.2293:  96%|█████████████████████ | 19199/20000 [1:44:31<02:46,  4.81it/s]11/11/2019 00:31:14 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 00:31:14 - INFO - __main__ -     global_step = 4800\n",
      "11/11/2019 00:31:14 - INFO - __main__ -     train loss = 0.2293\n",
      "11/11/2019 00:31:36 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 00:31:36 - INFO - __main__ -     Num examples = 2939\n",
      "11/11/2019 00:31:36 - INFO - __main__ -     Batch size = 32\n",
      "11/11/2019 00:33:55 - INFO - __main__ -     eval_F1 = 0.8098058908958068\n",
      "11/11/2019 00:33:55 - INFO - __main__ -     eval_loss = 0.3459982738384734\n",
      "11/11/2019 00:33:55 - INFO - __main__ -     global_step = 4800\n",
      "11/11/2019 00:33:55 - INFO - __main__ -     loss = 0.2293\n",
      "================================================================================\n",
      "loss 0.2306: 100%|█████████████████████▉| 19999/20000 [1:49:45<00:00,  4.77it/s]11/11/2019 00:36:29 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 00:36:29 - INFO - __main__ -     global_step = 5000\n",
      "11/11/2019 00:36:29 - INFO - __main__ -     train loss = 0.2306\n",
      "11/11/2019 00:36:50 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 00:36:50 - INFO - __main__ -     Num examples = 2939\n",
      "11/11/2019 00:36:50 - INFO - __main__ -     Batch size = 32\n",
      "11/11/2019 00:39:09 - INFO - __main__ -     eval_F1 = 0.8128003456948679\n",
      "11/11/2019 00:39:09 - INFO - __main__ -     eval_loss = 0.34657503223127645\n",
      "11/11/2019 00:39:09 - INFO - __main__ -     global_step = 5000\n",
      "11/11/2019 00:39:09 - INFO - __main__ -     loss = 0.2306\n",
      "================================================================================\n",
      "loss 0.2306: 100%|██████████████████████| 20000/20000 [1:52:26<00:00, 48.30s/it]\n",
      "11/11/2019 00:39:09 - INFO - pytorch_transformers.modeling_utils -   loading weights file ../model/roberta_wwm_large_3002_gru1_42/roberta_wwm_large_3002_gru1_2/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "dev 0.8146164514738801\n",
      "/root/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "test 0.06929568321973385\n"
     ]
    }
   ],
   "source": [
    "# base:  首尾\n",
    "!python ./run_bert_2562.py \\\n",
    "--model_type bert \\\n",
    "--model_name_or_path ../model/chinese_roberta_wwm_large_ext_pytorch  \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_test \\\n",
    "--data_dir ../data/data_StratifiedKFold_42/data_origin_2 \\\n",
    "--output_dir ../model/roberta_wwm_large_3002_gru1_42/roberta_wwm_large_3002_gru1_2 \\\n",
    "--max_seq_length 300 \\\n",
    "--split_num 2 \\\n",
    "--lstm_hidden_size 512 \\\n",
    "--lstm_layers 1 \\\n",
    "--lstm_dropout 0.1 \\\n",
    "--eval_steps 200 \\\n",
    "--per_gpu_train_batch_size 4 \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--warmup_steps 0 \\\n",
    "--per_gpu_eval_batch_size 32 \\\n",
    "--learning_rate 5e-6 \\\n",
    "--adam_epsilon 1e-6 \\\n",
    "--weight_decay 0 \\\n",
    "--train_steps 20000 \\\n",
    "--freeze 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11/2019 00:48:46 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "11/11/2019 00:48:46 - INFO - pytorch_transformers.tokenization_utils -   Model name '../model/chinese_roberta_wwm_large_ext_pytorch' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming '../model/chinese_roberta_wwm_large_ext_pytorch' is a path or url to a directory containing tokenizer files.\n",
      "11/11/2019 00:48:46 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../model/chinese_roberta_wwm_large_ext_pytorch/added_tokens.json. We won't load it.\n",
      "11/11/2019 00:48:46 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../model/chinese_roberta_wwm_large_ext_pytorch/special_tokens_map.json. We won't load it.\n",
      "11/11/2019 00:48:46 - INFO - pytorch_transformers.tokenization_utils -   loading file ../model/chinese_roberta_wwm_large_ext_pytorch/vocab.txt\n",
      "11/11/2019 00:48:46 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/11/2019 00:48:46 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/11/2019 00:48:46 - INFO - pytorch_transformers.modeling_utils -   loading configuration file ../model/chinese_roberta_wwm_large_ext_pytorch/config.json\n",
      "11/11/2019 00:48:46 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 3,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "11/11/2019 00:48:46 - INFO - pytorch_transformers.modeling_utils -   loading weights file ../model/chinese_roberta_wwm_large_ext_pytorch/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "11/11/2019 00:48:59 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'W.0.weight', 'W.0.bias', 'gru.0.weight_ih_l0', 'gru.0.weight_hh_l0', 'gru.0.bias_ih_l0', 'gru.0.bias_hh_l0', 'gru.0.weight_ih_l0_reverse', 'gru.0.weight_hh_l0_reverse', 'gru.0.bias_ih_l0_reverse', 'gru.0.bias_hh_l0_reverse']\n",
      "11/11/2019 00:48:59 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "11/11/2019 00:49:00 - INFO - __main__ -   *** Example ***\n",
      "11/11/2019 00:49:00 - INFO - __main__ -   idx: 0\n",
      "11/11/2019 00:49:00 - INFO - __main__ -   guid: 7640a5589bc7486ca199eeeb38af79dd\n",
      "11/11/2019 00:49:00 - INFO - __main__ -   tokens: [CLS] 江 歌 事 件 : 教 会 孩 子 ， 善 良 的 同 时 更 要 懂 得 保 护 自 己 ! [SEP] 过 去 一 年 的 江 歌 悲 剧 ， 这 几 日 再 次 刷 屏 ： 住 在 东 京 都 中 野 区 的 中 国 女 留 学 生 江 歌 ， 收 留 了 被 前 男 友 陈 世 锋 恶 意 纠 缠 的 闺 蜜 刘 鑫 ， 两 人 在 回 到 江 歌 公 寓 楼 时 ， 陈 世 锋 已 经 等 在 楼 下 ， 叫 嚣 着 要 刘 鑫 给 自 己 一 个 说 法 （ 男 友 此 时 的 情 绪 处 于 濒 临 崩 溃 的 状 态 ） 。 江 歌 为 了 保 护 刘 鑫 ， 就 让 她 先 进 了 房 间 ， 自 己 拦 在 外 面 要 求 陈 世 锋 离 开 。 结 果 江 歌 被 陈 世 锋 用 刀 多 处 刺 伤 脖 子 和 胸 部 ， 刀 刀 毙 命 ， 残 忍 至 极 ， 最 终 因 失 血 过 多 丧 生 。 十 几 刀 ， 刘 鑫 躲 在 屋 里 ， 躲 在 门 后 ， 亲 耳 听 着 闺 蜜 江 歌 的 声 声 求 助 及 惨 叫 ， 却 始 终 没 有 打 开 门 。 连 邻 居 都 听 到 呼 救 纷 纷 开 门 查 看 究 竟 ， 那 扇 可 以 救 命 的 们 始 终 没 有 打 开 。 江 歌 死 后 ， 刘 鑫 面 对 警 方 的 询 问 ， 称 自 [SEP]\n",
      "11/11/2019 00:49:00 - INFO - __main__ -   input_ids: 101 3736 3625 752 816 131 3136 833 2111 2094 8024 1587 5679 4638 1398 3198 3291 6206 2743 2533 924 2844 5632 2346 106 102 6814 1343 671 2399 4638 3736 3625 2650 1196 8024 6821 1126 3189 1086 3613 1170 2242 8038 857 1762 691 776 6963 704 7029 1277 4638 704 1744 1957 4522 2110 4495 3736 3625 8024 3119 4522 749 6158 1184 4511 1351 7357 686 7226 2626 2692 5272 5362 4638 7318 6057 1155 7144 8024 697 782 1762 1726 1168 3736 3625 1062 2171 3517 3198 8024 7357 686 7226 2347 5307 5023 1762 3517 678 8024 1373 1709 4708 6206 1155 7144 5314 5632 2346 671 702 6432 3791 8020 4511 1351 3634 3198 4638 2658 5328 1905 754 4085 707 2309 3971 4638 4307 2578 8021 511 3736 3625 711 749 924 2844 1155 7144 8024 2218 6375 1961 1044 6822 749 2791 7313 8024 5632 2346 2882 1762 1912 7481 6206 3724 7357 686 7226 4895 2458 511 5310 3362 3736 3625 6158 7357 686 7226 4500 1143 1914 1905 1173 839 5556 2094 1469 5541 6956 8024 1143 1143 3687 1462 8024 3655 2556 5635 3353 8024 3297 5303 1728 1927 6117 6814 1914 700 4495 511 1282 1126 1143 8024 1155 7144 6719 1762 2238 7027 8024 6719 1762 7305 1400 8024 779 5455 1420 4708 7318 6057 3736 3625 4638 1898 1898 3724 1221 1350 2673 1373 8024 1316 1993 5303 3766 3300 2802 2458 7305 511 6825 6943 2233 6963 1420 1168 1461 3131 5290 5290 2458 7305 3389 4692 4955 4994 8024 6929 2794 1377 809 3131 1462 4638 812 1993 5303 3766 3300 2802 2458 511 3736 3625 3647 1400 8024 1155 7144 7481 2190 6356 3175 4638 6418 7309 8024 4917 5632 102\n",
      "11/11/2019 00:49:00 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/11/2019 00:49:00 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/11/2019 00:49:00 - INFO - __main__ -   label: 1\n",
      "11/11/2019 00:49:00 - INFO - __main__ -   *** Example ***\n",
      "11/11/2019 00:49:00 - INFO - __main__ -   idx: 0\n",
      "11/11/2019 00:49:00 - INFO - __main__ -   guid: 7640a5589bc7486ca199eeeb38af79dd\n",
      "11/11/2019 00:49:00 - INFO - __main__ -   tokens: [CLS] 江 歌 事 件 : 教 会 孩 子 ， 善 良 的 同 时 更 要 懂 得 保 护 自 己 ! [SEP] 方 四 处 侦 查 ， 不 久 捕 获 邪 恶 的 孕 妇 谭 某 [UNK] [UNK] 网 络 上 的 新 闻 称 ， 此 女 为 讨 好 丈 夫 ， 假 装 在 街 头 跌 倒 ， 引 诱 善 良 女 孩 ， 拐 到 家 里 让 丈 夫 伤 害 。 毫 无 防 范 的 女 孩 ， 送 其 至 出 租 屋 后 ， 被 孕 妇 谭 某 以 一 瓶 掺 了 安 眠 药 的 酸 奶 迷 昏 ， 而 后 将 善 良 的 姑 娘 杀 害 。 有 天 下 雨 ， 有 个 孕 妇 见 门 口 有 个 乞 丐 在 淋 雨 ， 于 心 不 忍 ， 于 是 请 他 进 来 歇 歇 脚 。 乞 丐 感 恩 戴 德 地 进 了 她 的 家 ， 在 换 了 干 净 的 衣 服 ， 吃 饱 饭 之 后 ， 他 发 现 只 有 孕 妇 一 个 人 在 家 ， 于 是 威 胁 孕 妇 给 他 一 笔 生 活 费 。 在 这 样 封 闭 的 环 境 下 ， 只 有 他 们 两 人 在 ， 她 失 去 了 讨 价 还 价 的 余 地 ， 为 了 避 免 遭 受 更 大 的 伤 害 ， 破 财 消 灾 ， 付 了 一 万 块 钱 给 乞 丐 。 当 一 个 场 合 ， 从 开 放 变 成 封 闭 ， 缺 少 了 大 家 的 [SEP]\n",
      "11/11/2019 00:49:00 - INFO - __main__ -   input_ids: 101 3736 3625 752 816 131 3136 833 2111 2094 8024 1587 5679 4638 1398 3198 3291 6206 2743 2533 924 2844 5632 2346 106 102 3175 1724 1905 903 3389 8024 679 719 2936 5815 6932 2626 4638 2097 1967 6478 3378 100 100 5381 5317 677 4638 3173 7319 4917 8024 3634 1957 711 6374 1962 675 1923 8024 969 6163 1762 6125 1928 6649 948 8024 2471 6430 1587 5679 1957 2111 8024 2866 1168 2157 7027 6375 675 1923 839 2154 511 3690 3187 7344 5745 4638 1957 2111 8024 6843 1071 5635 1139 4909 2238 1400 8024 6158 2097 1967 6478 3378 809 671 4486 2982 749 2128 4697 5790 4638 7000 1959 6837 3210 8024 5445 1400 2199 1587 5679 4638 1996 2023 3324 2154 511 3300 1921 678 7433 8024 3300 702 2097 1967 6224 7305 1366 3300 702 737 681 1762 3900 7433 8024 754 2552 679 2556 8024 754 3221 6435 800 6822 3341 3623 3623 5558 511 737 681 2697 2617 2785 2548 1765 6822 749 1961 4638 2157 8024 1762 2940 749 2397 1112 4638 6132 3302 8024 1391 7653 7649 722 1400 8024 800 1355 4385 1372 3300 2097 1967 671 702 782 1762 2157 8024 754 3221 2014 5516 2097 1967 5314 800 671 5011 4495 3833 6589 511 1762 6821 3416 2196 7308 4638 4384 1862 678 8024 1372 3300 800 812 697 782 1762 8024 1961 1927 1343 749 6374 817 6820 817 4638 865 1765 8024 711 749 6912 1048 6901 1358 3291 1920 4638 839 2154 8024 4788 6568 3867 4135 8024 802 749 671 674 1779 7178 5314 737 681 511 2496 671 702 1767 1394 8024 794 2458 3123 1359 2768 2196 7308 8024 5375 2208 749 1920 2157 4638 102\n",
      "11/11/2019 00:49:00 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/11/2019 00:49:00 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/11/2019 00:49:00 - INFO - __main__ -   label: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11/2019 00:50:22 - INFO - __main__ -   ***** Running training *****\n",
      "11/11/2019 00:50:22 - INFO - __main__ -     Num examples = 11758\n",
      "11/11/2019 00:50:22 - INFO - __main__ -     Batch size = 4\n",
      "11/11/2019 00:50:22 - INFO - __main__ -     Num steps = 20000\n",
      "  0%|                                                 | 0/20000 [00:00<?, ?it/s]11/11/2019 00:50:44 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 00:50:44 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 00:50:44 - INFO - __main__ -     Batch size = 32\n",
      "/root/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "11/11/2019 00:53:02 - INFO - __main__ -     eval_F1 = 0.06251285214887929\n",
      "11/11/2019 00:53:02 - INFO - __main__ -     eval_loss = 1.187326994927033\n",
      "11/11/2019 00:53:02 - INFO - __main__ -     global_step = 0\n",
      "================================================================================\n",
      "Best F1 0.06251285214887929\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.7648:   4%|▉                       | 799/20000 [05:12<1:06:21,  4.82it/s]11/11/2019 00:55:35 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 00:55:35 - INFO - __main__ -     global_step = 200\n",
      "11/11/2019 00:55:35 - INFO - __main__ -     train loss = 0.7648\n",
      "loss 0.4748:   8%|█▊                     | 1599/20000 [07:44<1:03:17,  4.85it/s]11/11/2019 00:58:07 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 00:58:07 - INFO - __main__ -     global_step = 400\n",
      "11/11/2019 00:58:07 - INFO - __main__ -     train loss = 0.4748\n",
      "loss 0.4347:  12%|██▊                    | 2399/20000 [10:16<1:00:23,  4.86it/s]11/11/2019 01:00:39 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 01:00:39 - INFO - __main__ -     global_step = 600\n",
      "11/11/2019 01:00:39 - INFO - __main__ -     train loss = 0.4347\n",
      "loss 0.388:  16%|████▏                     | 3199/20000 [12:48<57:58,  4.83it/s]11/11/2019 01:03:11 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 01:03:11 - INFO - __main__ -     global_step = 800\n",
      "11/11/2019 01:03:11 - INFO - __main__ -     train loss = 0.388\n",
      "loss 0.3677:  20%|████▉                    | 3999/20000 [15:21<55:34,  4.80it/s]11/11/2019 01:05:44 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 01:05:44 - INFO - __main__ -     global_step = 1000\n",
      "11/11/2019 01:05:44 - INFO - __main__ -     train loss = 0.3677\n",
      "loss 0.3986:  24%|█████▉                   | 4799/20000 [17:53<52:30,  4.82it/s]11/11/2019 01:08:16 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 01:08:16 - INFO - __main__ -     global_step = 1200\n",
      "11/11/2019 01:08:16 - INFO - __main__ -     train loss = 0.3986\n",
      "loss 0.365:  28%|███████▎                  | 5599/20000 [20:25<49:45,  4.82it/s]11/11/2019 01:10:48 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 01:10:48 - INFO - __main__ -     global_step = 1400\n",
      "11/11/2019 01:10:48 - INFO - __main__ -     train loss = 0.365\n",
      "loss 0.3972:  32%|███████▉                 | 6399/20000 [22:58<47:00,  4.82it/s]11/11/2019 01:13:21 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 01:13:21 - INFO - __main__ -     global_step = 1600\n",
      "11/11/2019 01:13:21 - INFO - __main__ -     train loss = 0.3972\n",
      "loss 0.3722:  36%|████████▉                | 7199/20000 [25:29<44:12,  4.83it/s]11/11/2019 01:15:52 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 01:15:52 - INFO - __main__ -     global_step = 1800\n",
      "11/11/2019 01:15:52 - INFO - __main__ -     train loss = 0.3722\n",
      "11/11/2019 01:16:15 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 01:16:15 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 01:16:15 - INFO - __main__ -     Batch size = 32\n",
      "11/11/2019 01:18:34 - INFO - __main__ -     eval_F1 = 0.748214890503482\n",
      "11/11/2019 01:18:34 - INFO - __main__ -     eval_loss = 0.35390238974081434\n",
      "11/11/2019 01:18:34 - INFO - __main__ -     global_step = 1800\n",
      "11/11/2019 01:18:34 - INFO - __main__ -     loss = 0.3722\n",
      "================================================================================\n",
      "Best F1 0.748214890503482\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.3453:  40%|█████████▉               | 7999/20000 [30:46<41:27,  4.83it/s]11/11/2019 01:21:09 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 01:21:09 - INFO - __main__ -     global_step = 2000\n",
      "11/11/2019 01:21:09 - INFO - __main__ -     train loss = 0.3453\n",
      "11/11/2019 01:21:33 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 01:21:33 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 01:21:33 - INFO - __main__ -     Batch size = 32\n",
      "11/11/2019 01:23:52 - INFO - __main__ -     eval_F1 = 0.788846674334486\n",
      "11/11/2019 01:23:52 - INFO - __main__ -     eval_loss = 0.33670860063284636\n",
      "11/11/2019 01:23:52 - INFO - __main__ -     global_step = 2000\n",
      "11/11/2019 01:23:52 - INFO - __main__ -     loss = 0.3453\n",
      "================================================================================\n",
      "Best F1 0.788846674334486\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.3571:  44%|██████████▉              | 8799/20000 [36:05<38:42,  4.82it/s]11/11/2019 01:26:28 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 01:26:28 - INFO - __main__ -     global_step = 2200\n",
      "11/11/2019 01:26:28 - INFO - __main__ -     train loss = 0.3571\n",
      "11/11/2019 01:26:49 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 01:26:49 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 01:26:49 - INFO - __main__ -     Batch size = 32\n",
      "11/11/2019 01:29:08 - INFO - __main__ -     eval_F1 = 0.8107186959049564\n",
      "11/11/2019 01:29:08 - INFO - __main__ -     eval_loss = 0.32747938577085733\n",
      "11/11/2019 01:29:08 - INFO - __main__ -     global_step = 2200\n",
      "11/11/2019 01:29:08 - INFO - __main__ -     loss = 0.3571\n",
      "================================================================================\n",
      "Best F1 0.8107186959049564\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.3641:  48%|███████████▉             | 9599/20000 [41:22<35:47,  4.84it/s]11/11/2019 01:31:45 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 01:31:45 - INFO - __main__ -     global_step = 2400\n",
      "11/11/2019 01:31:45 - INFO - __main__ -     train loss = 0.3641\n",
      "11/11/2019 01:32:07 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 01:32:07 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 01:32:07 - INFO - __main__ -     Batch size = 32\n",
      "11/11/2019 01:34:26 - INFO - __main__ -     eval_F1 = 0.7728779843459699\n",
      "11/11/2019 01:34:26 - INFO - __main__ -     eval_loss = 0.3379017055520545\n",
      "11/11/2019 01:34:26 - INFO - __main__ -     global_step = 2400\n",
      "11/11/2019 01:34:26 - INFO - __main__ -     loss = 0.3641\n",
      "================================================================================\n",
      "loss 0.3429:  52%|████████████▍           | 10399/20000 [46:36<33:13,  4.82it/s]11/11/2019 01:36:59 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 01:36:59 - INFO - __main__ -     global_step = 2600\n",
      "11/11/2019 01:36:59 - INFO - __main__ -     train loss = 0.3429\n",
      "11/11/2019 01:37:22 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 01:37:22 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 01:37:22 - INFO - __main__ -     Batch size = 32\n",
      "11/11/2019 01:39:41 - INFO - __main__ -     eval_F1 = 0.8125252668111914\n",
      "11/11/2019 01:39:41 - INFO - __main__ -     eval_loss = 0.3313831832340878\n",
      "11/11/2019 01:39:41 - INFO - __main__ -     global_step = 2600\n",
      "11/11/2019 01:39:41 - INFO - __main__ -     loss = 0.3429\n",
      "================================================================================\n",
      "Best F1 0.8125252668111914\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.337:  56%|█████████████▉           | 11199/20000 [51:54<30:27,  4.82it/s]11/11/2019 01:42:17 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 01:42:17 - INFO - __main__ -     global_step = 2800\n",
      "11/11/2019 01:42:17 - INFO - __main__ -     train loss = 0.337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11/2019 01:42:40 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 01:42:40 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 01:42:40 - INFO - __main__ -     Batch size = 32\n",
      "11/11/2019 01:44:59 - INFO - __main__ -     eval_F1 = 0.8119198519154377\n",
      "11/11/2019 01:44:59 - INFO - __main__ -     eval_loss = 0.3356870121687003\n",
      "11/11/2019 01:44:59 - INFO - __main__ -     global_step = 2800\n",
      "11/11/2019 01:44:59 - INFO - __main__ -     loss = 0.337\n",
      "================================================================================\n",
      "loss 0.3921:  60%|██████████████▍         | 11999/20000 [57:09<27:45,  4.81it/s]11/11/2019 01:47:32 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 01:47:32 - INFO - __main__ -     global_step = 3000\n",
      "11/11/2019 01:47:32 - INFO - __main__ -     train loss = 0.3921\n",
      "11/11/2019 01:47:54 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 01:47:54 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 01:47:54 - INFO - __main__ -     Batch size = 32\n",
      "11/11/2019 01:50:13 - INFO - __main__ -     eval_F1 = 0.802802408661106\n",
      "11/11/2019 01:50:13 - INFO - __main__ -     eval_loss = 0.32326097041368484\n",
      "11/11/2019 01:50:13 - INFO - __main__ -     global_step = 3000\n",
      "11/11/2019 01:50:13 - INFO - __main__ -     loss = 0.3921\n",
      "================================================================================\n",
      "loss 0.2595:  64%|██████████████        | 12799/20000 [1:02:23<24:54,  4.82it/s]11/11/2019 01:52:46 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 01:52:46 - INFO - __main__ -     global_step = 3200\n",
      "11/11/2019 01:52:46 - INFO - __main__ -     train loss = 0.2595\n",
      "11/11/2019 01:53:09 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 01:53:09 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 01:53:09 - INFO - __main__ -     Batch size = 32\n",
      "11/11/2019 01:55:27 - INFO - __main__ -     eval_F1 = 0.8103914918076018\n",
      "11/11/2019 01:55:27 - INFO - __main__ -     eval_loss = 0.31674664674083824\n",
      "11/11/2019 01:55:27 - INFO - __main__ -     global_step = 3200\n",
      "11/11/2019 01:55:27 - INFO - __main__ -     loss = 0.2595\n",
      "================================================================================\n",
      "loss 0.2746:  68%|██████████████▉       | 13599/20000 [1:07:38<22:18,  4.78it/s]11/11/2019 01:58:00 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 01:58:00 - INFO - __main__ -     global_step = 3400\n",
      "11/11/2019 01:58:00 - INFO - __main__ -     train loss = 0.2746\n",
      "11/11/2019 01:58:24 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 01:58:24 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 01:58:24 - INFO - __main__ -     Batch size = 32\n",
      "11/11/2019 02:00:43 - INFO - __main__ -     eval_F1 = 0.8152274075860548\n",
      "11/11/2019 02:00:43 - INFO - __main__ -     eval_loss = 0.3267178517239897\n",
      "11/11/2019 02:00:43 - INFO - __main__ -     global_step = 3400\n",
      "11/11/2019 02:00:43 - INFO - __main__ -     loss = 0.2746\n",
      "================================================================================\n",
      "Best F1 0.8152274075860548\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.2343:  72%|███████████████▊      | 14399/20000 [1:12:56<19:21,  4.82it/s]11/11/2019 02:03:19 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 02:03:19 - INFO - __main__ -     global_step = 3600\n",
      "11/11/2019 02:03:19 - INFO - __main__ -     train loss = 0.2343\n",
      "11/11/2019 02:03:43 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 02:03:43 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 02:03:43 - INFO - __main__ -     Batch size = 32\n",
      "11/11/2019 02:06:01 - INFO - __main__ -     eval_F1 = 0.7984358387905216\n",
      "11/11/2019 02:06:01 - INFO - __main__ -     eval_loss = 0.34717131774548604\n",
      "11/11/2019 02:06:01 - INFO - __main__ -     global_step = 3600\n",
      "11/11/2019 02:06:01 - INFO - __main__ -     loss = 0.2343\n",
      "================================================================================\n",
      "loss 0.2254:  76%|████████████████▋     | 15199/20000 [1:18:11<16:36,  4.82it/s]11/11/2019 02:08:34 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 02:08:34 - INFO - __main__ -     global_step = 3800\n",
      "11/11/2019 02:08:34 - INFO - __main__ -     train loss = 0.2254\n",
      "11/11/2019 02:08:58 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 02:08:58 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 02:08:58 - INFO - __main__ -     Batch size = 32\n",
      "11/11/2019 02:11:17 - INFO - __main__ -     eval_F1 = 0.8100305882590714\n",
      "11/11/2019 02:11:17 - INFO - __main__ -     eval_loss = 0.3381252606075419\n",
      "11/11/2019 02:11:17 - INFO - __main__ -     global_step = 3800\n",
      "11/11/2019 02:11:17 - INFO - __main__ -     loss = 0.2254\n",
      "================================================================================\n",
      "loss 0.2335:  80%|█████████████████▌    | 15999/20000 [1:23:27<13:54,  4.80it/s]11/11/2019 02:13:50 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 02:13:50 - INFO - __main__ -     global_step = 4000\n",
      "11/11/2019 02:13:50 - INFO - __main__ -     train loss = 0.2335\n",
      "11/11/2019 02:14:13 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 02:14:13 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 02:14:13 - INFO - __main__ -     Batch size = 32\n",
      "11/11/2019 02:16:31 - INFO - __main__ -     eval_F1 = 0.8042314019547785\n",
      "11/11/2019 02:16:31 - INFO - __main__ -     eval_loss = 0.33708455733469\n",
      "11/11/2019 02:16:31 - INFO - __main__ -     global_step = 4000\n",
      "11/11/2019 02:16:31 - INFO - __main__ -     loss = 0.2335\n",
      "================================================================================\n",
      "loss 0.2269:  84%|██████████████████▍   | 16799/20000 [1:28:42<11:06,  4.80it/s]11/11/2019 02:19:05 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 02:19:05 - INFO - __main__ -     global_step = 4200\n",
      "11/11/2019 02:19:05 - INFO - __main__ -     train loss = 0.2269\n",
      "11/11/2019 02:19:26 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 02:19:26 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 02:19:26 - INFO - __main__ -     Batch size = 32\n",
      "11/11/2019 02:21:45 - INFO - __main__ -     eval_F1 = 0.8169811702435593\n",
      "11/11/2019 02:21:45 - INFO - __main__ -     eval_loss = 0.33725427406961506\n",
      "11/11/2019 02:21:45 - INFO - __main__ -     global_step = 4200\n",
      "11/11/2019 02:21:45 - INFO - __main__ -     loss = 0.2269\n",
      "================================================================================\n",
      "Best F1 0.8169811702435593\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.2374:  88%|███████████████████▎  | 17599/20000 [1:33:59<08:18,  4.82it/s]11/11/2019 02:24:22 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 02:24:22 - INFO - __main__ -     global_step = 4400\n",
      "11/11/2019 02:24:22 - INFO - __main__ -     train loss = 0.2374\n",
      "11/11/2019 02:24:45 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 02:24:45 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 02:24:45 - INFO - __main__ -     Batch size = 32\n",
      "11/11/2019 02:27:04 - INFO - __main__ -     eval_F1 = 0.8221574786712047\n",
      "11/11/2019 02:27:04 - INFO - __main__ -     eval_loss = 0.33559514723880135\n",
      "11/11/2019 02:27:04 - INFO - __main__ -     global_step = 4400\n",
      "11/11/2019 02:27:04 - INFO - __main__ -     loss = 0.2374\n",
      "================================================================================\n",
      "Best F1 0.8221574786712047\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.2416:  92%|████████████████████▏ | 18399/20000 [1:39:17<05:35,  4.78it/s]11/11/2019 02:29:40 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 02:29:40 - INFO - __main__ -     global_step = 4600\n",
      "11/11/2019 02:29:40 - INFO - __main__ -     train loss = 0.2416\n",
      "11/11/2019 02:30:04 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 02:30:04 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 02:30:04 - INFO - __main__ -     Batch size = 32\n",
      "11/11/2019 02:32:22 - INFO - __main__ -     eval_F1 = 0.8184054619510954\n",
      "11/11/2019 02:32:22 - INFO - __main__ -     eval_loss = 0.34051715072406374\n",
      "11/11/2019 02:32:22 - INFO - __main__ -     global_step = 4600\n",
      "11/11/2019 02:32:22 - INFO - __main__ -     loss = 0.2416\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.2197:  96%|█████████████████████ | 19199/20000 [1:44:33<02:47,  4.79it/s]11/11/2019 02:34:56 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 02:34:56 - INFO - __main__ -     global_step = 4800\n",
      "11/11/2019 02:34:56 - INFO - __main__ -     train loss = 0.2197\n",
      "11/11/2019 02:35:20 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 02:35:20 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 02:35:20 - INFO - __main__ -     Batch size = 32\n",
      "11/11/2019 02:37:39 - INFO - __main__ -     eval_F1 = 0.8164966754102361\n",
      "11/11/2019 02:37:39 - INFO - __main__ -     eval_loss = 0.3334363781968537\n",
      "11/11/2019 02:37:39 - INFO - __main__ -     global_step = 4800\n",
      "11/11/2019 02:37:39 - INFO - __main__ -     loss = 0.2197\n",
      "================================================================================\n",
      "loss 0.1946: 100%|█████████████████████▉| 19999/20000 [1:49:49<00:00,  4.79it/s]11/11/2019 02:40:12 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 02:40:12 - INFO - __main__ -     global_step = 5000\n",
      "11/11/2019 02:40:12 - INFO - __main__ -     train loss = 0.1946\n",
      "11/11/2019 02:40:35 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 02:40:35 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 02:40:35 - INFO - __main__ -     Batch size = 32\n",
      "11/11/2019 02:42:54 - INFO - __main__ -     eval_F1 = 0.8166947979106971\n",
      "11/11/2019 02:42:54 - INFO - __main__ -     eval_loss = 0.3337720118705993\n",
      "11/11/2019 02:42:54 - INFO - __main__ -     global_step = 5000\n",
      "11/11/2019 02:42:54 - INFO - __main__ -     loss = 0.1946\n",
      "================================================================================\n",
      "loss 0.1946: 100%|██████████████████████| 20000/20000 [1:52:31<00:00, 48.75s/it]\n",
      "11/11/2019 02:42:54 - INFO - pytorch_transformers.modeling_utils -   loading weights file ../model/roberta_wwm_large_3002_gru1_42/roberta_wwm_large_3002_gru1_3/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "dev 0.8221574786712047\n",
      "/root/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "test 0.05939123979213066\n"
     ]
    }
   ],
   "source": [
    "# base:  首尾\n",
    "!python ./run_bert_2562.py \\\n",
    "--model_type bert \\\n",
    "--model_name_or_path ../model/chinese_roberta_wwm_large_ext_pytorch  \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_test \\\n",
    "--data_dir ../data/data_StratifiedKFold_42/data_origin_3 \\\n",
    "--output_dir ../model/roberta_wwm_large_3002_gru1_42/roberta_wwm_large_3002_gru1_3 \\\n",
    "--max_seq_length 300 \\\n",
    "--split_num 2 \\\n",
    "--lstm_hidden_size 512 \\\n",
    "--lstm_layers 1 \\\n",
    "--lstm_dropout 0.1 \\\n",
    "--eval_steps 200 \\\n",
    "--per_gpu_train_batch_size 4 \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--warmup_steps 0 \\\n",
    "--per_gpu_eval_batch_size 32 \\\n",
    "--learning_rate 5e-6 \\\n",
    "--adam_epsilon 1e-6 \\\n",
    "--weight_decay 0 \\\n",
    "--train_steps 20000 \\\n",
    "--freeze 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11/2019 02:52:30 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "11/11/2019 02:52:30 - INFO - pytorch_transformers.tokenization_utils -   Model name '../model/chinese_roberta_wwm_large_ext_pytorch' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming '../model/chinese_roberta_wwm_large_ext_pytorch' is a path or url to a directory containing tokenizer files.\n",
      "11/11/2019 02:52:30 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../model/chinese_roberta_wwm_large_ext_pytorch/added_tokens.json. We won't load it.\n",
      "11/11/2019 02:52:30 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../model/chinese_roberta_wwm_large_ext_pytorch/special_tokens_map.json. We won't load it.\n",
      "11/11/2019 02:52:30 - INFO - pytorch_transformers.tokenization_utils -   loading file ../model/chinese_roberta_wwm_large_ext_pytorch/vocab.txt\n",
      "11/11/2019 02:52:30 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/11/2019 02:52:30 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/11/2019 02:52:30 - INFO - pytorch_transformers.modeling_utils -   loading configuration file ../model/chinese_roberta_wwm_large_ext_pytorch/config.json\n",
      "11/11/2019 02:52:30 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 3,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "11/11/2019 02:52:30 - INFO - pytorch_transformers.modeling_utils -   loading weights file ../model/chinese_roberta_wwm_large_ext_pytorch/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "11/11/2019 02:52:45 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'W.0.weight', 'W.0.bias', 'gru.0.weight_ih_l0', 'gru.0.weight_hh_l0', 'gru.0.bias_ih_l0', 'gru.0.bias_hh_l0', 'gru.0.weight_ih_l0_reverse', 'gru.0.weight_hh_l0_reverse', 'gru.0.bias_ih_l0_reverse', 'gru.0.bias_hh_l0_reverse']\n",
      "11/11/2019 02:52:45 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "11/11/2019 02:52:45 - INFO - __main__ -   *** Example ***\n",
      "11/11/2019 02:52:45 - INFO - __main__ -   idx: 0\n",
      "11/11/2019 02:52:45 - INFO - __main__ -   guid: 7a3dd79f90ee419da87190cff60f7a86\n",
      "11/11/2019 02:52:45 - INFO - __main__ -   tokens: [CLS] 问 责 领 导 ( 上 黄 镇 党 委 书 记 张 涛 ， 宣 国 才 真 能 一 手 遮 天 吗 ？ ) [SEP] 这 几 天 看 了 有 人 举 报 施 某 某 的 贴 子 ， 经 与 举 报 人 联 系 证 实 ， 是 宣 某 当 天 中 午 请 举 报 人 和 枪 手 喝 酒 后 ， 晚 上 才 发 的 贴 子 ！ 本 人 不 去 讨 论 前 二 天 的 举 报 ， 相 信 总 归 会 有 说 法 的 ！ 今 天 一 看 施 全 军 2017 年 1 月 2 日 实 名 举 报 上 黄 镇 宣 国 才 的 贴 子 （ 仍 被 锁 定 禁 止 评 论 ） 已 经 正 好 一 整 年 了 = 750 ) window . open ( ' http : / / img . js ##ly ##001 . com / at ##ta ##ch ##ment / mon _ 180 ##1 / 4 _ 291 ##08 ##5 _ c ##79 ##6 ##a ##6 ##a ##86 ##e ##17 ##12 ##1 . jpg ? 123 ' ) ; \" on ##load = \" if ( this . off ##set ##wi ##dt ##h > ' 750 ' ) this . wi ##dt ##h = ' 750 ' ; \" sr ##c = \" http : / / img . js ##ly ##001 . com / at ##ta ##ch ##ment / mon _ 180 ##1 / 4 _ 291 ##08 ##5 _ c ##79 ##6 ##a ##6 ##a ##86 ##e ##17 ##12 ##1 . jpg ? 123 \" style = \" max - wi ##dt ##h : 750 ##px ; \" / > 图 片 : / [SEP]\n",
      "11/11/2019 02:52:45 - INFO - __main__ -   input_ids: 101 7309 6569 7566 2193 113 677 7942 7252 1054 1999 741 6381 2476 3875 8024 2146 1744 2798 4696 5543 671 2797 6902 1921 1408 8043 114 102 6821 1126 1921 4692 749 3300 782 715 2845 3177 3378 3378 4638 6585 2094 8024 5307 680 715 2845 782 5468 5143 6395 2141 8024 3221 2146 3378 2496 1921 704 1286 6435 715 2845 782 1469 3366 2797 1600 6983 1400 8024 3241 677 2798 1355 4638 6585 2094 8013 3315 782 679 1343 6374 6389 1184 753 1921 4638 715 2845 8024 4685 928 2600 2495 833 3300 6432 3791 4638 8013 791 1921 671 4692 3177 1059 1092 8109 2399 122 3299 123 3189 2141 1399 715 2845 677 7942 7252 2146 1744 2798 4638 6585 2094 8020 793 6158 7219 2137 4881 3632 6397 6389 8021 2347 5307 3633 1962 671 3146 2399 749 134 9180 114 12158 119 8893 113 112 8184 131 120 120 11412 119 9016 8436 9942 119 8134 120 8243 8383 8370 8631 120 8556 142 8420 8148 120 125 142 11777 9153 8157 142 145 9495 8158 8139 8158 8139 9219 8154 8408 8455 8148 119 9248 136 8604 112 114 132 107 8281 11713 134 107 8898 113 8554 119 9594 9852 10958 12672 8199 135 112 9180 112 114 8554 119 8541 12672 8199 134 112 9180 112 132 107 12109 8177 134 107 8184 131 120 120 11412 119 9016 8436 9942 119 8134 120 8243 8383 8370 8631 120 8556 142 8420 8148 120 125 142 11777 9153 8157 142 145 9495 8158 8139 8158 8139 9219 8154 8408 8455 8148 119 9248 136 8604 107 8969 134 107 8621 118 8541 12672 8199 131 9180 10605 132 107 120 135 1745 4275 131 120 102\n",
      "11/11/2019 02:52:45 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/11/2019 02:52:45 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/11/2019 02:52:45 - INFO - __main__ -   label: 2\n",
      "11/11/2019 02:52:45 - INFO - __main__ -   *** Example ***\n",
      "11/11/2019 02:52:45 - INFO - __main__ -   idx: 0\n",
      "11/11/2019 02:52:45 - INFO - __main__ -   guid: 7a3dd79f90ee419da87190cff60f7a86\n",
      "11/11/2019 02:52:45 - INFO - __main__ -   tokens: [CLS] 问 责 领 导 ( 上 黄 镇 党 委 书 记 张 涛 ， 宣 国 才 真 能 一 手 遮 天 吗 ？ ) [SEP] / www / data / tm ##p / q ##fu ##pl ##oa ##d / 4 _ 291 ##08 ##5 _ 151 ##49 ##81 ##47 ##23 ##53 ##07 ##5 . jpg 一 年 的 贴 子 ， 再 次 被 网 友 顶 起 来 后 ， 才 发 现 施 某 几 天 前 回 复 网 友 的 处 理 结 果 竟 如 下 图 ： = 750 ) window . open ( ' http : / / img . js ##ly ##001 . com / at ##ta ##ch ##ment / mon _ 180 ##1 / 4 _ 291 ##08 ##5 _ 9 ##d ##32 ##ee ##57 ##27 ##60 ##d ##85 . jpg ? 131 ' ) ; \" on ##load = \" if ( this . off ##set ##wi ##dt ##h > ' 750 ' ) this . wi ##dt ##h = ' 750 ' ; \" sr ##c = \" http : / / img . js ##ly ##001 . com / at ##ta ##ch ##ment / mon _ 180 ##1 / 4 _ 291 ##08 ##5 _ 9 ##d ##32 ##ee ##57 ##27 ##60 ##d ##85 . jpg ? 131 \" style = \" max - wi ##dt ##h : 750 ##px ; \" / > 图 片 : / home / al ##ida ##ta / www / data / tm ##p / q ##fu ##pl ##oa ##d / 4 _ 291 ##08 ##5 _ 151 ##49 ##81 ##47 ##35 ##47 ##17 ##2 . jpg 现 责 问 张 涛 书 记 ： 1 、 宣 国 才 被 举 报 这 么 多 问 [SEP]\n",
      "11/11/2019 02:52:45 - INFO - __main__ -   input_ids: 101 7309 6569 7566 2193 113 677 7942 7252 1054 1999 741 6381 2476 3875 8024 2146 1744 2798 4696 5543 671 2797 6902 1921 1408 8043 114 102 120 8173 120 9000 120 9908 8187 120 159 12043 12569 11355 8168 120 125 142 11777 9153 8157 142 9564 9500 9313 9050 8748 9310 9131 8157 119 9248 671 2399 4638 6585 2094 8024 1086 3613 6158 5381 1351 7553 6629 3341 1400 8024 2798 1355 4385 3177 3378 1126 1921 1184 1726 1908 5381 1351 4638 1905 4415 5310 3362 4994 1963 678 1745 8038 134 9180 114 12158 119 8893 113 112 8184 131 120 120 11412 119 9016 8436 9942 119 8134 120 8243 8383 8370 8631 120 8556 142 8420 8148 120 125 142 11777 9153 8157 142 130 8168 8709 8854 9647 8976 8581 8168 9169 119 9248 136 9403 112 114 132 107 8281 11713 134 107 8898 113 8554 119 9594 9852 10958 12672 8199 135 112 9180 112 114 8554 119 8541 12672 8199 134 112 9180 112 132 107 12109 8177 134 107 8184 131 120 120 11412 119 9016 8436 9942 119 8134 120 8243 8383 8370 8631 120 8556 142 8420 8148 120 125 142 11777 9153 8157 142 130 8168 8709 8854 9647 8976 8581 8168 9169 119 9248 136 9403 107 8969 134 107 8621 118 8541 12672 8199 131 9180 10605 132 107 120 135 1745 4275 131 120 8563 120 9266 12708 8383 120 8173 120 9000 120 9908 8187 120 159 12043 12569 11355 8168 120 125 142 11777 9153 8157 142 9564 9500 9313 9050 8852 9050 8408 8144 119 9248 4385 6569 7309 2476 3875 741 6381 8038 122 510 2146 1744 2798 6158 715 2845 6821 720 1914 7309 102\n",
      "11/11/2019 02:52:45 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/11/2019 02:52:45 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/11/2019 02:52:45 - INFO - __main__ -   label: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11/2019 02:54:13 - INFO - __main__ -   ***** Running training *****\n",
      "11/11/2019 02:54:13 - INFO - __main__ -     Num examples = 11758\n",
      "11/11/2019 02:54:13 - INFO - __main__ -     Batch size = 4\n",
      "11/11/2019 02:54:13 - INFO - __main__ -     Num steps = 20000\n",
      "  0%|                                                 | 0/20000 [00:00<?, ?it/s]11/11/2019 02:54:37 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 02:54:37 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 02:54:37 - INFO - __main__ -     Batch size = 32\n",
      "/root/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "11/11/2019 02:56:54 - INFO - __main__ -     eval_F1 = 0.06309903370995627\n",
      "11/11/2019 02:56:54 - INFO - __main__ -     eval_loss = 1.188367381044056\n",
      "11/11/2019 02:56:54 - INFO - __main__ -     global_step = 0\n",
      "================================================================================\n",
      "Best F1 0.06309903370995627\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.7386:   4%|▉                       | 799/20000 [05:14<1:06:25,  4.82it/s]11/11/2019 02:59:27 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 02:59:27 - INFO - __main__ -     global_step = 200\n",
      "11/11/2019 02:59:27 - INFO - __main__ -     train loss = 0.7386\n",
      "loss 0.4673:   8%|█▊                     | 1599/20000 [07:46<1:03:22,  4.84it/s]11/11/2019 03:01:59 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 03:01:59 - INFO - __main__ -     global_step = 400\n",
      "11/11/2019 03:01:59 - INFO - __main__ -     train loss = 0.4673\n",
      "loss 0.4668:  12%|██▊                    | 2399/20000 [10:18<1:00:36,  4.84it/s]11/11/2019 03:04:31 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 03:04:31 - INFO - __main__ -     global_step = 600\n",
      "11/11/2019 03:04:31 - INFO - __main__ -     train loss = 0.4668\n",
      "loss 0.4047:  16%|███▉                     | 3199/20000 [12:50<57:48,  4.84it/s]11/11/2019 03:07:03 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 03:07:03 - INFO - __main__ -     global_step = 800\n",
      "11/11/2019 03:07:03 - INFO - __main__ -     train loss = 0.4047\n",
      "loss 0.3839:  20%|████▉                    | 3999/20000 [15:22<55:06,  4.84it/s]11/11/2019 03:09:35 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 03:09:35 - INFO - __main__ -     global_step = 1000\n",
      "11/11/2019 03:09:35 - INFO - __main__ -     train loss = 0.3839\n",
      "loss 0.354:  24%|██████▏                   | 4799/20000 [17:54<52:32,  4.82it/s]11/11/2019 03:12:07 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 03:12:07 - INFO - __main__ -     global_step = 1200\n",
      "11/11/2019 03:12:07 - INFO - __main__ -     train loss = 0.354\n",
      "loss 0.3644:  28%|██████▉                  | 5599/20000 [20:26<50:14,  4.78it/s]11/11/2019 03:14:39 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 03:14:39 - INFO - __main__ -     global_step = 1400\n",
      "11/11/2019 03:14:39 - INFO - __main__ -     train loss = 0.3644\n",
      "loss 0.3747:  32%|███████▉                 | 6399/20000 [22:58<46:32,  4.87it/s]11/11/2019 03:17:11 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 03:17:11 - INFO - __main__ -     global_step = 1600\n",
      "11/11/2019 03:17:11 - INFO - __main__ -     train loss = 0.3747\n",
      "loss 0.4111:  36%|████████▉                | 7199/20000 [25:30<43:59,  4.85it/s]11/11/2019 03:19:43 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 03:19:43 - INFO - __main__ -     global_step = 1800\n",
      "11/11/2019 03:19:43 - INFO - __main__ -     train loss = 0.4111\n",
      "11/11/2019 03:20:07 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 03:20:07 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 03:20:07 - INFO - __main__ -     Batch size = 32\n",
      "11/11/2019 03:22:25 - INFO - __main__ -     eval_F1 = 0.7529520476521091\n",
      "11/11/2019 03:22:25 - INFO - __main__ -     eval_loss = 0.3646431660036678\n",
      "11/11/2019 03:22:25 - INFO - __main__ -     global_step = 1800\n",
      "11/11/2019 03:22:25 - INFO - __main__ -     loss = 0.4111\n",
      "================================================================================\n",
      "Best F1 0.7529520476521091\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.348:  40%|██████████▍               | 7999/20000 [30:48<41:35,  4.81it/s]11/11/2019 03:25:01 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 03:25:01 - INFO - __main__ -     global_step = 2000\n",
      "11/11/2019 03:25:01 - INFO - __main__ -     train loss = 0.348\n",
      "11/11/2019 03:25:24 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 03:25:24 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 03:25:24 - INFO - __main__ -     Batch size = 32\n",
      "11/11/2019 03:27:43 - INFO - __main__ -     eval_F1 = 0.7872407708638206\n",
      "11/11/2019 03:27:43 - INFO - __main__ -     eval_loss = 0.35678404861170315\n",
      "11/11/2019 03:27:43 - INFO - __main__ -     global_step = 2000\n",
      "11/11/2019 03:27:43 - INFO - __main__ -     loss = 0.348\n",
      "================================================================================\n",
      "Best F1 0.7872407708638206\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.3666:  44%|██████████▉              | 8799/20000 [36:05<38:41,  4.83it/s]11/11/2019 03:30:19 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 03:30:19 - INFO - __main__ -     global_step = 2200\n",
      "11/11/2019 03:30:19 - INFO - __main__ -     train loss = 0.3666\n",
      "11/11/2019 03:30:42 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 03:30:42 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 03:30:42 - INFO - __main__ -     Batch size = 32\n",
      "11/11/2019 03:33:01 - INFO - __main__ -     eval_F1 = 0.7874790488562206\n",
      "11/11/2019 03:33:01 - INFO - __main__ -     eval_loss = 0.34819522605318093\n",
      "11/11/2019 03:33:01 - INFO - __main__ -     global_step = 2200\n",
      "11/11/2019 03:33:01 - INFO - __main__ -     loss = 0.3666\n",
      "================================================================================\n",
      "Best F1 0.7874790488562206\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.3734:  48%|███████████▉             | 9599/20000 [41:23<35:47,  4.84it/s]11/11/2019 03:35:37 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 03:35:37 - INFO - __main__ -     global_step = 2400\n",
      "11/11/2019 03:35:37 - INFO - __main__ -     train loss = 0.3734\n",
      "11/11/2019 03:36:00 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 03:36:00 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 03:36:00 - INFO - __main__ -     Batch size = 32\n",
      "11/11/2019 03:38:19 - INFO - __main__ -     eval_F1 = 0.7955552978968048\n",
      "11/11/2019 03:38:19 - INFO - __main__ -     eval_loss = 0.3423037051182726\n",
      "11/11/2019 03:38:19 - INFO - __main__ -     global_step = 2400\n",
      "11/11/2019 03:38:19 - INFO - __main__ -     loss = 0.3734\n",
      "================================================================================\n",
      "Best F1 0.7955552978968048\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.3381:  52%|████████████▍           | 10399/20000 [46:41<33:05,  4.84it/s]11/11/2019 03:40:55 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 03:40:55 - INFO - __main__ -     global_step = 2600\n",
      "11/11/2019 03:40:55 - INFO - __main__ -     train loss = 0.3381\n",
      "11/11/2019 03:41:18 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 03:41:18 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 03:41:18 - INFO - __main__ -     Batch size = 32\n",
      "11/11/2019 03:43:37 - INFO - __main__ -     eval_F1 = 0.7918585736844825\n",
      "11/11/2019 03:43:37 - INFO - __main__ -     eval_loss = 0.3449531608949537\n",
      "11/11/2019 03:43:37 - INFO - __main__ -     global_step = 2600\n",
      "11/11/2019 03:43:37 - INFO - __main__ -     loss = 0.3381\n",
      "================================================================================\n",
      "loss 0.3448:  56%|█████████████▍          | 11199/20000 [51:56<30:19,  4.84it/s]11/11/2019 03:46:09 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 03:46:09 - INFO - __main__ -     global_step = 2800\n",
      "11/11/2019 03:46:09 - INFO - __main__ -     train loss = 0.3448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11/2019 03:46:33 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 03:46:33 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 03:46:33 - INFO - __main__ -     Batch size = 32\n",
      "11/11/2019 03:48:52 - INFO - __main__ -     eval_F1 = 0.7868938402478332\n",
      "11/11/2019 03:48:52 - INFO - __main__ -     eval_loss = 0.35448958663998736\n",
      "11/11/2019 03:48:52 - INFO - __main__ -     global_step = 2800\n",
      "11/11/2019 03:48:52 - INFO - __main__ -     loss = 0.3448\n",
      "================================================================================\n",
      "loss 0.3311:  60%|██████████████▍         | 11999/20000 [57:11<27:43,  4.81it/s]11/11/2019 03:51:25 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 03:51:25 - INFO - __main__ -     global_step = 3000\n",
      "11/11/2019 03:51:25 - INFO - __main__ -     train loss = 0.3311\n",
      "11/11/2019 03:51:48 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 03:51:48 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 03:51:48 - INFO - __main__ -     Batch size = 32\n",
      "11/11/2019 03:54:06 - INFO - __main__ -     eval_F1 = 0.7968631645400787\n",
      "11/11/2019 03:54:06 - INFO - __main__ -     eval_loss = 0.35297098132255283\n",
      "11/11/2019 03:54:06 - INFO - __main__ -     global_step = 3000\n",
      "11/11/2019 03:54:06 - INFO - __main__ -     loss = 0.3311\n",
      "================================================================================\n",
      "Best F1 0.7968631645400787\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.276:  64%|██████████████▋        | 12799/20000 [1:02:29<25:06,  4.78it/s]11/11/2019 03:56:43 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 03:56:43 - INFO - __main__ -     global_step = 3200\n",
      "11/11/2019 03:56:43 - INFO - __main__ -     train loss = 0.276\n",
      "11/11/2019 03:57:06 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 03:57:06 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 03:57:06 - INFO - __main__ -     Batch size = 32\n",
      "11/11/2019 03:59:25 - INFO - __main__ -     eval_F1 = 0.7886761343424723\n",
      "11/11/2019 03:59:25 - INFO - __main__ -     eval_loss = 0.3593002626591403\n",
      "11/11/2019 03:59:25 - INFO - __main__ -     global_step = 3200\n",
      "11/11/2019 03:59:25 - INFO - __main__ -     loss = 0.276\n",
      "================================================================================\n",
      "loss 0.2397:  68%|██████████████▉       | 13599/20000 [1:07:44<22:07,  4.82it/s]11/11/2019 04:01:58 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 04:01:58 - INFO - __main__ -     global_step = 3400\n",
      "11/11/2019 04:01:58 - INFO - __main__ -     train loss = 0.2397\n",
      "11/11/2019 04:02:21 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 04:02:21 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 04:02:21 - INFO - __main__ -     Batch size = 32\n",
      "11/11/2019 04:04:40 - INFO - __main__ -     eval_F1 = 0.8012369442645789\n",
      "11/11/2019 04:04:40 - INFO - __main__ -     eval_loss = 0.35602592203118233\n",
      "11/11/2019 04:04:40 - INFO - __main__ -     global_step = 3400\n",
      "11/11/2019 04:04:40 - INFO - __main__ -     loss = 0.2397\n",
      "================================================================================\n",
      "Best F1 0.8012369442645789\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.2655:  72%|███████████████▊      | 14399/20000 [1:13:02<19:22,  4.82it/s]11/11/2019 04:07:16 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 04:07:16 - INFO - __main__ -     global_step = 3600\n",
      "11/11/2019 04:07:16 - INFO - __main__ -     train loss = 0.2655\n",
      "11/11/2019 04:07:38 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 04:07:38 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 04:07:38 - INFO - __main__ -     Batch size = 32\n",
      "11/11/2019 04:09:57 - INFO - __main__ -     eval_F1 = 0.7956093183353973\n",
      "11/11/2019 04:09:57 - INFO - __main__ -     eval_loss = 0.3501980394205969\n",
      "11/11/2019 04:09:57 - INFO - __main__ -     global_step = 3600\n",
      "11/11/2019 04:09:57 - INFO - __main__ -     loss = 0.2655\n",
      "================================================================================\n",
      "loss 0.2195:  76%|████████████████▋     | 15199/20000 [1:18:17<16:37,  4.81it/s]11/11/2019 04:12:30 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 04:12:30 - INFO - __main__ -     global_step = 3800\n",
      "11/11/2019 04:12:30 - INFO - __main__ -     train loss = 0.2195\n",
      "11/11/2019 04:12:54 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 04:12:54 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 04:12:54 - INFO - __main__ -     Batch size = 32\n",
      "11/11/2019 04:15:13 - INFO - __main__ -     eval_F1 = 0.7923710287671776\n",
      "11/11/2019 04:15:13 - INFO - __main__ -     eval_loss = 0.382802989333868\n",
      "11/11/2019 04:15:13 - INFO - __main__ -     global_step = 3800\n",
      "11/11/2019 04:15:13 - INFO - __main__ -     loss = 0.2195\n",
      "================================================================================\n",
      "loss 0.2402:  80%|█████████████████▌    | 15999/20000 [1:23:33<13:54,  4.80it/s]11/11/2019 04:17:46 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 04:17:46 - INFO - __main__ -     global_step = 4000\n",
      "11/11/2019 04:17:46 - INFO - __main__ -     train loss = 0.2402\n",
      "11/11/2019 04:18:10 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 04:18:10 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 04:18:10 - INFO - __main__ -     Batch size = 32\n",
      "11/11/2019 04:20:28 - INFO - __main__ -     eval_F1 = 0.7936529313382991\n",
      "11/11/2019 04:20:28 - INFO - __main__ -     eval_loss = 0.3633212734661672\n",
      "11/11/2019 04:20:28 - INFO - __main__ -     global_step = 4000\n",
      "11/11/2019 04:20:28 - INFO - __main__ -     loss = 0.2402\n",
      "================================================================================\n",
      "loss 0.2057:  84%|██████████████████▍   | 16799/20000 [1:28:48<11:06,  4.80it/s]11/11/2019 04:23:02 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 04:23:02 - INFO - __main__ -     global_step = 4200\n",
      "11/11/2019 04:23:02 - INFO - __main__ -     train loss = 0.2057\n",
      "11/11/2019 04:23:25 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 04:23:25 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 04:23:25 - INFO - __main__ -     Batch size = 32\n",
      "11/11/2019 04:25:44 - INFO - __main__ -     eval_F1 = 0.7924889364605234\n",
      "11/11/2019 04:25:44 - INFO - __main__ -     eval_loss = 0.36308206613782956\n",
      "11/11/2019 04:25:44 - INFO - __main__ -     global_step = 4200\n",
      "11/11/2019 04:25:44 - INFO - __main__ -     loss = 0.2057\n",
      "================================================================================\n",
      "loss 0.2335:  88%|███████████████████▎  | 17599/20000 [1:34:03<08:18,  4.82it/s]11/11/2019 04:28:17 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 04:28:17 - INFO - __main__ -     global_step = 4400\n",
      "11/11/2019 04:28:17 - INFO - __main__ -     train loss = 0.2335\n",
      "11/11/2019 04:28:40 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 04:28:40 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 04:28:40 - INFO - __main__ -     Batch size = 32\n",
      "11/11/2019 04:30:58 - INFO - __main__ -     eval_F1 = 0.7935550601948799\n",
      "11/11/2019 04:30:58 - INFO - __main__ -     eval_loss = 0.35604943043511844\n",
      "11/11/2019 04:30:58 - INFO - __main__ -     global_step = 4400\n",
      "11/11/2019 04:30:58 - INFO - __main__ -     loss = 0.2335\n",
      "================================================================================\n",
      "loss 0.2233:  92%|████████████████████▏ | 18399/20000 [1:39:18<05:32,  4.81it/s]11/11/2019 04:33:32 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 04:33:32 - INFO - __main__ -     global_step = 4600\n",
      "11/11/2019 04:33:32 - INFO - __main__ -     train loss = 0.2233\n",
      "11/11/2019 04:33:54 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 04:33:54 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 04:33:54 - INFO - __main__ -     Batch size = 32\n",
      "11/11/2019 04:36:13 - INFO - __main__ -     eval_F1 = 0.7974032907718205\n",
      "11/11/2019 04:36:13 - INFO - __main__ -     eval_loss = 0.3579515792915355\n",
      "11/11/2019 04:36:13 - INFO - __main__ -     global_step = 4600\n",
      "11/11/2019 04:36:13 - INFO - __main__ -     loss = 0.2233\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.2866:  96%|█████████████████████ | 19199/20000 [1:44:33<02:49,  4.74it/s]11/11/2019 04:38:46 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 04:38:46 - INFO - __main__ -     global_step = 4800\n",
      "11/11/2019 04:38:46 - INFO - __main__ -     train loss = 0.2866\n",
      "11/11/2019 04:39:09 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 04:39:09 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 04:39:09 - INFO - __main__ -     Batch size = 32\n",
      "11/11/2019 04:41:28 - INFO - __main__ -     eval_F1 = 0.7937890739417816\n",
      "11/11/2019 04:41:28 - INFO - __main__ -     eval_loss = 0.35473107526321773\n",
      "11/11/2019 04:41:28 - INFO - __main__ -     global_step = 4800\n",
      "11/11/2019 04:41:28 - INFO - __main__ -     loss = 0.2866\n",
      "================================================================================\n",
      "loss 0.2093: 100%|█████████████████████▉| 19999/20000 [1:49:47<00:00,  4.83it/s]11/11/2019 04:44:01 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 04:44:01 - INFO - __main__ -     global_step = 5000\n",
      "11/11/2019 04:44:01 - INFO - __main__ -     train loss = 0.2093\n",
      "11/11/2019 04:44:24 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 04:44:24 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 04:44:24 - INFO - __main__ -     Batch size = 32\n",
      "11/11/2019 04:46:42 - INFO - __main__ -     eval_F1 = 0.7979730585705394\n",
      "11/11/2019 04:46:42 - INFO - __main__ -     eval_loss = 0.35548378762019717\n",
      "11/11/2019 04:46:42 - INFO - __main__ -     global_step = 5000\n",
      "11/11/2019 04:46:42 - INFO - __main__ -     loss = 0.2093\n",
      "================================================================================\n",
      "loss 0.2093: 100%|██████████████████████| 20000/20000 [1:52:29<00:00, 48.66s/it]\n",
      "11/11/2019 04:46:43 - INFO - pytorch_transformers.modeling_utils -   loading weights file ../model/roberta_wwm_large_3002_gru1_42/roberta_wwm_large_3002_gru1_4/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "dev 0.8012369442645789\n",
      "/root/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "test 0.07491359215497147\n"
     ]
    }
   ],
   "source": [
    "# base:  首尾\n",
    "!python ./run_bert_2562.py \\\n",
    "--model_type bert \\\n",
    "--model_name_or_path ../model/chinese_roberta_wwm_large_ext_pytorch  \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_test \\\n",
    "--data_dir ../data/data_StratifiedKFold_42/data_origin_4 \\\n",
    "--output_dir ../model/roberta_wwm_large_3002_gru1_42/roberta_wwm_large_3002_gru1_4 \\\n",
    "--max_seq_length 300 \\\n",
    "--split_num 2 \\\n",
    "--lstm_hidden_size 512 \\\n",
    "--lstm_layers 1 \\\n",
    "--lstm_dropout 0.1 \\\n",
    "--eval_steps 200 \\\n",
    "--per_gpu_train_batch_size 4 \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--warmup_steps 0 \\\n",
    "--per_gpu_eval_batch_size 32 \\\n",
    "--learning_rate 5e-6 \\\n",
    "--adam_epsilon 1e-6 \\\n",
    "--weight_decay 0 \\\n",
    "--train_steps 20000 \\\n",
    "--freeze 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11764858122077997\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df = pd.read_csv('../model/roberta_wwm_large_3002_gru1_42/roberta_wwm_large_3002_gru1_0/sub.csv')\n",
    "df = df[['id']]\n",
    "df['0'] = 0\n",
    "df['1'] = 0\n",
    "df['2'] = 0\n",
    "\n",
    "k=5\n",
    "for i in [0,1,2,3,4]:\n",
    "    temp=pd.read_csv('../model/roberta_wwm_large_3002_gru1_42/roberta_wwm_large_3002_gru1_{}/sub.csv'.format(i))\n",
    "    df['0']+=temp['label_0']/k\n",
    "    df['1']+=temp['label_1']/k\n",
    "    df['2']+=temp['label_2']/k\n",
    "print(df['0'].mean())\n",
    "\n",
    "df['label']=np.argmax(df[['0','1','2']].values,-1)\n",
    "df[['id','label']].to_csv('../model/roberta_wwm_large_3002_gru1_42/roberta_wwm_large_3002_gru1_42_sub.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
