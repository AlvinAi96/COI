{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11/2019 09:01:39 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "11/11/2019 09:01:39 - INFO - pytorch_transformers.tokenization_utils -   Model name '../model/chinese_roberta_wwm_large_ext_pytorch' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming '../model/chinese_roberta_wwm_large_ext_pytorch' is a path or url to a directory containing tokenizer files.\n",
      "11/11/2019 09:01:39 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../model/chinese_roberta_wwm_large_ext_pytorch/added_tokens.json. We won't load it.\n",
      "11/11/2019 09:01:39 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../model/chinese_roberta_wwm_large_ext_pytorch/special_tokens_map.json. We won't load it.\n",
      "11/11/2019 09:01:39 - INFO - pytorch_transformers.tokenization_utils -   loading file ../model/chinese_roberta_wwm_large_ext_pytorch/vocab.txt\n",
      "11/11/2019 09:01:39 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/11/2019 09:01:39 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/11/2019 09:01:39 - INFO - pytorch_transformers.modeling_utils -   loading configuration file ../model/chinese_roberta_wwm_large_ext_pytorch/config.json\n",
      "11/11/2019 09:01:39 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 3,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "11/11/2019 09:01:39 - INFO - pytorch_transformers.modeling_utils -   loading weights file ../model/chinese_roberta_wwm_large_ext_pytorch/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "11/11/2019 09:01:44 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification_last2embedding_cls not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "11/11/2019 09:01:44 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification_last2embedding_cls: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "11/11/2019 09:01:47 - INFO - __main__ -   ** RAW EXAMPLE **\n",
      "11/11/2019 09:01:47 - INFO - __main__ -   content: ['这', '几', '天', '看', '了', '有', '人', '举', '报', '施', '某', '某', '的', '贴', '子', '，', '经', '与', '举', '报', '人', '联', '系', '证', '实', '，', '是', '宣', '某', '当', '天', '中', '午', '请', '举', '报', '人', '和', '枪', '手', '喝', '酒', '后', '，', '晚', '上', '才', '发', '的', '贴', '子', '！', '本', '人', '不', '去', '讨', '论', '前', '二', '天', '的', '举', '报', '，', '相', '信', '总', '归', '会', '有', '说', '法', '的', '！', '今', '天', '一', '看', '施', '全', '军', '2017', '年', '1', '月', '2', '日', '实', '名', '举', '报', '上', '黄', '镇', '宣', '国', '才', '的', '贴', '子', '（', '仍', '被', '锁', '定', '禁', '止', '评', '论', '）', '已', '经', '正', '好', '一', '整', '年', '了', '=', '750', ')', 'window', '.', 'open', '(', \"'\", 'http', ':', '/', '/', 'img', '.', 'js', '##ly', '##001', '.', 'com', '/', 'at', '##ta', '##ch', '##ment', '/', 'mon', '_', '180', '##1', '/', '4', '_', '291', '##08', '##5', '_', 'c', '##79', '##6', '##a', '##6', '##a', '##86', '##e', '##17', '##12', '##1', '.', 'jpg', '?', '123', \"'\", ')', ';', '\"', 'on', '##load', '=', '\"', 'if', '(', 'this', '.', 'off', '##set', '##wi', '##dt', '##h', '>', \"'\", '750', \"'\", ')', 'this', '.', 'wi', '##dt', '##h', '=', \"'\", '750', \"'\", ';', '\"', 'sr', '##c', '=', '\"', 'http', ':', '/', '/', 'img', '.', 'js', '##ly', '##001', '.', 'com', '/', 'at', '##ta', '##ch', '##ment', '/', 'mon', '_', '180', '##1', '/', '4', '_', '291', '##08', '##5', '_', 'c', '##79', '##6', '##a', '##6', '##a', '##86', '##e', '##17', '##12', '##1', '.', 'jpg', '?', '123', '\"', 'style', '=', '\"', 'max', '-', 'wi', '##dt', '##h', ':', '750', '##px', ';', '\"', '/', '>', '图', '片', ':', '/', 'home', '/', 'al', '##ida', '##ta', '/', 'www', '/', 'data', '/', 'tm', '##p', '/', 'q', '##fu', '##pl', '##oa', '##d', '/', '4', '_', '291', '##08', '##5', '_', '151', '##49', '##81', '##47', '##14', '##78', '##95', '##2', '.', 'jpg', '施', '全', '军', '实', '名', '举', '报', '50', '天', '后', '，', '上', '黄', '镇', '党', '委', '政', '府', '回', '复', '如', '下', '图', '：', '=', '750', ')', 'window', '.', 'open', '(', \"'\", 'http', ':', '/', '/', 'img', '.', 'js', '##ly', '##001', '.', 'com', '/', 'at', '##ta', '##ch', '##ment', '/', 'mon', '_', '180', '##1', '/', '4', '_', '291', '##08', '##5', '_', 'a9', '##b', '##11', '##b', '##7', '##ea', '##2', '##b', '##1', '##ce', '##9', '.', 'jpg', '?', '90', \"'\", ')', ';', '\"', 'on', '##load', '=', '\"', 'if', '(', 'this', '.', 'off', '##set', '##wi', '##dt', '##h', '>', \"'\", '750', \"'\", ')', 'this', '.', 'wi', '##dt', '##h', '=', \"'\", '750', \"'\", ';', '\"', 'sr', '##c', '=', '\"', 'http', ':', '/', '/', 'img', '.', 'js', '##ly', '##001', '.', 'com', '/', 'at', '##ta', '##ch', '##ment', '/', 'mon', '_', '180', '##1', '/', '4', '_', '291', '##08', '##5', '_', 'a9', '##b', '##11', '##b', '##7', '##ea', '##2', '##b', '##1', '##ce', '##9', '.', 'jpg', '?', '90', '\"', 'style', '=', '\"', 'max', '-', 'wi', '##dt', '##h', ':', '750', '##px', ';', '\"', '/', '>', '图', '片', ':', '/', 'home', '/', 'al', '##ida', '##ta', '/', 'www', '/', 'data', '/', 'tm', '##p', '/', 'q', '##fu', '##pl', '##oa', '##d', '/', '4', '_', '291', '##08', '##5', '_', '151', '##49', '##81', '##47', '##26', '##31', '##66', '##8', '.', 'jpg', '=', '750', ')', 'window', '.', 'open', '(', \"'\", 'http', ':', '/', '/', 'img', '.', 'js', '##ly', '##001', '.', 'com', '/', 'at', '##ta', '##ch', '##ment', '/', 'mon', '_', '180', '##1', '/', '4', '_', '291', '##08', '##5', '_', '9', '##cd', '##e', '##9', '##b', '##39', '##43', '##fe', '##20', '##c', '.', 'jpg', '?', '75', \"'\", ')', ';', '\"', 'on', '##load', '=', '\"', 'if', '(', 'this', '.', 'off', '##set', '##wi', '##dt', '##h', '>', \"'\", '750', \"'\", ')', 'this', '.', 'wi', '##dt', '##h', '=', \"'\", '750', \"'\", ';', '\"', 'sr', '##c', '=', '\"', 'http', ':', '/', '/', 'img', '.', 'js', '##ly', '##001', '.', 'com', '/', 'at', '##ta', '##ch', '##ment', '/', 'mon', '_', '180', '##1', '/', '4', '_', '291', '##08', '##5', '_', '9', '##cd', '##e', '##9', '##b', '##39', '##43', '##fe', '##20', '##c', '.', 'jpg', '?', '75', '\"', 'style', '=', '\"', 'max', '-', 'wi', '##dt', '##h', ':', '750', '##px', ';', '\"', '/', '>', '图', '片', ':', '/', 'home', '/', 'al', '##ida', '##ta', '/', 'www', '/', 'data', '/', 'tm', '##p', '/', 'q', '##fu', '##pl', '##oa', '##d', '/', '4', '_', '291', '##08', '##5', '_', '151', '##49', '##81', '##47', '##23', '##53', '##07', '##5', '.', 'jpg', '一', '年', '的', '贴', '子', '，', '再', '次', '被', '网', '友', '顶', '起', '来', '后', '，', '才', '发', '现', '施', '某', '几', '天', '前', '回', '复', '网', '友', '的', '处', '理', '结', '果', '竟', '如', '下', '图', '：', '=', '750', ')', 'window', '.', 'open', '(', \"'\", 'http', ':', '/', '/', 'img', '.', 'js', '##ly', '##001', '.', 'com', '/', 'at', '##ta', '##ch', '##ment', '/', 'mon', '_', '180', '##1', '/', '4', '_', '291', '##08', '##5', '_', '9', '##d', '##32', '##ee', '##57', '##27', '##60', '##d', '##85', '.', 'jpg', '?', '131', \"'\", ')', ';', '\"', 'on', '##load', '=', '\"', 'if', '(', 'this', '.', 'off', '##set', '##wi', '##dt', '##h', '>', \"'\", '750', \"'\", ')', 'this', '.', 'wi', '##dt', '##h', '=', \"'\", '750', \"'\", ';', '\"', 'sr', '##c', '=', '\"', 'http', ':', '/', '/', 'img', '.', 'js', '##ly', '##001', '.', 'com', '/', 'at', '##ta', '##ch', '##ment', '/', 'mon', '_', '180', '##1', '/', '4', '_', '291', '##08', '##5', '_', '9', '##d', '##32', '##ee', '##57', '##27', '##60', '##d', '##85', '.', 'jpg', '?', '131', '\"', 'style', '=', '\"', 'max', '-', 'wi', '##dt', '##h', ':', '750', '##px', ';', '\"', '/', '>', '图', '片', ':', '/', 'home', '/', 'al', '##ida', '##ta', '/', 'www', '/', 'data', '/', 'tm', '##p', '/', 'q', '##fu', '##pl', '##oa', '##d', '/', '4', '_', '291', '##08', '##5', '_', '151', '##49', '##81', '##47', '##35', '##47', '##17', '##2', '.', 'jpg', '现', '责', '问', '张', '涛', '书', '记', '：', '1', '、', '宣', '国', '才', '被', '举', '报', '这', '么', '多', '问', '题', '，', '什', '么', '时', '候', '有', '答', '复', '。', '2', '、', '宣', '国', '才', '被', '举', '报', '后', '，', '为', '什', '么', '被', '立', '刻', '免', '了', '村', '书', '记', '职', '务', '？', '为', '什', '么', '又', '被', '安', '排', '到', '城', '管', '队', '[UNK]', '吃', '空', '响', '[UNK]', '，', '自', '己', '却', '天', '天', '在', '我', '们', '水', '泥', '厂', '上', '班', '赚', '黑', '钱', '？', '3', '、', '这', '几', '个', '月', '，', '水', '泥', '每', '吨', '近', '200', '元', '纯', '利', '润', '，', '还', '供', '不', '应', '求', '，', '宣', '国', '才', '还', '清', '上', '黄', '政', '府', '担', '保', '借', '给', '宣', '国', '才', '代', '付', '振', '东', '厂', '工', '资', '社', '保', '的', '钱', '了', '吗', '？', '4', '、', '据', '了', '解', '宣', '国', '才', '占', '他', '人', '企', '业', '经', '营', '，', '又', '欠', '税', '52', '.', '16', '万', '元', '、', '欠', '社', '保', '32', '.', '76', '万', '元', '、', '应', '该', '还', '欠', '了', '职', '工', '工', '资', '几', '十', '万', '，', '上', '黄', '政', '府', '打', '算', '替', '宣', '国', '才', '担', '保', '还', '是', '归', '还', '？', '5', '、', '我', '们', '厂', '合', '法', '会', '计', '和', '老', '板', '被', '判', '刑', '四', '到', '六', '年', '，', '现', '在', '服', '刑', '。', '厂', '子', '给', '宣', '国', '才', '强', '占', '，', '宣', '国', '才', '每', '天', '赚', '20', '多', '万', '净', '利', '润', '，', '却', '对', '外', '宣', '称', '天', '天', '亏', '本', '！', '等', '咱', '老', '板', '刑', '满', '回', '厂', '，', '宣', '国', '才', '给', '咱', '厂', '[UNK]', '天', '天', '亏', '[UNK]', '可', '能', '要', '[UNK]', '亏', '[UNK]', '的', '几', '千', '万', '元', '，', '甚', '至', '几', '个', '亿', '，', '张', '涛', '书', '记', '您', '承', '担', '还', '是', '上', '黄', '政', '府', '承', '担', '？', '当', '初', '可', '是', '您', '亲', '自', '把', '厂', '交', '给', '宣', '国', '才', '生', '产', '的', '！', '希', '望', '徐', '市', '长', '看', '到', '本', '贴', '后', '能', '像', '批', '示', '263', '、', '批', '示', '违', '建', '等', '民', '生', '问', '题', '一', '样', '，', '关', '注', '一', '下', '我', '们', '水', '泥', '厂', '的', '将', '来', '！', '也', '请', '徐', '市', '长', '抽', '日', '理', '万', '机', '之', '空', '亲', '自', '约', '谈', '一', '下', '当', '事', '人', '（', '特', '别', '是', '那', '位', '施', '站', '长', '）', '，', '千', '万', '不', '能', '听', '取', '一', '面', '之', '辞', '！']\n",
      "11/11/2019 09:01:47 - INFO - __main__ -   *** Example ***\n",
      "11/11/2019 09:01:47 - INFO - __main__ -   idx: 0\n",
      "11/11/2019 09:01:47 - INFO - __main__ -   guid: 7a3dd79f90ee419da87190cff60f7a86\n",
      "11/11/2019 09:01:47 - INFO - __main__ -   tokens: [CLS] 问 责 领 导 ( 上 黄 镇 党 委 书 记 张 涛 ， 宣 国 才 真 能 一 手 遮 天 吗 ？ ) [SEP] ##57 ##27 ##60 ##d ##85 . jpg ? 131 \" style = \" max - wi ##dt ##h : 750 ##px ; \" / > 图 片 : / home / al ##ida ##ta / www / data / tm ##p / q ##fu ##pl ##oa ##d / 4 _ 291 ##08 ##5 _ 151 ##49 ##81 ##47 ##35 ##47 ##17 ##2 . jpg 现 责 问 张 涛 书 记 ： 1 、 宣 国 才 被 举 报 这 么 多 问 题 ， 什 么 时 候 有 答 复 。 2 、 宣 国 才 被 举 报 后 ， 为 什 么 被 立 刻 免 了 村 书 记 职 务 ？ 为 什 么 又 被 安 排 到 城 管 队 [UNK] 吃 空 响 [UNK] ， 自 己 却 天 天 在 我 们 水 泥 厂 上 班 赚 黑 钱 ？ 3 、 这 几 个 月 ， 水 泥 每 吨 近 200 元 纯 利 润 ， 还 供 不 应 求 ， 宣 国 才 还 清 上 黄 政 府 担 保 借 给 宣 国 才 代 付 振 东 厂 工 资 社 保 的 钱 了 吗 ？ 4 、 据 了 解 宣 国 才 占 他 人 企 业 经 营 ， 又 欠 税 52 . 16 万 元 、 欠 社 保 32 . 76 万 元 、 应 该 还 欠 了 职 工 工 资 几 十 万 ， 上 黄 政 府 打 算 替 宣 国 才 担 保 还 是 归 还 ？ 5 、 我 们 厂 合 法 会 计 和 老 板 被 判 刑 四 到 六 年 ， 现 在 服 刑 。 厂 子 给 宣 国 才 强 占 ， 宣 国 才 每 天 赚 20 多 万 净 利 润 ， 却 对 外 宣 称 天 天 亏 本 ！ 等 咱 老 板 刑 满 回 厂 ， 宣 国 才 给 咱 厂 [UNK] 天 天 亏 [UNK] 可 能 要 [UNK] 亏 [UNK] 的 几 千 万 元 ， 甚 至 几 个 亿 ， 张 涛 书 记 您 承 担 还 是 上 黄 政 府 承 担 ？ 当 初 可 是 您 亲 自 把 厂 交 给 宣 国 才 生 产 的 ！ 希 望 徐 市 长 看 到 本 贴 后 能 像 批 示 263 、 批 示 违 建 等 民 生 问 题 一 样 ， 关 注 一 下 我 们 水 泥 厂 的 将 来 ！ 也 请 徐 市 长 抽 日 理 万 机 之 空 亲 自 约 谈 一 下 当 事 人 （ 特 别 是 那 位 施 站 长 ） ， 千 万 不 能 听 取 一 面 之 辞 [SEP]\n",
      "11/11/2019 09:01:47 - INFO - __main__ -   input_ids: 101 7309 6569 7566 2193 113 677 7942 7252 1054 1999 741 6381 2476 3875 8024 2146 1744 2798 4696 5543 671 2797 6902 1921 1408 8043 114 102 9647 8976 8581 8168 9169 119 9248 136 9403 107 8969 134 107 8621 118 8541 12672 8199 131 9180 10605 132 107 120 135 1745 4275 131 120 8563 120 9266 12708 8383 120 8173 120 9000 120 9908 8187 120 159 12043 12569 11355 8168 120 125 142 11777 9153 8157 142 9564 9500 9313 9050 8852 9050 8408 8144 119 9248 4385 6569 7309 2476 3875 741 6381 8038 122 510 2146 1744 2798 6158 715 2845 6821 720 1914 7309 7579 8024 784 720 3198 952 3300 5031 1908 511 123 510 2146 1744 2798 6158 715 2845 1400 8024 711 784 720 6158 4989 1174 1048 749 3333 741 6381 5466 1218 8043 711 784 720 1348 6158 2128 2961 1168 1814 5052 7339 100 1391 4958 1510 100 8024 5632 2346 1316 1921 1921 1762 2769 812 3717 3799 1322 677 4408 6611 7946 7178 8043 124 510 6821 1126 702 3299 8024 3717 3799 3680 1417 6818 8185 1039 5283 1164 3883 8024 6820 897 679 2418 3724 8024 2146 1744 2798 6820 3926 677 7942 3124 2424 2857 924 955 5314 2146 1744 2798 807 802 2920 691 1322 2339 6598 4852 924 4638 7178 749 1408 8043 125 510 2945 749 6237 2146 1744 2798 1304 800 782 821 689 5307 5852 8024 1348 3612 4925 8247 119 8121 674 1039 510 3612 4852 924 8211 119 8399 674 1039 510 2418 6421 6820 3612 749 5466 2339 2339 6598 1126 1282 674 8024 677 7942 3124 2424 2802 5050 3296 2146 1744 2798 2857 924 6820 3221 2495 6820 8043 126 510 2769 812 1322 1394 3791 833 6369 1469 5439 3352 6158 1161 1152 1724 1168 1063 2399 8024 4385 1762 3302 1152 511 1322 2094 5314 2146 1744 2798 2487 1304 8024 2146 1744 2798 3680 1921 6611 8113 1914 674 1112 1164 3883 8024 1316 2190 1912 2146 4917 1921 1921 755 3315 8013 5023 1493 5439 3352 1152 4007 1726 1322 8024 2146 1744 2798 5314 1493 1322 100 1921 1921 755 100 1377 5543 6206 100 755 100 4638 1126 1283 674 1039 8024 4493 5635 1126 702 783 8024 2476 3875 741 6381 2644 2824 2857 6820 3221 677 7942 3124 2424 2824 2857 8043 2496 1159 1377 3221 2644 779 5632 2828 1322 769 5314 2146 1744 2798 4495 772 4638 8013 2361 3307 2528 2356 7270 4692 1168 3315 6585 1400 5543 1008 2821 4850 10864 510 2821 4850 6824 2456 5023 3696 4495 7309 7579 671 3416 8024 1068 3800 671 678 2769 812 3717 3799 1322 4638 2199 3341 8013 738 6435 2528 2356 7270 2853 3189 4415 674 3322 722 4958 779 5632 5276 6448 671 678 2496 752 782 8020 4294 1166 3221 6929 855 3177 4991 7270 8021 8024 1283 674 679 5543 1420 1357 671 7481 722 6791 102\n",
      "11/11/2019 09:01:47 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/11/2019 09:01:47 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/11/2019 09:01:47 - INFO - __main__ -   label: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11/2019 09:02:37 - INFO - __main__ -   ***** Running training *****\n",
      "11/11/2019 09:02:37 - INFO - __main__ -     Num examples = 11755\n",
      "11/11/2019 09:02:37 - INFO - __main__ -     Batch size = 4\n",
      "11/11/2019 09:02:37 - INFO - __main__ -     Num steps = 30000\n",
      "  0%|                                                 | 0/30000 [00:00<?, ?it/s]11/11/2019 09:02:50 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 09:02:50 - INFO - __main__ -     Num examples = 2941\n",
      "11/11/2019 09:02:50 - INFO - __main__ -     Batch size = 48\n",
      "/home/haizhi/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "11/11/2019 09:05:40 - INFO - __main__ -     eval_F1 = 0.20464637224558666\n",
      "11/11/2019 09:05:40 - INFO - __main__ -     eval_loss = 1.3352864874947457\n",
      "11/11/2019 09:05:40 - INFO - __main__ -     global_step = 0\n",
      "================================================================================\n",
      "Best F1 0.20464637224558666\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.655:   3%|▋                        | 799/30000 [06:20<2:06:18,  3.85it/s]11/11/2019 09:08:58 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 09:08:58 - INFO - __main__ -     global_step = 200\n",
      "11/11/2019 09:08:58 - INFO - __main__ -     train loss = 0.655\n",
      "loss 0.4804:   5%|█▏                     | 1599/30000 [09:31<2:03:10,  3.84it/s]11/11/2019 09:12:09 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 09:12:09 - INFO - __main__ -     global_step = 400\n",
      "11/11/2019 09:12:09 - INFO - __main__ -     train loss = 0.4804\n",
      "loss 0.4971:   8%|█▊                     | 2399/30000 [12:44<2:03:46,  3.72it/s]11/11/2019 09:15:22 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 09:15:22 - INFO - __main__ -     global_step = 600\n",
      "11/11/2019 09:15:22 - INFO - __main__ -     train loss = 0.4971\n",
      "loss 0.4464:  11%|██▍                    | 3199/30000 [15:57<1:56:53,  3.82it/s]11/11/2019 09:18:35 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 09:18:35 - INFO - __main__ -     global_step = 800\n",
      "11/11/2019 09:18:35 - INFO - __main__ -     train loss = 0.4464\n",
      "loss 0.4015:  13%|███                    | 3999/30000 [19:11<1:54:40,  3.78it/s]11/11/2019 09:21:49 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 09:21:49 - INFO - __main__ -     global_step = 1000\n",
      "11/11/2019 09:21:49 - INFO - __main__ -     train loss = 0.4015\n",
      "loss 0.3856:  16%|███▋                   | 4799/30000 [22:25<1:56:29,  3.61it/s]11/11/2019 09:25:03 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 09:25:03 - INFO - __main__ -     global_step = 1200\n",
      "11/11/2019 09:25:03 - INFO - __main__ -     train loss = 0.3856\n",
      "loss 0.4121:  19%|████▎                  | 5599/30000 [25:39<1:46:20,  3.82it/s]11/11/2019 09:28:17 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 09:28:17 - INFO - __main__ -     global_step = 1400\n",
      "11/11/2019 09:28:17 - INFO - __main__ -     train loss = 0.4121\n",
      "loss 0.3742:  21%|████▉                  | 6399/30000 [28:54<1:42:47,  3.83it/s]11/11/2019 09:31:32 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 09:31:32 - INFO - __main__ -     global_step = 1600\n",
      "11/11/2019 09:31:32 - INFO - __main__ -     train loss = 0.3742\n",
      "loss 0.4143:  24%|█████▌                 | 7199/30000 [32:08<1:39:39,  3.81it/s]11/11/2019 09:34:45 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 09:34:45 - INFO - __main__ -     global_step = 1800\n",
      "11/11/2019 09:34:45 - INFO - __main__ -     train loss = 0.4143\n",
      "loss 0.3842:  27%|██████▏                | 7999/30000 [35:22<1:35:46,  3.83it/s]11/11/2019 09:38:00 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 09:38:00 - INFO - __main__ -     global_step = 2000\n",
      "11/11/2019 09:38:00 - INFO - __main__ -     train loss = 0.3842\n",
      "loss 0.3802:  29%|██████▋                | 8799/30000 [38:36<1:34:11,  3.75it/s]11/11/2019 09:41:14 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 09:41:14 - INFO - __main__ -     global_step = 2200\n",
      "11/11/2019 09:41:14 - INFO - __main__ -     train loss = 0.3802\n",
      "loss 0.4067:  32%|███████▎               | 9599/30000 [41:52<1:35:03,  3.58it/s]11/11/2019 09:44:30 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 09:44:30 - INFO - __main__ -     global_step = 2400\n",
      "11/11/2019 09:44:30 - INFO - __main__ -     train loss = 0.4067\n",
      "loss 0.3608:  35%|███████▋              | 10399/30000 [45:08<1:27:00,  3.75it/s]11/11/2019 09:47:46 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 09:47:46 - INFO - __main__ -     global_step = 2600\n",
      "11/11/2019 09:47:46 - INFO - __main__ -     train loss = 0.3608\n",
      "loss 0.4074:  37%|████████▏             | 11199/30000 [48:24<1:23:18,  3.76it/s]11/11/2019 09:51:02 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 09:51:02 - INFO - __main__ -     global_step = 2800\n",
      "11/11/2019 09:51:02 - INFO - __main__ -     train loss = 0.4074\n",
      "11/11/2019 09:51:15 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 09:51:15 - INFO - __main__ -     Num examples = 2941\n",
      "11/11/2019 09:51:15 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 09:54:14 - INFO - __main__ -     eval_F1 = 0.7873674334006336\n",
      "11/11/2019 09:54:14 - INFO - __main__ -     eval_loss = 0.3958863839507103\n",
      "11/11/2019 09:54:14 - INFO - __main__ -     global_step = 2800\n",
      "11/11/2019 09:54:14 - INFO - __main__ -     loss = 0.4074\n",
      "================================================================================\n",
      "Best F1 0.7873674334006336\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.3608:  40%|████████▊             | 11999/30000 [55:04<1:19:59,  3.75it/s]11/11/2019 09:57:42 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 09:57:42 - INFO - __main__ -     global_step = 3000\n",
      "11/11/2019 09:57:42 - INFO - __main__ -     train loss = 0.3608\n",
      "11/11/2019 09:57:54 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 09:57:54 - INFO - __main__ -     Num examples = 2941\n",
      "11/11/2019 09:57:54 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 10:00:54 - INFO - __main__ -     eval_F1 = 0.7974728933243737\n",
      "11/11/2019 10:00:54 - INFO - __main__ -     eval_loss = 0.3682414701869411\n",
      "11/11/2019 10:00:54 - INFO - __main__ -     global_step = 3000\n",
      "11/11/2019 10:00:54 - INFO - __main__ -     loss = 0.3608\n",
      "================================================================================\n",
      "Best F1 0.7974728933243737\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.2921:  43%|████████▌           | 12799/30000 [1:01:42<1:15:20,  3.81it/s]11/11/2019 10:04:20 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 10:04:20 - INFO - __main__ -     global_step = 3200\n",
      "11/11/2019 10:04:20 - INFO - __main__ -     train loss = 0.2921\n",
      "11/11/2019 10:04:33 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 10:04:33 - INFO - __main__ -     Num examples = 2941\n",
      "11/11/2019 10:04:33 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 10:07:32 - INFO - __main__ -     eval_F1 = 0.7870223708951204\n",
      "11/11/2019 10:07:32 - INFO - __main__ -     eval_loss = 0.37213934248974245\n",
      "11/11/2019 10:07:32 - INFO - __main__ -     global_step = 3200\n",
      "11/11/2019 10:07:32 - INFO - __main__ -     loss = 0.2921\n",
      "================================================================================\n",
      "loss 0.2509:  45%|█████████           | 13599/30000 [1:08:11<1:12:35,  3.77it/s]11/11/2019 10:10:48 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 10:10:48 - INFO - __main__ -     global_step = 3400\n",
      "11/11/2019 10:10:48 - INFO - __main__ -     train loss = 0.2509\n",
      "11/11/2019 10:11:01 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 10:11:01 - INFO - __main__ -     Num examples = 2941\n",
      "11/11/2019 10:11:01 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 10:14:00 - INFO - __main__ -     eval_F1 = 0.7937190440507415\n",
      "11/11/2019 10:14:00 - INFO - __main__ -     eval_loss = 0.38614352083494585\n",
      "11/11/2019 10:14:00 - INFO - __main__ -     global_step = 3400\n",
      "11/11/2019 10:14:00 - INFO - __main__ -     loss = 0.2509\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.2432:  48%|█████████▌          | 14399/30000 [1:14:38<1:10:20,  3.70it/s]11/11/2019 10:17:16 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 10:17:16 - INFO - __main__ -     global_step = 3600\n",
      "11/11/2019 10:17:16 - INFO - __main__ -     train loss = 0.2432\n",
      "11/11/2019 10:17:29 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 10:17:29 - INFO - __main__ -     Num examples = 2941\n",
      "11/11/2019 10:17:29 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 10:20:27 - INFO - __main__ -     eval_F1 = 0.7939416102364344\n",
      "11/11/2019 10:20:27 - INFO - __main__ -     eval_loss = 0.3612737726540335\n",
      "11/11/2019 10:20:27 - INFO - __main__ -     global_step = 3600\n",
      "11/11/2019 10:20:27 - INFO - __main__ -     loss = 0.2432\n",
      "================================================================================\n",
      "loss 0.216:  51%|██████████▋          | 15199/30000 [1:21:06<1:06:00,  3.74it/s]11/11/2019 10:23:44 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 10:23:44 - INFO - __main__ -     global_step = 3800\n",
      "11/11/2019 10:23:44 - INFO - __main__ -     train loss = 0.216\n",
      "11/11/2019 10:23:57 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 10:23:57 - INFO - __main__ -     Num examples = 2941\n",
      "11/11/2019 10:23:57 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 10:26:55 - INFO - __main__ -     eval_F1 = 0.7998589255753844\n",
      "11/11/2019 10:26:55 - INFO - __main__ -     eval_loss = 0.37874959247006523\n",
      "11/11/2019 10:26:55 - INFO - __main__ -     global_step = 3800\n",
      "11/11/2019 10:26:55 - INFO - __main__ -     loss = 0.216\n",
      "================================================================================\n",
      "Best F1 0.7998589255753844\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.1993:  53%|██████████▋         | 15999/30000 [1:27:44<1:05:39,  3.55it/s]11/11/2019 10:30:22 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 10:30:22 - INFO - __main__ -     global_step = 4000\n",
      "11/11/2019 10:30:22 - INFO - __main__ -     train loss = 0.1993\n",
      "11/11/2019 10:30:35 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 10:30:35 - INFO - __main__ -     Num examples = 2941\n",
      "11/11/2019 10:30:35 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 10:33:34 - INFO - __main__ -     eval_F1 = 0.7956049065422987\n",
      "11/11/2019 10:33:34 - INFO - __main__ -     eval_loss = 0.3937386514078225\n",
      "11/11/2019 10:33:34 - INFO - __main__ -     global_step = 4000\n",
      "11/11/2019 10:33:34 - INFO - __main__ -     loss = 0.1993\n",
      "================================================================================\n",
      "loss 0.1851:  56%|████████████▎         | 16799/30000 [1:34:14<57:33,  3.82it/s]11/11/2019 10:36:52 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 10:36:52 - INFO - __main__ -     global_step = 4200\n",
      "11/11/2019 10:36:52 - INFO - __main__ -     train loss = 0.1851\n",
      "11/11/2019 10:37:05 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 10:37:05 - INFO - __main__ -     Num examples = 2941\n",
      "11/11/2019 10:37:05 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 10:40:04 - INFO - __main__ -     eval_F1 = 0.7995173895418693\n",
      "11/11/2019 10:40:04 - INFO - __main__ -     eval_loss = 0.4234954291624167\n",
      "11/11/2019 10:40:04 - INFO - __main__ -     global_step = 4200\n",
      "11/11/2019 10:40:04 - INFO - __main__ -     loss = 0.1851\n",
      "================================================================================\n",
      "loss 0.2184:  59%|████████████▉         | 17599/30000 [1:40:45<54:23,  3.80it/s]11/11/2019 10:43:22 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 10:43:22 - INFO - __main__ -     global_step = 4400\n",
      "11/11/2019 10:43:22 - INFO - __main__ -     train loss = 0.2184\n",
      "11/11/2019 10:43:35 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 10:43:35 - INFO - __main__ -     Num examples = 2941\n",
      "11/11/2019 10:43:35 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 10:46:35 - INFO - __main__ -     eval_F1 = 0.7915208935399104\n",
      "11/11/2019 10:46:35 - INFO - __main__ -     eval_loss = 0.45162617643513986\n",
      "11/11/2019 10:46:35 - INFO - __main__ -     global_step = 4400\n",
      "11/11/2019 10:46:35 - INFO - __main__ -     loss = 0.2184\n",
      "================================================================================\n",
      "loss 0.2167:  61%|█████████████▍        | 18399/30000 [1:47:15<51:07,  3.78it/s]11/11/2019 10:49:53 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 10:49:53 - INFO - __main__ -     global_step = 4600\n",
      "11/11/2019 10:49:53 - INFO - __main__ -     train loss = 0.2167\n",
      "11/11/2019 10:50:05 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 10:50:05 - INFO - __main__ -     Num examples = 2941\n",
      "11/11/2019 10:50:05 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 10:53:05 - INFO - __main__ -     eval_F1 = 0.7922836168616119\n",
      "11/11/2019 10:53:05 - INFO - __main__ -     eval_loss = 0.43173080111943907\n",
      "11/11/2019 10:53:05 - INFO - __main__ -     global_step = 4600\n",
      "11/11/2019 10:53:05 - INFO - __main__ -     loss = 0.2167\n",
      "================================================================================\n",
      "loss 0.2414:  64%|██████████████        | 19199/30000 [1:53:46<50:45,  3.55it/s]11/11/2019 10:56:24 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 10:56:24 - INFO - __main__ -     global_step = 4800\n",
      "11/11/2019 10:56:24 - INFO - __main__ -     train loss = 0.2414\n",
      "11/11/2019 10:56:37 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 10:56:37 - INFO - __main__ -     Num examples = 2941\n",
      "11/11/2019 10:56:37 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 10:59:36 - INFO - __main__ -     eval_F1 = 0.8013827667985759\n",
      "11/11/2019 10:59:36 - INFO - __main__ -     eval_loss = 0.3698109023633503\n",
      "11/11/2019 10:59:36 - INFO - __main__ -     global_step = 4800\n",
      "11/11/2019 10:59:36 - INFO - __main__ -     loss = 0.2414\n",
      "================================================================================\n",
      "Best F1 0.8013827667985759\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.1639:  67%|██████████████▋       | 19999/30000 [2:00:25<44:53,  3.71it/s]11/11/2019 11:03:03 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 11:03:03 - INFO - __main__ -     global_step = 5000\n",
      "11/11/2019 11:03:03 - INFO - __main__ -     train loss = 0.1639\n",
      "11/11/2019 11:03:16 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 11:03:16 - INFO - __main__ -     Num examples = 2941\n",
      "11/11/2019 11:03:16 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 11:06:14 - INFO - __main__ -     eval_F1 = 0.800658810203267\n",
      "11/11/2019 11:06:14 - INFO - __main__ -     eval_loss = 0.39916646504594433\n",
      "11/11/2019 11:06:14 - INFO - __main__ -     global_step = 5000\n",
      "11/11/2019 11:06:14 - INFO - __main__ -     loss = 0.1639\n",
      "================================================================================\n",
      "loss 0.2066:  69%|███████████████▎      | 20799/30000 [2:06:54<43:11,  3.55it/s]11/11/2019 11:09:32 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 11:09:32 - INFO - __main__ -     global_step = 5200\n",
      "11/11/2019 11:09:32 - INFO - __main__ -     train loss = 0.2066\n",
      "11/11/2019 11:09:45 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 11:09:45 - INFO - __main__ -     Num examples = 2941\n",
      "11/11/2019 11:09:45 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 11:12:43 - INFO - __main__ -     eval_F1 = 0.8062648750402279\n",
      "11/11/2019 11:12:43 - INFO - __main__ -     eval_loss = 0.38421060313140193\n",
      "11/11/2019 11:12:43 - INFO - __main__ -     global_step = 5200\n",
      "11/11/2019 11:12:43 - INFO - __main__ -     loss = 0.2066\n",
      "================================================================================\n",
      "Best F1 0.8062648750402279\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.1934:  72%|███████████████▊      | 21599/30000 [2:13:32<37:26,  3.74it/s]11/11/2019 11:16:10 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 11:16:10 - INFO - __main__ -     global_step = 5400\n",
      "11/11/2019 11:16:10 - INFO - __main__ -     train loss = 0.1934\n",
      "11/11/2019 11:16:23 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 11:16:23 - INFO - __main__ -     Num examples = 2941\n",
      "11/11/2019 11:16:23 - INFO - __main__ -     Batch size = 48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11/2019 11:19:21 - INFO - __main__ -     eval_F1 = 0.7953400235009758\n",
      "11/11/2019 11:19:21 - INFO - __main__ -     eval_loss = 0.4539041118395905\n",
      "11/11/2019 11:19:21 - INFO - __main__ -     global_step = 5400\n",
      "11/11/2019 11:19:21 - INFO - __main__ -     loss = 0.1934\n",
      "================================================================================\n",
      "loss 0.1762:  75%|████████████████▍     | 22399/30000 [2:20:00<35:06,  3.61it/s]11/11/2019 11:22:38 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 11:22:38 - INFO - __main__ -     global_step = 5600\n",
      "11/11/2019 11:22:38 - INFO - __main__ -     train loss = 0.1762\n",
      "11/11/2019 11:22:51 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 11:22:51 - INFO - __main__ -     Num examples = 2941\n",
      "11/11/2019 11:22:51 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 11:25:50 - INFO - __main__ -     eval_F1 = 0.7957921573555963\n",
      "11/11/2019 11:25:50 - INFO - __main__ -     eval_loss = 0.42978005724087837\n",
      "11/11/2019 11:25:50 - INFO - __main__ -     global_step = 5600\n",
      "11/11/2019 11:25:50 - INFO - __main__ -     loss = 0.1762\n",
      "================================================================================\n",
      "loss 0.247:  77%|█████████████████▊     | 23199/30000 [2:26:30<29:42,  3.82it/s]11/11/2019 11:29:08 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 11:29:08 - INFO - __main__ -     global_step = 5800\n",
      "11/11/2019 11:29:08 - INFO - __main__ -     train loss = 0.247\n",
      "11/11/2019 11:29:21 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 11:29:21 - INFO - __main__ -     Num examples = 2941\n",
      "11/11/2019 11:29:21 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 11:32:20 - INFO - __main__ -     eval_F1 = 0.8136150350201894\n",
      "11/11/2019 11:32:20 - INFO - __main__ -     eval_loss = 0.3770461978931581\n",
      "11/11/2019 11:32:20 - INFO - __main__ -     global_step = 5800\n",
      "11/11/2019 11:32:20 - INFO - __main__ -     loss = 0.247\n",
      "================================================================================\n",
      "Best F1 0.8136150350201894\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.187:  80%|██████████████████▍    | 23999/30000 [2:33:10<26:39,  3.75it/s]11/11/2019 11:35:47 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 11:35:47 - INFO - __main__ -     global_step = 6000\n",
      "11/11/2019 11:35:47 - INFO - __main__ -     train loss = 0.187\n",
      "11/11/2019 11:36:00 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 11:36:00 - INFO - __main__ -     Num examples = 2941\n",
      "11/11/2019 11:36:00 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 11:39:00 - INFO - __main__ -     eval_F1 = 0.8079110245432247\n",
      "11/11/2019 11:39:00 - INFO - __main__ -     eval_loss = 0.37324472672996983\n",
      "11/11/2019 11:39:00 - INFO - __main__ -     global_step = 6000\n",
      "11/11/2019 11:39:00 - INFO - __main__ -     loss = 0.187\n",
      "================================================================================\n",
      "loss 0.1454:  83%|██████████████████▏   | 24799/30000 [2:39:41<24:27,  3.54it/s]11/11/2019 11:42:19 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 11:42:19 - INFO - __main__ -     global_step = 6200\n",
      "11/11/2019 11:42:19 - INFO - __main__ -     train loss = 0.1454\n",
      "11/11/2019 11:42:31 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 11:42:31 - INFO - __main__ -     Num examples = 2941\n",
      "11/11/2019 11:42:31 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 11:45:31 - INFO - __main__ -     eval_F1 = 0.7989760206156614\n",
      "11/11/2019 11:45:31 - INFO - __main__ -     eval_loss = 0.3823009261921529\n",
      "11/11/2019 11:45:31 - INFO - __main__ -     global_step = 6200\n",
      "11/11/2019 11:45:31 - INFO - __main__ -     loss = 0.1454\n",
      "================================================================================\n",
      "loss 0.1236:  85%|██████████████████▊   | 25599/30000 [2:46:12<19:16,  3.81it/s]11/11/2019 11:48:49 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 11:48:49 - INFO - __main__ -     global_step = 6400\n",
      "11/11/2019 11:48:49 - INFO - __main__ -     train loss = 0.1236\n",
      "11/11/2019 11:49:03 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 11:49:03 - INFO - __main__ -     Num examples = 2941\n",
      "11/11/2019 11:49:03 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 11:52:02 - INFO - __main__ -     eval_F1 = 0.8148304076709233\n",
      "11/11/2019 11:52:02 - INFO - __main__ -     eval_loss = 0.4175428672423286\n",
      "11/11/2019 11:52:02 - INFO - __main__ -     global_step = 6400\n",
      "11/11/2019 11:52:02 - INFO - __main__ -     loss = 0.1236\n",
      "================================================================================\n",
      "Best F1 0.8148304076709233\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.1167:  88%|███████████████████▎  | 26399/30000 [2:52:53<16:53,  3.55it/s]11/11/2019 11:55:30 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 11:55:30 - INFO - __main__ -     global_step = 6600\n",
      "11/11/2019 11:55:30 - INFO - __main__ -     train loss = 0.1167\n",
      "11/11/2019 11:55:43 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 11:55:43 - INFO - __main__ -     Num examples = 2941\n",
      "11/11/2019 11:55:43 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 11:58:43 - INFO - __main__ -     eval_F1 = 0.8133722259957707\n",
      "11/11/2019 11:58:43 - INFO - __main__ -     eval_loss = 0.42755666615501525\n",
      "11/11/2019 11:58:43 - INFO - __main__ -     global_step = 6600\n",
      "11/11/2019 11:58:43 - INFO - __main__ -     loss = 0.1167\n",
      "================================================================================\n",
      "loss 0.085:  91%|████████████████████▊  | 27199/30000 [2:59:24<13:10,  3.54it/s]11/11/2019 12:02:02 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 12:02:02 - INFO - __main__ -     global_step = 6800\n",
      "11/11/2019 12:02:02 - INFO - __main__ -     train loss = 0.085\n",
      "11/11/2019 12:02:15 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 12:02:15 - INFO - __main__ -     Num examples = 2941\n",
      "11/11/2019 12:02:15 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 12:05:14 - INFO - __main__ -     eval_F1 = 0.8083280169912053\n",
      "11/11/2019 12:05:14 - INFO - __main__ -     eval_loss = 0.4292663698715548\n",
      "11/11/2019 12:05:14 - INFO - __main__ -     global_step = 6800\n",
      "11/11/2019 12:05:14 - INFO - __main__ -     loss = 0.085\n",
      "================================================================================\n",
      "loss 0.0878:  93%|████████████████████▌ | 27999/30000 [3:05:54<08:46,  3.80it/s]11/11/2019 12:08:32 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 12:08:32 - INFO - __main__ -     global_step = 7000\n",
      "11/11/2019 12:08:32 - INFO - __main__ -     train loss = 0.0878\n",
      "11/11/2019 12:08:45 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 12:08:45 - INFO - __main__ -     Num examples = 2941\n",
      "11/11/2019 12:08:45 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 12:11:44 - INFO - __main__ -     eval_F1 = 0.8116288479757997\n",
      "11/11/2019 12:11:44 - INFO - __main__ -     eval_loss = 0.43545804413095596\n",
      "11/11/2019 12:11:44 - INFO - __main__ -     global_step = 7000\n",
      "11/11/2019 12:11:44 - INFO - __main__ -     loss = 0.0878\n",
      "================================================================================\n",
      "loss 0.0879:  96%|█████████████████████ | 28799/30000 [3:12:24<05:26,  3.68it/s]11/11/2019 12:15:02 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 12:15:02 - INFO - __main__ -     global_step = 7200\n",
      "11/11/2019 12:15:02 - INFO - __main__ -     train loss = 0.0879\n",
      "11/11/2019 12:15:14 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 12:15:14 - INFO - __main__ -     Num examples = 2941\n",
      "11/11/2019 12:15:14 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 12:18:13 - INFO - __main__ -     eval_F1 = 0.8065872694560333\n",
      "11/11/2019 12:18:13 - INFO - __main__ -     eval_loss = 0.4447492431368559\n",
      "11/11/2019 12:18:13 - INFO - __main__ -     global_step = 7200\n",
      "11/11/2019 12:18:13 - INFO - __main__ -     loss = 0.0879\n",
      "================================================================================\n",
      "loss 0.0946:  99%|█████████████████████▋| 29599/30000 [3:18:52<01:45,  3.79it/s]11/11/2019 12:21:30 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 12:21:30 - INFO - __main__ -     global_step = 7400\n",
      "11/11/2019 12:21:30 - INFO - __main__ -     train loss = 0.0946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11/2019 12:21:43 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 12:21:43 - INFO - __main__ -     Num examples = 2941\n",
      "11/11/2019 12:21:43 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 12:24:41 - INFO - __main__ -     eval_F1 = 0.8107799858106679\n",
      "11/11/2019 12:24:41 - INFO - __main__ -     eval_loss = 0.4411367998127976\n",
      "11/11/2019 12:24:41 - INFO - __main__ -     global_step = 7400\n",
      "11/11/2019 12:24:41 - INFO - __main__ -     loss = 0.0946\n",
      "================================================================================\n",
      "loss 0.1039: 100%|██████████████████████| 30000/30000 [3:23:42<00:00,  2.45it/s]\n",
      "11/11/2019 12:26:20 - INFO - pytorch_transformers.modeling_utils -   loading weights file ../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_0/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "Traceback (most recent call last):\n",
      "  File \"./run_bert_2562_last2embedding_cls.py\", line 841, in <module>\n",
      "    main()\n",
      "  File \"./run_bert_2562_last2embedding_cls.py\", line 757, in main\n",
      "    logits = model(input_ids=input_ids, token_type_ids=segment_ids, attention_mask=input_mask).detach().cpu().numpy()\n",
      "  File \"/home/haizhi/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/haizhi/workspace/TODO/2019互联网新闻/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 1119, in forward\n",
      "    attention_mask=flat_attention_mask, head_mask=head_mask)\n",
      "  File \"/home/haizhi/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/haizhi/workspace/TODO/2019互联网新闻/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 727, in forward\n",
      "    head_mask=head_mask)\n",
      "  File \"/home/haizhi/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/haizhi/workspace/TODO/2019互联网新闻/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 440, in forward\n",
      "    layer_outputs = layer_module(hidden_states, attention_mask, head_mask[i])\n",
      "  File \"/home/haizhi/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/haizhi/workspace/TODO/2019互联网新闻/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 420, in forward\n",
      "    intermediate_output = self.intermediate(attention_output)\n",
      "  File \"/home/haizhi/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/haizhi/workspace/TODO/2019互联网新闻/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 392, in forward\n",
      "    hidden_states = self.intermediate_act_fn(hidden_states)\n",
      "  File \"/home/haizhi/workspace/TODO/2019互联网新闻/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 142, in gelu\n",
      "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 384.00 MiB (GPU 0; 10.92 GiB total capacity; 9.35 GiB already allocated; 191.00 MiB free; 810.30 MiB cached)\n"
     ]
    }
   ],
   "source": [
    "!python ./run_bert_2562_last2embedding_cls.py \\\n",
    "--model_type bert \\\n",
    "--model_name_or_path ../model/chinese_roberta_wwm_large_ext_pytorch  \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_test \\\n",
    "--data_dir ../data/data_StratifiedKFold_42/data_origin_0 \\\n",
    "--output_dir ../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_0 \\\n",
    "--max_seq_length 512 \\\n",
    "--split_num 1 \\\n",
    "--lstm_hidden_size 512 \\\n",
    "--lstm_layers 1 \\\n",
    "--lstm_dropout 0.1 \\\n",
    "--eval_steps 200 \\\n",
    "--per_gpu_train_batch_size 4 \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--warmup_steps 0 \\\n",
    "--per_gpu_eval_batch_size 48 \\\n",
    "--learning_rate 1e-5 \\\n",
    "--adam_epsilon 1e-6 \\\n",
    "--weight_decay 0 \\\n",
    "--train_steps 30000 \\\n",
    "--freeze 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11/2019 12:26:43 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "11/11/2019 12:26:43 - INFO - pytorch_transformers.tokenization_utils -   Model name '../model/chinese_roberta_wwm_large_ext_pytorch' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming '../model/chinese_roberta_wwm_large_ext_pytorch' is a path or url to a directory containing tokenizer files.\n",
      "11/11/2019 12:26:43 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../model/chinese_roberta_wwm_large_ext_pytorch/added_tokens.json. We won't load it.\n",
      "11/11/2019 12:26:43 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../model/chinese_roberta_wwm_large_ext_pytorch/special_tokens_map.json. We won't load it.\n",
      "11/11/2019 12:26:43 - INFO - pytorch_transformers.tokenization_utils -   loading file ../model/chinese_roberta_wwm_large_ext_pytorch/vocab.txt\n",
      "11/11/2019 12:26:43 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/11/2019 12:26:43 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/11/2019 12:26:43 - INFO - pytorch_transformers.modeling_utils -   loading configuration file ../model/chinese_roberta_wwm_large_ext_pytorch/config.json\n",
      "11/11/2019 12:26:43 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 3,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "11/11/2019 12:26:43 - INFO - pytorch_transformers.modeling_utils -   loading weights file ../model/chinese_roberta_wwm_large_ext_pytorch/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "11/11/2019 12:26:48 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification_last2embedding_cls not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "11/11/2019 12:26:48 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification_last2embedding_cls: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "11/11/2019 12:26:50 - INFO - __main__ -   ** RAW EXAMPLE **\n",
      "11/11/2019 12:26:50 - INFO - __main__ -   content: ['这', '几', '天', '看', '了', '有', '人', '举', '报', '施', '某', '某', '的', '贴', '子', '，', '经', '与', '举', '报', '人', '联', '系', '证', '实', '，', '是', '宣', '某', '当', '天', '中', '午', '请', '举', '报', '人', '和', '枪', '手', '喝', '酒', '后', '，', '晚', '上', '才', '发', '的', '贴', '子', '！', '本', '人', '不', '去', '讨', '论', '前', '二', '天', '的', '举', '报', '，', '相', '信', '总', '归', '会', '有', '说', '法', '的', '！', '今', '天', '一', '看', '施', '全', '军', '2017', '年', '1', '月', '2', '日', '实', '名', '举', '报', '上', '黄', '镇', '宣', '国', '才', '的', '贴', '子', '（', '仍', '被', '锁', '定', '禁', '止', '评', '论', '）', '已', '经', '正', '好', '一', '整', '年', '了', '=', '750', ')', 'window', '.', 'open', '(', \"'\", 'http', ':', '/', '/', 'img', '.', 'js', '##ly', '##001', '.', 'com', '/', 'at', '##ta', '##ch', '##ment', '/', 'mon', '_', '180', '##1', '/', '4', '_', '291', '##08', '##5', '_', 'c', '##79', '##6', '##a', '##6', '##a', '##86', '##e', '##17', '##12', '##1', '.', 'jpg', '?', '123', \"'\", ')', ';', '\"', 'on', '##load', '=', '\"', 'if', '(', 'this', '.', 'off', '##set', '##wi', '##dt', '##h', '>', \"'\", '750', \"'\", ')', 'this', '.', 'wi', '##dt', '##h', '=', \"'\", '750', \"'\", ';', '\"', 'sr', '##c', '=', '\"', 'http', ':', '/', '/', 'img', '.', 'js', '##ly', '##001', '.', 'com', '/', 'at', '##ta', '##ch', '##ment', '/', 'mon', '_', '180', '##1', '/', '4', '_', '291', '##08', '##5', '_', 'c', '##79', '##6', '##a', '##6', '##a', '##86', '##e', '##17', '##12', '##1', '.', 'jpg', '?', '123', '\"', 'style', '=', '\"', 'max', '-', 'wi', '##dt', '##h', ':', '750', '##px', ';', '\"', '/', '>', '图', '片', ':', '/', 'home', '/', 'al', '##ida', '##ta', '/', 'www', '/', 'data', '/', 'tm', '##p', '/', 'q', '##fu', '##pl', '##oa', '##d', '/', '4', '_', '291', '##08', '##5', '_', '151', '##49', '##81', '##47', '##14', '##78', '##95', '##2', '.', 'jpg', '施', '全', '军', '实', '名', '举', '报', '50', '天', '后', '，', '上', '黄', '镇', '党', '委', '政', '府', '回', '复', '如', '下', '图', '：', '=', '750', ')', 'window', '.', 'open', '(', \"'\", 'http', ':', '/', '/', 'img', '.', 'js', '##ly', '##001', '.', 'com', '/', 'at', '##ta', '##ch', '##ment', '/', 'mon', '_', '180', '##1', '/', '4', '_', '291', '##08', '##5', '_', 'a9', '##b', '##11', '##b', '##7', '##ea', '##2', '##b', '##1', '##ce', '##9', '.', 'jpg', '?', '90', \"'\", ')', ';', '\"', 'on', '##load', '=', '\"', 'if', '(', 'this', '.', 'off', '##set', '##wi', '##dt', '##h', '>', \"'\", '750', \"'\", ')', 'this', '.', 'wi', '##dt', '##h', '=', \"'\", '750', \"'\", ';', '\"', 'sr', '##c', '=', '\"', 'http', ':', '/', '/', 'img', '.', 'js', '##ly', '##001', '.', 'com', '/', 'at', '##ta', '##ch', '##ment', '/', 'mon', '_', '180', '##1', '/', '4', '_', '291', '##08', '##5', '_', 'a9', '##b', '##11', '##b', '##7', '##ea', '##2', '##b', '##1', '##ce', '##9', '.', 'jpg', '?', '90', '\"', 'style', '=', '\"', 'max', '-', 'wi', '##dt', '##h', ':', '750', '##px', ';', '\"', '/', '>', '图', '片', ':', '/', 'home', '/', 'al', '##ida', '##ta', '/', 'www', '/', 'data', '/', 'tm', '##p', '/', 'q', '##fu', '##pl', '##oa', '##d', '/', '4', '_', '291', '##08', '##5', '_', '151', '##49', '##81', '##47', '##26', '##31', '##66', '##8', '.', 'jpg', '=', '750', ')', 'window', '.', 'open', '(', \"'\", 'http', ':', '/', '/', 'img', '.', 'js', '##ly', '##001', '.', 'com', '/', 'at', '##ta', '##ch', '##ment', '/', 'mon', '_', '180', '##1', '/', '4', '_', '291', '##08', '##5', '_', '9', '##cd', '##e', '##9', '##b', '##39', '##43', '##fe', '##20', '##c', '.', 'jpg', '?', '75', \"'\", ')', ';', '\"', 'on', '##load', '=', '\"', 'if', '(', 'this', '.', 'off', '##set', '##wi', '##dt', '##h', '>', \"'\", '750', \"'\", ')', 'this', '.', 'wi', '##dt', '##h', '=', \"'\", '750', \"'\", ';', '\"', 'sr', '##c', '=', '\"', 'http', ':', '/', '/', 'img', '.', 'js', '##ly', '##001', '.', 'com', '/', 'at', '##ta', '##ch', '##ment', '/', 'mon', '_', '180', '##1', '/', '4', '_', '291', '##08', '##5', '_', '9', '##cd', '##e', '##9', '##b', '##39', '##43', '##fe', '##20', '##c', '.', 'jpg', '?', '75', '\"', 'style', '=', '\"', 'max', '-', 'wi', '##dt', '##h', ':', '750', '##px', ';', '\"', '/', '>', '图', '片', ':', '/', 'home', '/', 'al', '##ida', '##ta', '/', 'www', '/', 'data', '/', 'tm', '##p', '/', 'q', '##fu', '##pl', '##oa', '##d', '/', '4', '_', '291', '##08', '##5', '_', '151', '##49', '##81', '##47', '##23', '##53', '##07', '##5', '.', 'jpg', '一', '年', '的', '贴', '子', '，', '再', '次', '被', '网', '友', '顶', '起', '来', '后', '，', '才', '发', '现', '施', '某', '几', '天', '前', '回', '复', '网', '友', '的', '处', '理', '结', '果', '竟', '如', '下', '图', '：', '=', '750', ')', 'window', '.', 'open', '(', \"'\", 'http', ':', '/', '/', 'img', '.', 'js', '##ly', '##001', '.', 'com', '/', 'at', '##ta', '##ch', '##ment', '/', 'mon', '_', '180', '##1', '/', '4', '_', '291', '##08', '##5', '_', '9', '##d', '##32', '##ee', '##57', '##27', '##60', '##d', '##85', '.', 'jpg', '?', '131', \"'\", ')', ';', '\"', 'on', '##load', '=', '\"', 'if', '(', 'this', '.', 'off', '##set', '##wi', '##dt', '##h', '>', \"'\", '750', \"'\", ')', 'this', '.', 'wi', '##dt', '##h', '=', \"'\", '750', \"'\", ';', '\"', 'sr', '##c', '=', '\"', 'http', ':', '/', '/', 'img', '.', 'js', '##ly', '##001', '.', 'com', '/', 'at', '##ta', '##ch', '##ment', '/', 'mon', '_', '180', '##1', '/', '4', '_', '291', '##08', '##5', '_', '9', '##d', '##32', '##ee', '##57', '##27', '##60', '##d', '##85', '.', 'jpg', '?', '131', '\"', 'style', '=', '\"', 'max', '-', 'wi', '##dt', '##h', ':', '750', '##px', ';', '\"', '/', '>', '图', '片', ':', '/', 'home', '/', 'al', '##ida', '##ta', '/', 'www', '/', 'data', '/', 'tm', '##p', '/', 'q', '##fu', '##pl', '##oa', '##d', '/', '4', '_', '291', '##08', '##5', '_', '151', '##49', '##81', '##47', '##35', '##47', '##17', '##2', '.', 'jpg', '现', '责', '问', '张', '涛', '书', '记', '：', '1', '、', '宣', '国', '才', '被', '举', '报', '这', '么', '多', '问', '题', '，', '什', '么', '时', '候', '有', '答', '复', '。', '2', '、', '宣', '国', '才', '被', '举', '报', '后', '，', '为', '什', '么', '被', '立', '刻', '免', '了', '村', '书', '记', '职', '务', '？', '为', '什', '么', '又', '被', '安', '排', '到', '城', '管', '队', '[UNK]', '吃', '空', '响', '[UNK]', '，', '自', '己', '却', '天', '天', '在', '我', '们', '水', '泥', '厂', '上', '班', '赚', '黑', '钱', '？', '3', '、', '这', '几', '个', '月', '，', '水', '泥', '每', '吨', '近', '200', '元', '纯', '利', '润', '，', '还', '供', '不', '应', '求', '，', '宣', '国', '才', '还', '清', '上', '黄', '政', '府', '担', '保', '借', '给', '宣', '国', '才', '代', '付', '振', '东', '厂', '工', '资', '社', '保', '的', '钱', '了', '吗', '？', '4', '、', '据', '了', '解', '宣', '国', '才', '占', '他', '人', '企', '业', '经', '营', '，', '又', '欠', '税', '52', '.', '16', '万', '元', '、', '欠', '社', '保', '32', '.', '76', '万', '元', '、', '应', '该', '还', '欠', '了', '职', '工', '工', '资', '几', '十', '万', '，', '上', '黄', '政', '府', '打', '算', '替', '宣', '国', '才', '担', '保', '还', '是', '归', '还', '？', '5', '、', '我', '们', '厂', '合', '法', '会', '计', '和', '老', '板', '被', '判', '刑', '四', '到', '六', '年', '，', '现', '在', '服', '刑', '。', '厂', '子', '给', '宣', '国', '才', '强', '占', '，', '宣', '国', '才', '每', '天', '赚', '20', '多', '万', '净', '利', '润', '，', '却', '对', '外', '宣', '称', '天', '天', '亏', '本', '！', '等', '咱', '老', '板', '刑', '满', '回', '厂', '，', '宣', '国', '才', '给', '咱', '厂', '[UNK]', '天', '天', '亏', '[UNK]', '可', '能', '要', '[UNK]', '亏', '[UNK]', '的', '几', '千', '万', '元', '，', '甚', '至', '几', '个', '亿', '，', '张', '涛', '书', '记', '您', '承', '担', '还', '是', '上', '黄', '政', '府', '承', '担', '？', '当', '初', '可', '是', '您', '亲', '自', '把', '厂', '交', '给', '宣', '国', '才', '生', '产', '的', '！', '希', '望', '徐', '市', '长', '看', '到', '本', '贴', '后', '能', '像', '批', '示', '263', '、', '批', '示', '违', '建', '等', '民', '生', '问', '题', '一', '样', '，', '关', '注', '一', '下', '我', '们', '水', '泥', '厂', '的', '将', '来', '！', '也', '请', '徐', '市', '长', '抽', '日', '理', '万', '机', '之', '空', '亲', '自', '约', '谈', '一', '下', '当', '事', '人', '（', '特', '别', '是', '那', '位', '施', '站', '长', '）', '，', '千', '万', '不', '能', '听', '取', '一', '面', '之', '辞', '！']\n",
      "11/11/2019 12:26:50 - INFO - __main__ -   *** Example ***\n",
      "11/11/2019 12:26:50 - INFO - __main__ -   idx: 0\n",
      "11/11/2019 12:26:50 - INFO - __main__ -   guid: 7a3dd79f90ee419da87190cff60f7a86\n",
      "11/11/2019 12:26:50 - INFO - __main__ -   tokens: [CLS] 问 责 领 导 ( 上 黄 镇 党 委 书 记 张 涛 ， 宣 国 才 真 能 一 手 遮 天 吗 ？ ) [SEP] ##57 ##27 ##60 ##d ##85 . jpg ? 131 \" style = \" max - wi ##dt ##h : 750 ##px ; \" / > 图 片 : / home / al ##ida ##ta / www / data / tm ##p / q ##fu ##pl ##oa ##d / 4 _ 291 ##08 ##5 _ 151 ##49 ##81 ##47 ##35 ##47 ##17 ##2 . jpg 现 责 问 张 涛 书 记 ： 1 、 宣 国 才 被 举 报 这 么 多 问 题 ， 什 么 时 候 有 答 复 。 2 、 宣 国 才 被 举 报 后 ， 为 什 么 被 立 刻 免 了 村 书 记 职 务 ？ 为 什 么 又 被 安 排 到 城 管 队 [UNK] 吃 空 响 [UNK] ， 自 己 却 天 天 在 我 们 水 泥 厂 上 班 赚 黑 钱 ？ 3 、 这 几 个 月 ， 水 泥 每 吨 近 200 元 纯 利 润 ， 还 供 不 应 求 ， 宣 国 才 还 清 上 黄 政 府 担 保 借 给 宣 国 才 代 付 振 东 厂 工 资 社 保 的 钱 了 吗 ？ 4 、 据 了 解 宣 国 才 占 他 人 企 业 经 营 ， 又 欠 税 52 . 16 万 元 、 欠 社 保 32 . 76 万 元 、 应 该 还 欠 了 职 工 工 资 几 十 万 ， 上 黄 政 府 打 算 替 宣 国 才 担 保 还 是 归 还 ？ 5 、 我 们 厂 合 法 会 计 和 老 板 被 判 刑 四 到 六 年 ， 现 在 服 刑 。 厂 子 给 宣 国 才 强 占 ， 宣 国 才 每 天 赚 20 多 万 净 利 润 ， 却 对 外 宣 称 天 天 亏 本 ！ 等 咱 老 板 刑 满 回 厂 ， 宣 国 才 给 咱 厂 [UNK] 天 天 亏 [UNK] 可 能 要 [UNK] 亏 [UNK] 的 几 千 万 元 ， 甚 至 几 个 亿 ， 张 涛 书 记 您 承 担 还 是 上 黄 政 府 承 担 ？ 当 初 可 是 您 亲 自 把 厂 交 给 宣 国 才 生 产 的 ！ 希 望 徐 市 长 看 到 本 贴 后 能 像 批 示 263 、 批 示 违 建 等 民 生 问 题 一 样 ， 关 注 一 下 我 们 水 泥 厂 的 将 来 ！ 也 请 徐 市 长 抽 日 理 万 机 之 空 亲 自 约 谈 一 下 当 事 人 （ 特 别 是 那 位 施 站 长 ） ， 千 万 不 能 听 取 一 面 之 辞 [SEP]\n",
      "11/11/2019 12:26:50 - INFO - __main__ -   input_ids: 101 7309 6569 7566 2193 113 677 7942 7252 1054 1999 741 6381 2476 3875 8024 2146 1744 2798 4696 5543 671 2797 6902 1921 1408 8043 114 102 9647 8976 8581 8168 9169 119 9248 136 9403 107 8969 134 107 8621 118 8541 12672 8199 131 9180 10605 132 107 120 135 1745 4275 131 120 8563 120 9266 12708 8383 120 8173 120 9000 120 9908 8187 120 159 12043 12569 11355 8168 120 125 142 11777 9153 8157 142 9564 9500 9313 9050 8852 9050 8408 8144 119 9248 4385 6569 7309 2476 3875 741 6381 8038 122 510 2146 1744 2798 6158 715 2845 6821 720 1914 7309 7579 8024 784 720 3198 952 3300 5031 1908 511 123 510 2146 1744 2798 6158 715 2845 1400 8024 711 784 720 6158 4989 1174 1048 749 3333 741 6381 5466 1218 8043 711 784 720 1348 6158 2128 2961 1168 1814 5052 7339 100 1391 4958 1510 100 8024 5632 2346 1316 1921 1921 1762 2769 812 3717 3799 1322 677 4408 6611 7946 7178 8043 124 510 6821 1126 702 3299 8024 3717 3799 3680 1417 6818 8185 1039 5283 1164 3883 8024 6820 897 679 2418 3724 8024 2146 1744 2798 6820 3926 677 7942 3124 2424 2857 924 955 5314 2146 1744 2798 807 802 2920 691 1322 2339 6598 4852 924 4638 7178 749 1408 8043 125 510 2945 749 6237 2146 1744 2798 1304 800 782 821 689 5307 5852 8024 1348 3612 4925 8247 119 8121 674 1039 510 3612 4852 924 8211 119 8399 674 1039 510 2418 6421 6820 3612 749 5466 2339 2339 6598 1126 1282 674 8024 677 7942 3124 2424 2802 5050 3296 2146 1744 2798 2857 924 6820 3221 2495 6820 8043 126 510 2769 812 1322 1394 3791 833 6369 1469 5439 3352 6158 1161 1152 1724 1168 1063 2399 8024 4385 1762 3302 1152 511 1322 2094 5314 2146 1744 2798 2487 1304 8024 2146 1744 2798 3680 1921 6611 8113 1914 674 1112 1164 3883 8024 1316 2190 1912 2146 4917 1921 1921 755 3315 8013 5023 1493 5439 3352 1152 4007 1726 1322 8024 2146 1744 2798 5314 1493 1322 100 1921 1921 755 100 1377 5543 6206 100 755 100 4638 1126 1283 674 1039 8024 4493 5635 1126 702 783 8024 2476 3875 741 6381 2644 2824 2857 6820 3221 677 7942 3124 2424 2824 2857 8043 2496 1159 1377 3221 2644 779 5632 2828 1322 769 5314 2146 1744 2798 4495 772 4638 8013 2361 3307 2528 2356 7270 4692 1168 3315 6585 1400 5543 1008 2821 4850 10864 510 2821 4850 6824 2456 5023 3696 4495 7309 7579 671 3416 8024 1068 3800 671 678 2769 812 3717 3799 1322 4638 2199 3341 8013 738 6435 2528 2356 7270 2853 3189 4415 674 3322 722 4958 779 5632 5276 6448 671 678 2496 752 782 8020 4294 1166 3221 6929 855 3177 4991 7270 8021 8024 1283 674 679 5543 1420 1357 671 7481 722 6791 102\n",
      "11/11/2019 12:26:50 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/11/2019 12:26:50 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/11/2019 12:26:50 - INFO - __main__ -   label: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11/2019 12:27:41 - INFO - __main__ -   ***** Running training *****\n",
      "11/11/2019 12:27:41 - INFO - __main__ -     Num examples = 11756\n",
      "11/11/2019 12:27:41 - INFO - __main__ -     Batch size = 4\n",
      "11/11/2019 12:27:41 - INFO - __main__ -     Num steps = 30000\n",
      "  0%|                                                 | 0/30000 [00:00<?, ?it/s]11/11/2019 12:27:53 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 12:27:53 - INFO - __main__ -     Num examples = 2940\n",
      "11/11/2019 12:27:53 - INFO - __main__ -     Batch size = 48\n",
      "/home/haizhi/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "11/11/2019 12:30:47 - INFO - __main__ -     eval_F1 = 0.21080379979101713\n",
      "11/11/2019 12:30:47 - INFO - __main__ -     eval_loss = 1.3429770152415\n",
      "11/11/2019 12:30:47 - INFO - __main__ -     global_step = 0\n",
      "================================================================================\n",
      "Best F1 0.21080379979101713\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.7152:   3%|▋                       | 799/30000 [06:28<2:06:20,  3.85it/s]11/11/2019 12:34:10 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 12:34:10 - INFO - __main__ -     global_step = 200\n",
      "11/11/2019 12:34:10 - INFO - __main__ -     train loss = 0.7152\n",
      "loss 0.4983:   5%|█▏                     | 1599/30000 [09:43<2:09:34,  3.65it/s]11/11/2019 12:37:25 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 12:37:25 - INFO - __main__ -     global_step = 400\n",
      "11/11/2019 12:37:25 - INFO - __main__ -     train loss = 0.4983\n",
      "loss 0.421:   8%|█▉                      | 2399/30000 [12:58<2:06:13,  3.64it/s]11/11/2019 12:40:40 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 12:40:40 - INFO - __main__ -     global_step = 600\n",
      "11/11/2019 12:40:40 - INFO - __main__ -     train loss = 0.421\n",
      "loss 0.476:  11%|██▌                     | 3199/30000 [16:14<2:00:24,  3.71it/s]11/11/2019 12:43:55 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 12:43:55 - INFO - __main__ -     global_step = 800\n",
      "11/11/2019 12:43:55 - INFO - __main__ -     train loss = 0.476\n",
      "loss 0.4022:  13%|███                    | 3999/30000 [19:29<1:55:51,  3.74it/s]11/11/2019 12:47:11 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 12:47:11 - INFO - __main__ -     global_step = 1000\n",
      "11/11/2019 12:47:11 - INFO - __main__ -     train loss = 0.4022\n",
      "loss 0.4113:  16%|███▋                   | 4799/30000 [22:45<1:50:28,  3.80it/s]11/11/2019 12:50:26 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 12:50:26 - INFO - __main__ -     global_step = 1200\n",
      "11/11/2019 12:50:26 - INFO - __main__ -     train loss = 0.4113\n",
      "loss 0.3792:  19%|████▎                  | 5599/30000 [26:00<1:46:17,  3.83it/s]11/11/2019 12:53:41 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 12:53:41 - INFO - __main__ -     global_step = 1400\n",
      "11/11/2019 12:53:41 - INFO - __main__ -     train loss = 0.3792\n",
      "loss 0.4067:  21%|████▉                  | 6399/30000 [29:16<1:49:23,  3.60it/s]11/11/2019 12:56:57 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 12:56:57 - INFO - __main__ -     global_step = 1600\n",
      "11/11/2019 12:56:57 - INFO - __main__ -     train loss = 0.4067\n",
      "loss 0.3778:  24%|█████▌                 | 7199/30000 [32:32<1:46:12,  3.58it/s]11/11/2019 13:00:14 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 13:00:14 - INFO - __main__ -     global_step = 1800\n",
      "11/11/2019 13:00:14 - INFO - __main__ -     train loss = 0.3778\n",
      "loss 0.4215:  27%|██████▏                | 7999/30000 [35:49<1:42:33,  3.58it/s]11/11/2019 13:03:30 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 13:03:30 - INFO - __main__ -     global_step = 2000\n",
      "11/11/2019 13:03:30 - INFO - __main__ -     train loss = 0.4215\n",
      "loss 0.3821:  29%|██████▋                | 8799/30000 [39:05<1:32:52,  3.80it/s]11/11/2019 13:06:47 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 13:06:47 - INFO - __main__ -     global_step = 2200\n",
      "11/11/2019 13:06:47 - INFO - __main__ -     train loss = 0.3821\n",
      "loss 0.4164:  32%|███████▎               | 9599/30000 [42:21<1:35:23,  3.56it/s]11/11/2019 13:10:02 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 13:10:02 - INFO - __main__ -     global_step = 2400\n",
      "11/11/2019 13:10:02 - INFO - __main__ -     train loss = 0.4164\n",
      "loss 0.3611:  35%|███████▋              | 10399/30000 [45:38<1:30:15,  3.62it/s]11/11/2019 13:13:19 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 13:13:19 - INFO - __main__ -     global_step = 2600\n",
      "11/11/2019 13:13:19 - INFO - __main__ -     train loss = 0.3611\n",
      "loss 0.4176:  37%|████████▏             | 11199/30000 [48:53<1:23:18,  3.76it/s]11/11/2019 13:16:35 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 13:16:35 - INFO - __main__ -     global_step = 2800\n",
      "11/11/2019 13:16:35 - INFO - __main__ -     train loss = 0.4176\n",
      "11/11/2019 13:16:47 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 13:16:47 - INFO - __main__ -     Num examples = 2940\n",
      "11/11/2019 13:16:47 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 13:19:46 - INFO - __main__ -     eval_F1 = 0.7651019221167136\n",
      "11/11/2019 13:19:46 - INFO - __main__ -     eval_loss = 0.3607653509224615\n",
      "11/11/2019 13:19:46 - INFO - __main__ -     global_step = 2800\n",
      "11/11/2019 13:19:46 - INFO - __main__ -     loss = 0.4176\n",
      "================================================================================\n",
      "Best F1 0.7651019221167136\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.3642:  40%|████████▊             | 11999/30000 [55:30<1:18:31,  3.82it/s]11/11/2019 13:23:12 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 13:23:12 - INFO - __main__ -     global_step = 3000\n",
      "11/11/2019 13:23:12 - INFO - __main__ -     train loss = 0.3642\n",
      "11/11/2019 13:23:24 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 13:23:24 - INFO - __main__ -     Num examples = 2940\n",
      "11/11/2019 13:23:24 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 13:26:22 - INFO - __main__ -     eval_F1 = 0.7833425976187401\n",
      "11/11/2019 13:26:22 - INFO - __main__ -     eval_loss = 0.3394716391039471\n",
      "11/11/2019 13:26:22 - INFO - __main__ -     global_step = 3000\n",
      "11/11/2019 13:26:22 - INFO - __main__ -     loss = 0.3642\n",
      "================================================================================\n",
      "Best F1 0.7833425976187401\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.2903:  43%|████████▌           | 12799/30000 [1:02:08<1:20:38,  3.55it/s]11/11/2019 13:29:49 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 13:29:49 - INFO - __main__ -     global_step = 3200\n",
      "11/11/2019 13:29:49 - INFO - __main__ -     train loss = 0.2903\n",
      "11/11/2019 13:30:01 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 13:30:01 - INFO - __main__ -     Num examples = 2940\n",
      "11/11/2019 13:30:01 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 13:33:00 - INFO - __main__ -     eval_F1 = 0.7812565776762316\n",
      "11/11/2019 13:33:00 - INFO - __main__ -     eval_loss = 0.3923243076330231\n",
      "11/11/2019 13:33:00 - INFO - __main__ -     global_step = 3200\n",
      "11/11/2019 13:33:00 - INFO - __main__ -     loss = 0.2903\n",
      "================================================================================\n",
      "loss 0.2385:  45%|█████████           | 13599/30000 [1:08:36<1:11:44,  3.81it/s]11/11/2019 13:36:18 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 13:36:18 - INFO - __main__ -     global_step = 3400\n",
      "11/11/2019 13:36:18 - INFO - __main__ -     train loss = 0.2385\n",
      "11/11/2019 13:36:30 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 13:36:30 - INFO - __main__ -     Num examples = 2940\n",
      "11/11/2019 13:36:30 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 13:39:29 - INFO - __main__ -     eval_F1 = 0.7978414291804993\n",
      "11/11/2019 13:39:29 - INFO - __main__ -     eval_loss = 0.4422820415948668\n",
      "11/11/2019 13:39:29 - INFO - __main__ -     global_step = 3400\n",
      "11/11/2019 13:39:29 - INFO - __main__ -     loss = 0.2385\n",
      "================================================================================\n",
      "Best F1 0.7978414291804993\n",
      "Saving Model......\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "loss 0.2024:  48%|█████████▌          | 14399/30000 [1:15:15<1:13:15,  3.55it/s]11/11/2019 13:42:57 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 13:42:57 - INFO - __main__ -     global_step = 3600\n",
      "11/11/2019 13:42:57 - INFO - __main__ -     train loss = 0.2024\n",
      "11/11/2019 13:43:09 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 13:43:09 - INFO - __main__ -     Num examples = 2940\n",
      "11/11/2019 13:43:09 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 13:46:08 - INFO - __main__ -     eval_F1 = 0.7951144379678589\n",
      "11/11/2019 13:46:08 - INFO - __main__ -     eval_loss = 0.44398676599526116\n",
      "11/11/2019 13:46:08 - INFO - __main__ -     global_step = 3600\n",
      "11/11/2019 13:46:08 - INFO - __main__ -     loss = 0.2024\n",
      "================================================================================\n",
      "loss 0.2346:  51%|██████████▏         | 15199/30000 [1:21:45<1:04:25,  3.83it/s]11/11/2019 13:49:27 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 13:49:27 - INFO - __main__ -     global_step = 3800\n",
      "11/11/2019 13:49:27 - INFO - __main__ -     train loss = 0.2346\n",
      "11/11/2019 13:49:39 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 13:49:39 - INFO - __main__ -     Num examples = 2940\n",
      "11/11/2019 13:49:39 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 13:52:38 - INFO - __main__ -     eval_F1 = 0.7949620194437138\n",
      "11/11/2019 13:52:38 - INFO - __main__ -     eval_loss = 0.4040414808738616\n",
      "11/11/2019 13:52:38 - INFO - __main__ -     global_step = 3800\n",
      "11/11/2019 13:52:38 - INFO - __main__ -     loss = 0.2346\n",
      "================================================================================\n",
      "loss 0.2038:  53%|██████████▋         | 15999/30000 [1:28:14<1:02:17,  3.75it/s]11/11/2019 13:55:56 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 13:55:56 - INFO - __main__ -     global_step = 4000\n",
      "11/11/2019 13:55:56 - INFO - __main__ -     train loss = 0.2038\n",
      "11/11/2019 13:56:08 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 13:56:08 - INFO - __main__ -     Num examples = 2940\n",
      "11/11/2019 13:56:08 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 13:59:08 - INFO - __main__ -     eval_F1 = 0.7949278588436647\n",
      "11/11/2019 13:59:08 - INFO - __main__ -     eval_loss = 0.44391422662433355\n",
      "11/11/2019 13:59:08 - INFO - __main__ -     global_step = 4000\n",
      "11/11/2019 13:59:08 - INFO - __main__ -     loss = 0.2038\n",
      "================================================================================\n",
      "loss 0.1829:  56%|███████████▏        | 16799/30000 [1:34:45<1:01:18,  3.59it/s]11/11/2019 14:02:27 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 14:02:27 - INFO - __main__ -     global_step = 4200\n",
      "11/11/2019 14:02:27 - INFO - __main__ -     train loss = 0.1829\n",
      "11/11/2019 14:02:39 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 14:02:39 - INFO - __main__ -     Num examples = 2940\n",
      "11/11/2019 14:02:39 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 14:05:38 - INFO - __main__ -     eval_F1 = 0.7965619150871875\n",
      "11/11/2019 14:05:38 - INFO - __main__ -     eval_loss = 0.4319697125634599\n",
      "11/11/2019 14:05:38 - INFO - __main__ -     global_step = 4200\n",
      "11/11/2019 14:05:38 - INFO - __main__ -     loss = 0.1829\n",
      "================================================================================\n",
      "loss 0.1923:  59%|████████████▉         | 17599/30000 [1:41:13<57:53,  3.57it/s]11/11/2019 14:08:54 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 14:08:54 - INFO - __main__ -     global_step = 4400\n",
      "11/11/2019 14:08:54 - INFO - __main__ -     train loss = 0.1923\n",
      "11/11/2019 14:09:07 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 14:09:07 - INFO - __main__ -     Num examples = 2940\n",
      "11/11/2019 14:09:07 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 14:12:06 - INFO - __main__ -     eval_F1 = 0.7955653225674931\n",
      "11/11/2019 14:12:06 - INFO - __main__ -     eval_loss = 0.42128544207662344\n",
      "11/11/2019 14:12:06 - INFO - __main__ -     global_step = 4400\n",
      "11/11/2019 14:12:06 - INFO - __main__ -     loss = 0.1923\n",
      "================================================================================\n",
      "loss 0.2277:  61%|█████████████▍        | 18399/30000 [1:47:41<53:18,  3.63it/s]11/11/2019 14:15:22 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 14:15:22 - INFO - __main__ -     global_step = 4600\n",
      "11/11/2019 14:15:22 - INFO - __main__ -     train loss = 0.2277\n",
      "11/11/2019 14:15:34 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 14:15:34 - INFO - __main__ -     Num examples = 2940\n",
      "11/11/2019 14:15:34 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 14:18:33 - INFO - __main__ -     eval_F1 = 0.7791841745214879\n",
      "11/11/2019 14:18:33 - INFO - __main__ -     eval_loss = 0.4216324659364839\n",
      "11/11/2019 14:18:33 - INFO - __main__ -     global_step = 4600\n",
      "11/11/2019 14:18:33 - INFO - __main__ -     loss = 0.2277\n",
      "================================================================================\n",
      "loss 0.2343:  64%|██████████████        | 19199/30000 [1:54:07<47:24,  3.80it/s]11/11/2019 14:21:49 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 14:21:49 - INFO - __main__ -     global_step = 4800\n",
      "11/11/2019 14:21:49 - INFO - __main__ -     train loss = 0.2343\n",
      "11/11/2019 14:22:01 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 14:22:01 - INFO - __main__ -     Num examples = 2940\n",
      "11/11/2019 14:22:01 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 14:24:59 - INFO - __main__ -     eval_F1 = 0.7866608870306063\n",
      "11/11/2019 14:24:59 - INFO - __main__ -     eval_loss = 0.3774441752222277\n",
      "11/11/2019 14:24:59 - INFO - __main__ -     global_step = 4800\n",
      "11/11/2019 14:24:59 - INFO - __main__ -     loss = 0.2343\n",
      "================================================================================\n",
      "loss 0.2272:  67%|██████████████▋       | 19999/30000 [2:00:34<45:20,  3.68it/s]11/11/2019 14:28:15 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 14:28:15 - INFO - __main__ -     global_step = 5000\n",
      "11/11/2019 14:28:15 - INFO - __main__ -     train loss = 0.2272\n",
      "11/11/2019 14:28:28 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 14:28:28 - INFO - __main__ -     Num examples = 2940\n",
      "11/11/2019 14:28:28 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 14:31:26 - INFO - __main__ -     eval_F1 = 0.7827724963958186\n",
      "11/11/2019 14:31:26 - INFO - __main__ -     eval_loss = 0.3844265172318105\n",
      "11/11/2019 14:31:26 - INFO - __main__ -     global_step = 5000\n",
      "11/11/2019 14:31:26 - INFO - __main__ -     loss = 0.2272\n",
      "================================================================================\n",
      "loss 0.2318:  69%|███████████████▎      | 20799/30000 [2:07:01<43:05,  3.56it/s]11/11/2019 14:34:43 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 14:34:43 - INFO - __main__ -     global_step = 5200\n",
      "11/11/2019 14:34:43 - INFO - __main__ -     train loss = 0.2318\n",
      "11/11/2019 14:34:55 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 14:34:55 - INFO - __main__ -     Num examples = 2940\n",
      "11/11/2019 14:34:55 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 14:37:54 - INFO - __main__ -     eval_F1 = 0.7605774725840663\n",
      "11/11/2019 14:37:54 - INFO - __main__ -     eval_loss = 0.4133945487800144\n",
      "11/11/2019 14:37:54 - INFO - __main__ -     global_step = 5200\n",
      "11/11/2019 14:37:54 - INFO - __main__ -     loss = 0.2318\n",
      "================================================================================\n",
      "loss 0.2369:  72%|███████████████▊      | 21599/30000 [2:13:30<36:45,  3.81it/s]11/11/2019 14:41:12 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 14:41:12 - INFO - __main__ -     global_step = 5400\n",
      "11/11/2019 14:41:12 - INFO - __main__ -     train loss = 0.2369\n",
      "11/11/2019 14:41:24 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 14:41:24 - INFO - __main__ -     Num examples = 2940\n",
      "11/11/2019 14:41:24 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 14:44:24 - INFO - __main__ -     eval_F1 = 0.7928453071410432\n",
      "11/11/2019 14:44:24 - INFO - __main__ -     eval_loss = 0.3802461422859661\n",
      "11/11/2019 14:44:24 - INFO - __main__ -     global_step = 5400\n",
      "11/11/2019 14:44:24 - INFO - __main__ -     loss = 0.2369\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.1911:  75%|████████████████▍     | 22399/30000 [2:20:01<35:04,  3.61it/s]11/11/2019 14:47:42 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 14:47:42 - INFO - __main__ -     global_step = 5600\n",
      "11/11/2019 14:47:42 - INFO - __main__ -     train loss = 0.1911\n",
      "11/11/2019 14:47:54 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 14:47:54 - INFO - __main__ -     Num examples = 2940\n",
      "11/11/2019 14:47:54 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 14:50:53 - INFO - __main__ -     eval_F1 = 0.7724328141653083\n",
      "11/11/2019 14:50:53 - INFO - __main__ -     eval_loss = 0.4086642535102944\n",
      "11/11/2019 14:50:53 - INFO - __main__ -     global_step = 5600\n",
      "11/11/2019 14:50:53 - INFO - __main__ -     loss = 0.1911\n",
      "================================================================================\n",
      "loss 0.2359:  77%|█████████████████     | 23199/30000 [2:26:30<30:01,  3.78it/s]11/11/2019 14:54:11 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 14:54:11 - INFO - __main__ -     global_step = 5800\n",
      "11/11/2019 14:54:11 - INFO - __main__ -     train loss = 0.2359\n",
      "11/11/2019 14:54:24 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 14:54:24 - INFO - __main__ -     Num examples = 2940\n",
      "11/11/2019 14:54:24 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 14:57:23 - INFO - __main__ -     eval_F1 = 0.7941700882792629\n",
      "11/11/2019 14:57:23 - INFO - __main__ -     eval_loss = 0.3924405162613238\n",
      "11/11/2019 14:57:23 - INFO - __main__ -     global_step = 5800\n",
      "11/11/2019 14:57:23 - INFO - __main__ -     loss = 0.2359\n",
      "================================================================================\n",
      "loss 0.1819:  80%|█████████████████▌    | 23999/30000 [2:33:00<26:40,  3.75it/s]11/11/2019 15:00:42 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 15:00:42 - INFO - __main__ -     global_step = 6000\n",
      "11/11/2019 15:00:42 - INFO - __main__ -     train loss = 0.1819\n",
      "11/11/2019 15:00:54 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 15:00:54 - INFO - __main__ -     Num examples = 2940\n",
      "11/11/2019 15:00:54 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 15:03:54 - INFO - __main__ -     eval_F1 = 0.7944598039891145\n",
      "11/11/2019 15:03:54 - INFO - __main__ -     eval_loss = 0.3876100788794218\n",
      "11/11/2019 15:03:54 - INFO - __main__ -     global_step = 6000\n",
      "11/11/2019 15:03:54 - INFO - __main__ -     loss = 0.1819\n",
      "================================================================================\n",
      "loss 0.1201:  83%|██████████████████▏   | 24799/30000 [2:39:30<24:05,  3.60it/s]11/11/2019 15:07:11 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 15:07:11 - INFO - __main__ -     global_step = 6200\n",
      "11/11/2019 15:07:11 - INFO - __main__ -     train loss = 0.1201\n",
      "11/11/2019 15:07:24 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 15:07:24 - INFO - __main__ -     Num examples = 2940\n",
      "11/11/2019 15:07:24 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 15:10:23 - INFO - __main__ -     eval_F1 = 0.8014772104325004\n",
      "11/11/2019 15:10:23 - INFO - __main__ -     eval_loss = 0.3978031189331124\n",
      "11/11/2019 15:10:23 - INFO - __main__ -     global_step = 6200\n",
      "11/11/2019 15:10:23 - INFO - __main__ -     loss = 0.1201\n",
      "================================================================================\n",
      "Best F1 0.8014772104325004\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.1021:  85%|██████████████████▊   | 25599/30000 [2:46:09<20:36,  3.56it/s]11/11/2019 15:13:51 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 15:13:51 - INFO - __main__ -     global_step = 6400\n",
      "11/11/2019 15:13:51 - INFO - __main__ -     train loss = 0.1021\n",
      "11/11/2019 15:14:03 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 15:14:03 - INFO - __main__ -     Num examples = 2940\n",
      "11/11/2019 15:14:03 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 15:17:02 - INFO - __main__ -     eval_F1 = 0.7928830191082707\n",
      "11/11/2019 15:17:02 - INFO - __main__ -     eval_loss = 0.4440073729943364\n",
      "11/11/2019 15:17:02 - INFO - __main__ -     global_step = 6400\n",
      "11/11/2019 15:17:02 - INFO - __main__ -     loss = 0.1021\n",
      "================================================================================\n",
      "loss 0.0959:  88%|███████████████████▎  | 26399/30000 [2:52:37<15:50,  3.79it/s]11/11/2019 15:20:19 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 15:20:19 - INFO - __main__ -     global_step = 6600\n",
      "11/11/2019 15:20:19 - INFO - __main__ -     train loss = 0.0959\n",
      "11/11/2019 15:20:31 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 15:20:31 - INFO - __main__ -     Num examples = 2940\n",
      "11/11/2019 15:20:31 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 15:23:29 - INFO - __main__ -     eval_F1 = 0.799366901440171\n",
      "11/11/2019 15:23:29 - INFO - __main__ -     eval_loss = 0.4806130097938641\n",
      "11/11/2019 15:23:29 - INFO - __main__ -     global_step = 6600\n",
      "11/11/2019 15:23:29 - INFO - __main__ -     loss = 0.0959\n",
      "================================================================================\n",
      "loss 0.1086:  91%|███████████████████▉  | 27199/30000 [2:59:05<13:09,  3.55it/s]11/11/2019 15:26:47 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 15:26:47 - INFO - __main__ -     global_step = 6800\n",
      "11/11/2019 15:26:47 - INFO - __main__ -     train loss = 0.1086\n",
      "11/11/2019 15:26:59 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 15:26:59 - INFO - __main__ -     Num examples = 2940\n",
      "11/11/2019 15:26:59 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 15:29:58 - INFO - __main__ -     eval_F1 = 0.7903346151500497\n",
      "11/11/2019 15:29:58 - INFO - __main__ -     eval_loss = 0.4557783109015755\n",
      "11/11/2019 15:29:58 - INFO - __main__ -     global_step = 6800\n",
      "11/11/2019 15:29:58 - INFO - __main__ -     loss = 0.1086\n",
      "================================================================================\n",
      "loss 0.0952:  93%|████████████████████▌ | 27999/30000 [3:05:33<08:45,  3.81it/s]11/11/2019 15:33:15 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 15:33:15 - INFO - __main__ -     global_step = 7000\n",
      "11/11/2019 15:33:15 - INFO - __main__ -     train loss = 0.0952\n",
      "11/11/2019 15:33:27 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 15:33:27 - INFO - __main__ -     Num examples = 2940\n",
      "11/11/2019 15:33:27 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 15:36:25 - INFO - __main__ -     eval_F1 = 0.8067649470296248\n",
      "11/11/2019 15:36:25 - INFO - __main__ -     eval_loss = 0.4543699422295416\n",
      "11/11/2019 15:36:25 - INFO - __main__ -     global_step = 7000\n",
      "11/11/2019 15:36:25 - INFO - __main__ -     loss = 0.0952\n",
      "================================================================================\n",
      "Best F1 0.8067649470296248\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.0917:  96%|█████████████████████ | 28799/30000 [3:12:11<05:14,  3.81it/s]11/11/2019 15:39:53 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 15:39:53 - INFO - __main__ -     global_step = 7200\n",
      "11/11/2019 15:39:53 - INFO - __main__ -     train loss = 0.0917\n",
      "11/11/2019 15:40:05 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 15:40:05 - INFO - __main__ -     Num examples = 2940\n",
      "11/11/2019 15:40:05 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 15:43:04 - INFO - __main__ -     eval_F1 = 0.807653769267959\n",
      "11/11/2019 15:43:04 - INFO - __main__ -     eval_loss = 0.4671423701614502\n",
      "11/11/2019 15:43:04 - INFO - __main__ -     global_step = 7200\n",
      "11/11/2019 15:43:04 - INFO - __main__ -     loss = 0.0917\n",
      "================================================================================\n",
      "Best F1 0.807653769267959\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.0751:  99%|█████████████████████▋| 29599/30000 [3:18:52<01:45,  3.81it/s]11/11/2019 15:46:33 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 15:46:33 - INFO - __main__ -     global_step = 7400\n",
      "11/11/2019 15:46:33 - INFO - __main__ -     train loss = 0.0751\n",
      "11/11/2019 15:46:45 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 15:46:45 - INFO - __main__ -     Num examples = 2940\n",
      "11/11/2019 15:46:45 - INFO - __main__ -     Batch size = 48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11/2019 15:49:45 - INFO - __main__ -     eval_F1 = 0.8049427468675217\n",
      "11/11/2019 15:49:45 - INFO - __main__ -     eval_loss = 0.46975615088106887\n",
      "11/11/2019 15:49:45 - INFO - __main__ -     global_step = 7400\n",
      "11/11/2019 15:49:45 - INFO - __main__ -     loss = 0.0751\n",
      "================================================================================\n",
      "loss 0.093: 100%|███████████████████████| 30000/30000 [3:23:43<00:00,  2.45it/s]\n",
      "11/11/2019 15:51:25 - INFO - pytorch_transformers.modeling_utils -   loading weights file ../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_1/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "Traceback (most recent call last):\n",
      "  File \"./run_bert_2562_last2embedding_cls.py\", line 841, in <module>\n",
      "    main()\n",
      "  File \"./run_bert_2562_last2embedding_cls.py\", line 757, in main\n",
      "    logits = model(input_ids=input_ids, token_type_ids=segment_ids, attention_mask=input_mask).detach().cpu().numpy()\n",
      "  File \"/home/haizhi/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/haizhi/workspace/TODO/2019互联网新闻/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 1119, in forward\n",
      "    attention_mask=flat_attention_mask, head_mask=head_mask)\n",
      "  File \"/home/haizhi/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/haizhi/workspace/TODO/2019互联网新闻/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 727, in forward\n",
      "    head_mask=head_mask)\n",
      "  File \"/home/haizhi/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/haizhi/workspace/TODO/2019互联网新闻/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 440, in forward\n",
      "    layer_outputs = layer_module(hidden_states, attention_mask, head_mask[i])\n",
      "  File \"/home/haizhi/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/haizhi/workspace/TODO/2019互联网新闻/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 420, in forward\n",
      "    intermediate_output = self.intermediate(attention_output)\n",
      "  File \"/home/haizhi/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/haizhi/workspace/TODO/2019互联网新闻/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 392, in forward\n",
      "    hidden_states = self.intermediate_act_fn(hidden_states)\n",
      "  File \"/home/haizhi/workspace/TODO/2019互联网新闻/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 142, in gelu\n",
      "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 384.00 MiB (GPU 0; 10.92 GiB total capacity; 9.35 GiB already allocated; 191.00 MiB free; 810.30 MiB cached)\n"
     ]
    }
   ],
   "source": [
    "!python ./run_bert_2562_last2embedding_cls.py \\\n",
    "--model_type bert \\\n",
    "--model_name_or_path ../model/chinese_roberta_wwm_large_ext_pytorch  \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_test \\\n",
    "--data_dir ../data/data_StratifiedKFold_42/data_origin_1 \\\n",
    "--output_dir ../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_1 \\\n",
    "--max_seq_length 512 \\\n",
    "--split_num 1 \\\n",
    "--lstm_hidden_size 512 \\\n",
    "--lstm_layers 1 \\\n",
    "--lstm_dropout 0.1 \\\n",
    "--eval_steps 200 \\\n",
    "--per_gpu_train_batch_size 4 \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--warmup_steps 0 \\\n",
    "--per_gpu_eval_batch_size 48 \\\n",
    "--learning_rate 1e-5 \\\n",
    "--adam_epsilon 1e-6 \\\n",
    "--weight_decay 0 \\\n",
    "--train_steps 30000 \\\n",
    "--freeze 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11/2019 15:51:46 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "11/11/2019 15:51:46 - INFO - pytorch_transformers.tokenization_utils -   Model name '../model/chinese_roberta_wwm_large_ext_pytorch' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming '../model/chinese_roberta_wwm_large_ext_pytorch' is a path or url to a directory containing tokenizer files.\n",
      "11/11/2019 15:51:46 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../model/chinese_roberta_wwm_large_ext_pytorch/added_tokens.json. We won't load it.\n",
      "11/11/2019 15:51:46 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../model/chinese_roberta_wwm_large_ext_pytorch/special_tokens_map.json. We won't load it.\n",
      "11/11/2019 15:51:46 - INFO - pytorch_transformers.tokenization_utils -   loading file ../model/chinese_roberta_wwm_large_ext_pytorch/vocab.txt\n",
      "11/11/2019 15:51:46 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/11/2019 15:51:46 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/11/2019 15:51:46 - INFO - pytorch_transformers.modeling_utils -   loading configuration file ../model/chinese_roberta_wwm_large_ext_pytorch/config.json\n",
      "11/11/2019 15:51:46 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 3,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "11/11/2019 15:51:46 - INFO - pytorch_transformers.modeling_utils -   loading weights file ../model/chinese_roberta_wwm_large_ext_pytorch/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "11/11/2019 15:51:51 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification_last2embedding_cls not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "11/11/2019 15:51:51 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification_last2embedding_cls: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "11/11/2019 15:51:54 - INFO - __main__ -   ** RAW EXAMPLE **\n",
      "11/11/2019 15:51:54 - INFO - __main__ -   content: ['这', '几', '天', '看', '了', '有', '人', '举', '报', '施', '某', '某', '的', '贴', '子', '，', '经', '与', '举', '报', '人', '联', '系', '证', '实', '，', '是', '宣', '某', '当', '天', '中', '午', '请', '举', '报', '人', '和', '枪', '手', '喝', '酒', '后', '，', '晚', '上', '才', '发', '的', '贴', '子', '！', '本', '人', '不', '去', '讨', '论', '前', '二', '天', '的', '举', '报', '，', '相', '信', '总', '归', '会', '有', '说', '法', '的', '！', '今', '天', '一', '看', '施', '全', '军', '2017', '年', '1', '月', '2', '日', '实', '名', '举', '报', '上', '黄', '镇', '宣', '国', '才', '的', '贴', '子', '（', '仍', '被', '锁', '定', '禁', '止', '评', '论', '）', '已', '经', '正', '好', '一', '整', '年', '了', '=', '750', ')', 'window', '.', 'open', '(', \"'\", 'http', ':', '/', '/', 'img', '.', 'js', '##ly', '##001', '.', 'com', '/', 'at', '##ta', '##ch', '##ment', '/', 'mon', '_', '180', '##1', '/', '4', '_', '291', '##08', '##5', '_', 'c', '##79', '##6', '##a', '##6', '##a', '##86', '##e', '##17', '##12', '##1', '.', 'jpg', '?', '123', \"'\", ')', ';', '\"', 'on', '##load', '=', '\"', 'if', '(', 'this', '.', 'off', '##set', '##wi', '##dt', '##h', '>', \"'\", '750', \"'\", ')', 'this', '.', 'wi', '##dt', '##h', '=', \"'\", '750', \"'\", ';', '\"', 'sr', '##c', '=', '\"', 'http', ':', '/', '/', 'img', '.', 'js', '##ly', '##001', '.', 'com', '/', 'at', '##ta', '##ch', '##ment', '/', 'mon', '_', '180', '##1', '/', '4', '_', '291', '##08', '##5', '_', 'c', '##79', '##6', '##a', '##6', '##a', '##86', '##e', '##17', '##12', '##1', '.', 'jpg', '?', '123', '\"', 'style', '=', '\"', 'max', '-', 'wi', '##dt', '##h', ':', '750', '##px', ';', '\"', '/', '>', '图', '片', ':', '/', 'home', '/', 'al', '##ida', '##ta', '/', 'www', '/', 'data', '/', 'tm', '##p', '/', 'q', '##fu', '##pl', '##oa', '##d', '/', '4', '_', '291', '##08', '##5', '_', '151', '##49', '##81', '##47', '##14', '##78', '##95', '##2', '.', 'jpg', '施', '全', '军', '实', '名', '举', '报', '50', '天', '后', '，', '上', '黄', '镇', '党', '委', '政', '府', '回', '复', '如', '下', '图', '：', '=', '750', ')', 'window', '.', 'open', '(', \"'\", 'http', ':', '/', '/', 'img', '.', 'js', '##ly', '##001', '.', 'com', '/', 'at', '##ta', '##ch', '##ment', '/', 'mon', '_', '180', '##1', '/', '4', '_', '291', '##08', '##5', '_', 'a9', '##b', '##11', '##b', '##7', '##ea', '##2', '##b', '##1', '##ce', '##9', '.', 'jpg', '?', '90', \"'\", ')', ';', '\"', 'on', '##load', '=', '\"', 'if', '(', 'this', '.', 'off', '##set', '##wi', '##dt', '##h', '>', \"'\", '750', \"'\", ')', 'this', '.', 'wi', '##dt', '##h', '=', \"'\", '750', \"'\", ';', '\"', 'sr', '##c', '=', '\"', 'http', ':', '/', '/', 'img', '.', 'js', '##ly', '##001', '.', 'com', '/', 'at', '##ta', '##ch', '##ment', '/', 'mon', '_', '180', '##1', '/', '4', '_', '291', '##08', '##5', '_', 'a9', '##b', '##11', '##b', '##7', '##ea', '##2', '##b', '##1', '##ce', '##9', '.', 'jpg', '?', '90', '\"', 'style', '=', '\"', 'max', '-', 'wi', '##dt', '##h', ':', '750', '##px', ';', '\"', '/', '>', '图', '片', ':', '/', 'home', '/', 'al', '##ida', '##ta', '/', 'www', '/', 'data', '/', 'tm', '##p', '/', 'q', '##fu', '##pl', '##oa', '##d', '/', '4', '_', '291', '##08', '##5', '_', '151', '##49', '##81', '##47', '##26', '##31', '##66', '##8', '.', 'jpg', '=', '750', ')', 'window', '.', 'open', '(', \"'\", 'http', ':', '/', '/', 'img', '.', 'js', '##ly', '##001', '.', 'com', '/', 'at', '##ta', '##ch', '##ment', '/', 'mon', '_', '180', '##1', '/', '4', '_', '291', '##08', '##5', '_', '9', '##cd', '##e', '##9', '##b', '##39', '##43', '##fe', '##20', '##c', '.', 'jpg', '?', '75', \"'\", ')', ';', '\"', 'on', '##load', '=', '\"', 'if', '(', 'this', '.', 'off', '##set', '##wi', '##dt', '##h', '>', \"'\", '750', \"'\", ')', 'this', '.', 'wi', '##dt', '##h', '=', \"'\", '750', \"'\", ';', '\"', 'sr', '##c', '=', '\"', 'http', ':', '/', '/', 'img', '.', 'js', '##ly', '##001', '.', 'com', '/', 'at', '##ta', '##ch', '##ment', '/', 'mon', '_', '180', '##1', '/', '4', '_', '291', '##08', '##5', '_', '9', '##cd', '##e', '##9', '##b', '##39', '##43', '##fe', '##20', '##c', '.', 'jpg', '?', '75', '\"', 'style', '=', '\"', 'max', '-', 'wi', '##dt', '##h', ':', '750', '##px', ';', '\"', '/', '>', '图', '片', ':', '/', 'home', '/', 'al', '##ida', '##ta', '/', 'www', '/', 'data', '/', 'tm', '##p', '/', 'q', '##fu', '##pl', '##oa', '##d', '/', '4', '_', '291', '##08', '##5', '_', '151', '##49', '##81', '##47', '##23', '##53', '##07', '##5', '.', 'jpg', '一', '年', '的', '贴', '子', '，', '再', '次', '被', '网', '友', '顶', '起', '来', '后', '，', '才', '发', '现', '施', '某', '几', '天', '前', '回', '复', '网', '友', '的', '处', '理', '结', '果', '竟', '如', '下', '图', '：', '=', '750', ')', 'window', '.', 'open', '(', \"'\", 'http', ':', '/', '/', 'img', '.', 'js', '##ly', '##001', '.', 'com', '/', 'at', '##ta', '##ch', '##ment', '/', 'mon', '_', '180', '##1', '/', '4', '_', '291', '##08', '##5', '_', '9', '##d', '##32', '##ee', '##57', '##27', '##60', '##d', '##85', '.', 'jpg', '?', '131', \"'\", ')', ';', '\"', 'on', '##load', '=', '\"', 'if', '(', 'this', '.', 'off', '##set', '##wi', '##dt', '##h', '>', \"'\", '750', \"'\", ')', 'this', '.', 'wi', '##dt', '##h', '=', \"'\", '750', \"'\", ';', '\"', 'sr', '##c', '=', '\"', 'http', ':', '/', '/', 'img', '.', 'js', '##ly', '##001', '.', 'com', '/', 'at', '##ta', '##ch', '##ment', '/', 'mon', '_', '180', '##1', '/', '4', '_', '291', '##08', '##5', '_', '9', '##d', '##32', '##ee', '##57', '##27', '##60', '##d', '##85', '.', 'jpg', '?', '131', '\"', 'style', '=', '\"', 'max', '-', 'wi', '##dt', '##h', ':', '750', '##px', ';', '\"', '/', '>', '图', '片', ':', '/', 'home', '/', 'al', '##ida', '##ta', '/', 'www', '/', 'data', '/', 'tm', '##p', '/', 'q', '##fu', '##pl', '##oa', '##d', '/', '4', '_', '291', '##08', '##5', '_', '151', '##49', '##81', '##47', '##35', '##47', '##17', '##2', '.', 'jpg', '现', '责', '问', '张', '涛', '书', '记', '：', '1', '、', '宣', '国', '才', '被', '举', '报', '这', '么', '多', '问', '题', '，', '什', '么', '时', '候', '有', '答', '复', '。', '2', '、', '宣', '国', '才', '被', '举', '报', '后', '，', '为', '什', '么', '被', '立', '刻', '免', '了', '村', '书', '记', '职', '务', '？', '为', '什', '么', '又', '被', '安', '排', '到', '城', '管', '队', '[UNK]', '吃', '空', '响', '[UNK]', '，', '自', '己', '却', '天', '天', '在', '我', '们', '水', '泥', '厂', '上', '班', '赚', '黑', '钱', '？', '3', '、', '这', '几', '个', '月', '，', '水', '泥', '每', '吨', '近', '200', '元', '纯', '利', '润', '，', '还', '供', '不', '应', '求', '，', '宣', '国', '才', '还', '清', '上', '黄', '政', '府', '担', '保', '借', '给', '宣', '国', '才', '代', '付', '振', '东', '厂', '工', '资', '社', '保', '的', '钱', '了', '吗', '？', '4', '、', '据', '了', '解', '宣', '国', '才', '占', '他', '人', '企', '业', '经', '营', '，', '又', '欠', '税', '52', '.', '16', '万', '元', '、', '欠', '社', '保', '32', '.', '76', '万', '元', '、', '应', '该', '还', '欠', '了', '职', '工', '工', '资', '几', '十', '万', '，', '上', '黄', '政', '府', '打', '算', '替', '宣', '国', '才', '担', '保', '还', '是', '归', '还', '？', '5', '、', '我', '们', '厂', '合', '法', '会', '计', '和', '老', '板', '被', '判', '刑', '四', '到', '六', '年', '，', '现', '在', '服', '刑', '。', '厂', '子', '给', '宣', '国', '才', '强', '占', '，', '宣', '国', '才', '每', '天', '赚', '20', '多', '万', '净', '利', '润', '，', '却', '对', '外', '宣', '称', '天', '天', '亏', '本', '！', '等', '咱', '老', '板', '刑', '满', '回', '厂', '，', '宣', '国', '才', '给', '咱', '厂', '[UNK]', '天', '天', '亏', '[UNK]', '可', '能', '要', '[UNK]', '亏', '[UNK]', '的', '几', '千', '万', '元', '，', '甚', '至', '几', '个', '亿', '，', '张', '涛', '书', '记', '您', '承', '担', '还', '是', '上', '黄', '政', '府', '承', '担', '？', '当', '初', '可', '是', '您', '亲', '自', '把', '厂', '交', '给', '宣', '国', '才', '生', '产', '的', '！', '希', '望', '徐', '市', '长', '看', '到', '本', '贴', '后', '能', '像', '批', '示', '263', '、', '批', '示', '违', '建', '等', '民', '生', '问', '题', '一', '样', '，', '关', '注', '一', '下', '我', '们', '水', '泥', '厂', '的', '将', '来', '！', '也', '请', '徐', '市', '长', '抽', '日', '理', '万', '机', '之', '空', '亲', '自', '约', '谈', '一', '下', '当', '事', '人', '（', '特', '别', '是', '那', '位', '施', '站', '长', '）', '，', '千', '万', '不', '能', '听', '取', '一', '面', '之', '辞', '！']\n",
      "11/11/2019 15:51:54 - INFO - __main__ -   *** Example ***\n",
      "11/11/2019 15:51:54 - INFO - __main__ -   idx: 0\n",
      "11/11/2019 15:51:54 - INFO - __main__ -   guid: 7a3dd79f90ee419da87190cff60f7a86\n",
      "11/11/2019 15:51:54 - INFO - __main__ -   tokens: [CLS] 问 责 领 导 ( 上 黄 镇 党 委 书 记 张 涛 ， 宣 国 才 真 能 一 手 遮 天 吗 ？ ) [SEP] ##57 ##27 ##60 ##d ##85 . jpg ? 131 \" style = \" max - wi ##dt ##h : 750 ##px ; \" / > 图 片 : / home / al ##ida ##ta / www / data / tm ##p / q ##fu ##pl ##oa ##d / 4 _ 291 ##08 ##5 _ 151 ##49 ##81 ##47 ##35 ##47 ##17 ##2 . jpg 现 责 问 张 涛 书 记 ： 1 、 宣 国 才 被 举 报 这 么 多 问 题 ， 什 么 时 候 有 答 复 。 2 、 宣 国 才 被 举 报 后 ， 为 什 么 被 立 刻 免 了 村 书 记 职 务 ？ 为 什 么 又 被 安 排 到 城 管 队 [UNK] 吃 空 响 [UNK] ， 自 己 却 天 天 在 我 们 水 泥 厂 上 班 赚 黑 钱 ？ 3 、 这 几 个 月 ， 水 泥 每 吨 近 200 元 纯 利 润 ， 还 供 不 应 求 ， 宣 国 才 还 清 上 黄 政 府 担 保 借 给 宣 国 才 代 付 振 东 厂 工 资 社 保 的 钱 了 吗 ？ 4 、 据 了 解 宣 国 才 占 他 人 企 业 经 营 ， 又 欠 税 52 . 16 万 元 、 欠 社 保 32 . 76 万 元 、 应 该 还 欠 了 职 工 工 资 几 十 万 ， 上 黄 政 府 打 算 替 宣 国 才 担 保 还 是 归 还 ？ 5 、 我 们 厂 合 法 会 计 和 老 板 被 判 刑 四 到 六 年 ， 现 在 服 刑 。 厂 子 给 宣 国 才 强 占 ， 宣 国 才 每 天 赚 20 多 万 净 利 润 ， 却 对 外 宣 称 天 天 亏 本 ！ 等 咱 老 板 刑 满 回 厂 ， 宣 国 才 给 咱 厂 [UNK] 天 天 亏 [UNK] 可 能 要 [UNK] 亏 [UNK] 的 几 千 万 元 ， 甚 至 几 个 亿 ， 张 涛 书 记 您 承 担 还 是 上 黄 政 府 承 担 ？ 当 初 可 是 您 亲 自 把 厂 交 给 宣 国 才 生 产 的 ！ 希 望 徐 市 长 看 到 本 贴 后 能 像 批 示 263 、 批 示 违 建 等 民 生 问 题 一 样 ， 关 注 一 下 我 们 水 泥 厂 的 将 来 ！ 也 请 徐 市 长 抽 日 理 万 机 之 空 亲 自 约 谈 一 下 当 事 人 （ 特 别 是 那 位 施 站 长 ） ， 千 万 不 能 听 取 一 面 之 辞 [SEP]\n",
      "11/11/2019 15:51:54 - INFO - __main__ -   input_ids: 101 7309 6569 7566 2193 113 677 7942 7252 1054 1999 741 6381 2476 3875 8024 2146 1744 2798 4696 5543 671 2797 6902 1921 1408 8043 114 102 9647 8976 8581 8168 9169 119 9248 136 9403 107 8969 134 107 8621 118 8541 12672 8199 131 9180 10605 132 107 120 135 1745 4275 131 120 8563 120 9266 12708 8383 120 8173 120 9000 120 9908 8187 120 159 12043 12569 11355 8168 120 125 142 11777 9153 8157 142 9564 9500 9313 9050 8852 9050 8408 8144 119 9248 4385 6569 7309 2476 3875 741 6381 8038 122 510 2146 1744 2798 6158 715 2845 6821 720 1914 7309 7579 8024 784 720 3198 952 3300 5031 1908 511 123 510 2146 1744 2798 6158 715 2845 1400 8024 711 784 720 6158 4989 1174 1048 749 3333 741 6381 5466 1218 8043 711 784 720 1348 6158 2128 2961 1168 1814 5052 7339 100 1391 4958 1510 100 8024 5632 2346 1316 1921 1921 1762 2769 812 3717 3799 1322 677 4408 6611 7946 7178 8043 124 510 6821 1126 702 3299 8024 3717 3799 3680 1417 6818 8185 1039 5283 1164 3883 8024 6820 897 679 2418 3724 8024 2146 1744 2798 6820 3926 677 7942 3124 2424 2857 924 955 5314 2146 1744 2798 807 802 2920 691 1322 2339 6598 4852 924 4638 7178 749 1408 8043 125 510 2945 749 6237 2146 1744 2798 1304 800 782 821 689 5307 5852 8024 1348 3612 4925 8247 119 8121 674 1039 510 3612 4852 924 8211 119 8399 674 1039 510 2418 6421 6820 3612 749 5466 2339 2339 6598 1126 1282 674 8024 677 7942 3124 2424 2802 5050 3296 2146 1744 2798 2857 924 6820 3221 2495 6820 8043 126 510 2769 812 1322 1394 3791 833 6369 1469 5439 3352 6158 1161 1152 1724 1168 1063 2399 8024 4385 1762 3302 1152 511 1322 2094 5314 2146 1744 2798 2487 1304 8024 2146 1744 2798 3680 1921 6611 8113 1914 674 1112 1164 3883 8024 1316 2190 1912 2146 4917 1921 1921 755 3315 8013 5023 1493 5439 3352 1152 4007 1726 1322 8024 2146 1744 2798 5314 1493 1322 100 1921 1921 755 100 1377 5543 6206 100 755 100 4638 1126 1283 674 1039 8024 4493 5635 1126 702 783 8024 2476 3875 741 6381 2644 2824 2857 6820 3221 677 7942 3124 2424 2824 2857 8043 2496 1159 1377 3221 2644 779 5632 2828 1322 769 5314 2146 1744 2798 4495 772 4638 8013 2361 3307 2528 2356 7270 4692 1168 3315 6585 1400 5543 1008 2821 4850 10864 510 2821 4850 6824 2456 5023 3696 4495 7309 7579 671 3416 8024 1068 3800 671 678 2769 812 3717 3799 1322 4638 2199 3341 8013 738 6435 2528 2356 7270 2853 3189 4415 674 3322 722 4958 779 5632 5276 6448 671 678 2496 752 782 8020 4294 1166 3221 6929 855 3177 4991 7270 8021 8024 1283 674 679 5543 1420 1357 671 7481 722 6791 102\n",
      "11/11/2019 15:51:54 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/11/2019 15:51:54 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/11/2019 15:51:54 - INFO - __main__ -   label: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11/2019 15:52:44 - INFO - __main__ -   ***** Running training *****\n",
      "11/11/2019 15:52:44 - INFO - __main__ -     Num examples = 11757\n",
      "11/11/2019 15:52:44 - INFO - __main__ -     Batch size = 4\n",
      "11/11/2019 15:52:44 - INFO - __main__ -     Num steps = 30000\n",
      "  0%|                                                 | 0/30000 [00:00<?, ?it/s]11/11/2019 15:52:56 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 15:52:56 - INFO - __main__ -     Num examples = 2939\n",
      "11/11/2019 15:52:56 - INFO - __main__ -     Batch size = 48\n",
      "/home/haizhi/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "11/11/2019 15:55:51 - INFO - __main__ -     eval_F1 = 0.20579092145306976\n",
      "11/11/2019 15:55:51 - INFO - __main__ -     eval_loss = 1.3422253535639854\n",
      "11/11/2019 15:55:51 - INFO - __main__ -     global_step = 0\n",
      "================================================================================\n",
      "Best F1 0.20579092145306976\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.6973:   3%|▋                       | 799/30000 [06:31<2:13:23,  3.65it/s]11/11/2019 15:59:16 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 15:59:16 - INFO - __main__ -     global_step = 200\n",
      "11/11/2019 15:59:16 - INFO - __main__ -     train loss = 0.6973\n",
      "loss 0.5281:   5%|█▏                     | 1599/30000 [09:46<2:11:10,  3.61it/s]11/11/2019 16:02:31 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 16:02:31 - INFO - __main__ -     global_step = 400\n",
      "11/11/2019 16:02:31 - INFO - __main__ -     train loss = 0.5281\n",
      "loss 0.4422:   8%|█▊                     | 2399/30000 [13:00<2:06:50,  3.63it/s]11/11/2019 16:05:45 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 16:05:45 - INFO - __main__ -     global_step = 600\n",
      "11/11/2019 16:05:45 - INFO - __main__ -     train loss = 0.4422\n",
      "loss 0.441:  11%|██▌                     | 3199/30000 [16:14<1:57:36,  3.80it/s]11/11/2019 16:08:58 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 16:08:58 - INFO - __main__ -     global_step = 800\n",
      "11/11/2019 16:08:58 - INFO - __main__ -     train loss = 0.441\n",
      "loss 0.4525:  13%|███                    | 3999/30000 [19:28<1:52:24,  3.86it/s]11/11/2019 16:12:12 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 16:12:12 - INFO - __main__ -     global_step = 1000\n",
      "11/11/2019 16:12:12 - INFO - __main__ -     train loss = 0.4525\n",
      "loss 0.4216:  16%|███▋                   | 4799/30000 [22:43<1:55:34,  3.63it/s]11/11/2019 16:15:27 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 16:15:27 - INFO - __main__ -     global_step = 1200\n",
      "11/11/2019 16:15:27 - INFO - __main__ -     train loss = 0.4216\n",
      "loss 0.4152:  19%|████▎                  | 5599/30000 [25:58<1:49:32,  3.71it/s]11/11/2019 16:18:43 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 16:18:43 - INFO - __main__ -     global_step = 1400\n",
      "11/11/2019 16:18:43 - INFO - __main__ -     train loss = 0.4152\n",
      "loss 0.3567:  21%|████▉                  | 6399/30000 [29:13<1:47:11,  3.67it/s]11/11/2019 16:21:57 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 16:21:57 - INFO - __main__ -     global_step = 1600\n",
      "11/11/2019 16:21:57 - INFO - __main__ -     train loss = 0.3567\n",
      "loss 0.362:  24%|█████▊                  | 7199/30000 [32:28<1:45:07,  3.61it/s]11/11/2019 16:25:12 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 16:25:12 - INFO - __main__ -     global_step = 1800\n",
      "11/11/2019 16:25:12 - INFO - __main__ -     train loss = 0.362\n",
      "loss 0.3405:  27%|██████▏                | 7999/30000 [35:43<1:36:13,  3.81it/s]11/11/2019 16:28:28 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 16:28:28 - INFO - __main__ -     global_step = 2000\n",
      "11/11/2019 16:28:28 - INFO - __main__ -     train loss = 0.3405\n",
      "loss 0.3865:  29%|██████▋                | 8799/30000 [38:59<1:33:53,  3.76it/s]11/11/2019 16:31:43 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 16:31:43 - INFO - __main__ -     global_step = 2200\n",
      "11/11/2019 16:31:43 - INFO - __main__ -     train loss = 0.3865\n",
      "loss 0.3802:  32%|███████▎               | 9599/30000 [42:14<1:35:14,  3.57it/s]11/11/2019 16:34:58 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 16:34:58 - INFO - __main__ -     global_step = 2400\n",
      "11/11/2019 16:34:58 - INFO - __main__ -     train loss = 0.3802\n",
      "loss 0.3929:  35%|███████▋              | 10399/30000 [45:30<1:26:43,  3.77it/s]11/11/2019 16:38:14 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 16:38:14 - INFO - __main__ -     global_step = 2600\n",
      "11/11/2019 16:38:14 - INFO - __main__ -     train loss = 0.3929\n",
      "loss 0.3954:  37%|████████▏             | 11199/30000 [48:46<1:28:02,  3.56it/s]11/11/2019 16:41:31 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 16:41:31 - INFO - __main__ -     global_step = 2800\n",
      "11/11/2019 16:41:31 - INFO - __main__ -     train loss = 0.3954\n",
      "11/11/2019 16:41:43 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 16:41:43 - INFO - __main__ -     Num examples = 2939\n",
      "11/11/2019 16:41:43 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 16:44:43 - INFO - __main__ -     eval_F1 = 0.7664293224383277\n",
      "11/11/2019 16:44:43 - INFO - __main__ -     eval_loss = 0.3960273754933188\n",
      "11/11/2019 16:44:43 - INFO - __main__ -     global_step = 2800\n",
      "11/11/2019 16:44:43 - INFO - __main__ -     loss = 0.3954\n",
      "================================================================================\n",
      "Best F1 0.7664293224383277\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.3346:  40%|████████▊             | 11999/30000 [55:27<1:24:25,  3.55it/s]11/11/2019 16:48:12 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 16:48:12 - INFO - __main__ -     global_step = 3000\n",
      "11/11/2019 16:48:12 - INFO - __main__ -     train loss = 0.3346\n",
      "11/11/2019 16:48:24 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 16:48:24 - INFO - __main__ -     Num examples = 2939\n",
      "11/11/2019 16:48:24 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 16:51:25 - INFO - __main__ -     eval_F1 = 0.7523342178877961\n",
      "11/11/2019 16:51:25 - INFO - __main__ -     eval_loss = 0.3843336976583927\n",
      "11/11/2019 16:51:25 - INFO - __main__ -     global_step = 3000\n",
      "11/11/2019 16:51:25 - INFO - __main__ -     loss = 0.3346\n",
      "================================================================================\n",
      "loss 0.295:  43%|████████▉            | 12799/30000 [1:01:59<1:20:27,  3.56it/s]11/11/2019 16:54:43 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 16:54:43 - INFO - __main__ -     global_step = 3200\n",
      "11/11/2019 16:54:43 - INFO - __main__ -     train loss = 0.295\n",
      "11/11/2019 16:54:56 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 16:54:56 - INFO - __main__ -     Num examples = 2939\n",
      "11/11/2019 16:54:56 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 16:57:56 - INFO - __main__ -     eval_F1 = 0.7855887269243707\n",
      "11/11/2019 16:57:56 - INFO - __main__ -     eval_loss = 0.40157084200050563\n",
      "11/11/2019 16:57:56 - INFO - __main__ -     global_step = 3200\n",
      "11/11/2019 16:57:56 - INFO - __main__ -     loss = 0.295\n",
      "================================================================================\n",
      "Best F1 0.7855887269243707\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.2535:  45%|█████████           | 13599/30000 [1:08:41<1:16:55,  3.55it/s]11/11/2019 17:01:25 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 17:01:25 - INFO - __main__ -     global_step = 3400\n",
      "11/11/2019 17:01:25 - INFO - __main__ -     train loss = 0.2535\n",
      "11/11/2019 17:01:38 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 17:01:38 - INFO - __main__ -     Num examples = 2939\n",
      "11/11/2019 17:01:38 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 17:04:38 - INFO - __main__ -     eval_F1 = 0.7631121467665999\n",
      "11/11/2019 17:04:38 - INFO - __main__ -     eval_loss = 0.4333425998807915\n",
      "11/11/2019 17:04:38 - INFO - __main__ -     global_step = 3400\n",
      "11/11/2019 17:04:38 - INFO - __main__ -     loss = 0.2535\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.2328:  48%|█████████▌          | 14399/30000 [1:15:13<1:10:00,  3.71it/s]11/11/2019 17:07:57 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 17:07:57 - INFO - __main__ -     global_step = 3600\n",
      "11/11/2019 17:07:57 - INFO - __main__ -     train loss = 0.2328\n",
      "11/11/2019 17:08:10 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 17:08:10 - INFO - __main__ -     Num examples = 2939\n",
      "11/11/2019 17:08:10 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 17:11:10 - INFO - __main__ -     eval_F1 = 0.7970378391501516\n",
      "11/11/2019 17:11:10 - INFO - __main__ -     eval_loss = 0.38664928240881813\n",
      "11/11/2019 17:11:10 - INFO - __main__ -     global_step = 3600\n",
      "11/11/2019 17:11:10 - INFO - __main__ -     loss = 0.2328\n",
      "================================================================================\n",
      "Best F1 0.7970378391501516\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.2119:  51%|██████████▏         | 15199/30000 [1:21:55<1:04:27,  3.83it/s]11/11/2019 17:14:39 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 17:14:39 - INFO - __main__ -     global_step = 3800\n",
      "11/11/2019 17:14:39 - INFO - __main__ -     train loss = 0.2119\n",
      "11/11/2019 17:14:52 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 17:14:52 - INFO - __main__ -     Num examples = 2939\n",
      "11/11/2019 17:14:52 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 17:17:52 - INFO - __main__ -     eval_F1 = 0.7878136797371004\n",
      "11/11/2019 17:17:52 - INFO - __main__ -     eval_loss = 0.43513489280256534\n",
      "11/11/2019 17:17:52 - INFO - __main__ -     global_step = 3800\n",
      "11/11/2019 17:17:52 - INFO - __main__ -     loss = 0.2119\n",
      "================================================================================\n",
      "loss 0.2634:  53%|██████████▋         | 15999/30000 [1:28:25<1:01:00,  3.83it/s]11/11/2019 17:21:09 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 17:21:09 - INFO - __main__ -     global_step = 4000\n",
      "11/11/2019 17:21:09 - INFO - __main__ -     train loss = 0.2634\n",
      "11/11/2019 17:21:22 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 17:21:22 - INFO - __main__ -     Num examples = 2939\n",
      "11/11/2019 17:21:22 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 17:24:21 - INFO - __main__ -     eval_F1 = 0.7964599887394183\n",
      "11/11/2019 17:24:21 - INFO - __main__ -     eval_loss = 0.3835286348156871\n",
      "11/11/2019 17:24:21 - INFO - __main__ -     global_step = 4000\n",
      "11/11/2019 17:24:21 - INFO - __main__ -     loss = 0.2634\n",
      "================================================================================\n",
      "loss 0.2131:  56%|████████████▎         | 16799/30000 [1:34:54<57:26,  3.83it/s]11/11/2019 17:27:38 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 17:27:38 - INFO - __main__ -     global_step = 4200\n",
      "11/11/2019 17:27:38 - INFO - __main__ -     train loss = 0.2131\n",
      "11/11/2019 17:27:51 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 17:27:51 - INFO - __main__ -     Num examples = 2939\n",
      "11/11/2019 17:27:51 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 17:30:51 - INFO - __main__ -     eval_F1 = 0.7942267534656474\n",
      "11/11/2019 17:30:51 - INFO - __main__ -     eval_loss = 0.3834451640986146\n",
      "11/11/2019 17:30:51 - INFO - __main__ -     global_step = 4200\n",
      "11/11/2019 17:30:51 - INFO - __main__ -     loss = 0.2131\n",
      "================================================================================\n",
      "loss 0.1822:  59%|████████████▉         | 17599/30000 [1:41:23<55:40,  3.71it/s]11/11/2019 17:34:07 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 17:34:07 - INFO - __main__ -     global_step = 4400\n",
      "11/11/2019 17:34:07 - INFO - __main__ -     train loss = 0.1822\n",
      "11/11/2019 17:34:20 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 17:34:20 - INFO - __main__ -     Num examples = 2939\n",
      "11/11/2019 17:34:20 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 17:37:19 - INFO - __main__ -     eval_F1 = 0.7952762409712616\n",
      "11/11/2019 17:37:19 - INFO - __main__ -     eval_loss = 0.4081322405667555\n",
      "11/11/2019 17:37:19 - INFO - __main__ -     global_step = 4400\n",
      "11/11/2019 17:37:19 - INFO - __main__ -     loss = 0.1822\n",
      "================================================================================\n",
      "loss 0.191:  61%|██████████████         | 18399/30000 [1:47:52<54:32,  3.55it/s]11/11/2019 17:40:37 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 17:40:37 - INFO - __main__ -     global_step = 4600\n",
      "11/11/2019 17:40:37 - INFO - __main__ -     train loss = 0.191\n",
      "11/11/2019 17:40:49 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 17:40:49 - INFO - __main__ -     Num examples = 2939\n",
      "11/11/2019 17:40:49 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 17:43:49 - INFO - __main__ -     eval_F1 = 0.7977385132791927\n",
      "11/11/2019 17:43:49 - INFO - __main__ -     eval_loss = 0.3894470489073184\n",
      "11/11/2019 17:43:49 - INFO - __main__ -     global_step = 4600\n",
      "11/11/2019 17:43:49 - INFO - __main__ -     loss = 0.191\n",
      "================================================================================\n",
      "Best F1 0.7977385132791927\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.1815:  64%|██████████████        | 19199/30000 [1:54:32<47:45,  3.77it/s]11/11/2019 17:47:17 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 17:47:17 - INFO - __main__ -     global_step = 4800\n",
      "11/11/2019 17:47:17 - INFO - __main__ -     train loss = 0.1815\n",
      "11/11/2019 17:47:29 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 17:47:29 - INFO - __main__ -     Num examples = 2939\n",
      "11/11/2019 17:47:29 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 17:50:30 - INFO - __main__ -     eval_F1 = 0.7936286341695817\n",
      "11/11/2019 17:50:30 - INFO - __main__ -     eval_loss = 0.42776869922395677\n",
      "11/11/2019 17:50:30 - INFO - __main__ -     global_step = 4800\n",
      "11/11/2019 17:50:30 - INFO - __main__ -     loss = 0.1815\n",
      "================================================================================\n",
      "loss 0.1773:  67%|██████████████▋       | 19999/30000 [2:01:03<46:52,  3.56it/s]11/11/2019 17:53:47 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 17:53:47 - INFO - __main__ -     global_step = 5000\n",
      "11/11/2019 17:53:47 - INFO - __main__ -     train loss = 0.1773\n",
      "11/11/2019 17:54:00 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 17:54:00 - INFO - __main__ -     Num examples = 2939\n",
      "11/11/2019 17:54:00 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 17:57:00 - INFO - __main__ -     eval_F1 = 0.7948812945237331\n",
      "11/11/2019 17:57:00 - INFO - __main__ -     eval_loss = 0.41563005610218934\n",
      "11/11/2019 17:57:00 - INFO - __main__ -     global_step = 5000\n",
      "11/11/2019 17:57:00 - INFO - __main__ -     loss = 0.1773\n",
      "================================================================================\n",
      "loss 0.1878:  69%|███████████████▎      | 20799/30000 [2:07:35<41:14,  3.72it/s]11/11/2019 18:00:19 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 18:00:19 - INFO - __main__ -     global_step = 5200\n",
      "11/11/2019 18:00:19 - INFO - __main__ -     train loss = 0.1878\n",
      "11/11/2019 18:00:32 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 18:00:32 - INFO - __main__ -     Num examples = 2939\n",
      "11/11/2019 18:00:32 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 18:03:32 - INFO - __main__ -     eval_F1 = 0.7902498472318231\n",
      "11/11/2019 18:03:32 - INFO - __main__ -     eval_loss = 0.4094010649188872\n",
      "11/11/2019 18:03:32 - INFO - __main__ -     global_step = 5200\n",
      "11/11/2019 18:03:32 - INFO - __main__ -     loss = 0.1878\n",
      "================================================================================\n",
      "loss 0.2105:  72%|███████████████▊      | 21599/30000 [2:14:07<36:43,  3.81it/s]11/11/2019 18:06:51 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 18:06:51 - INFO - __main__ -     global_step = 5400\n",
      "11/11/2019 18:06:51 - INFO - __main__ -     train loss = 0.2105\n",
      "11/11/2019 18:07:04 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 18:07:04 - INFO - __main__ -     Num examples = 2939\n",
      "11/11/2019 18:07:04 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 18:10:04 - INFO - __main__ -     eval_F1 = 0.7889196640425745\n",
      "11/11/2019 18:10:04 - INFO - __main__ -     eval_loss = 0.38455786775316925\n",
      "11/11/2019 18:10:04 - INFO - __main__ -     global_step = 5400\n",
      "11/11/2019 18:10:04 - INFO - __main__ -     loss = 0.2105\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.2103:  75%|████████████████▍     | 22399/30000 [2:20:38<34:18,  3.69it/s]11/11/2019 18:13:23 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 18:13:23 - INFO - __main__ -     global_step = 5600\n",
      "11/11/2019 18:13:23 - INFO - __main__ -     train loss = 0.2103\n",
      "11/11/2019 18:13:36 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 18:13:36 - INFO - __main__ -     Num examples = 2939\n",
      "11/11/2019 18:13:36 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 18:16:36 - INFO - __main__ -     eval_F1 = 0.7896313920629199\n",
      "11/11/2019 18:16:36 - INFO - __main__ -     eval_loss = 0.378362484696892\n",
      "11/11/2019 18:16:36 - INFO - __main__ -     global_step = 5600\n",
      "11/11/2019 18:16:36 - INFO - __main__ -     loss = 0.2103\n",
      "================================================================================\n",
      "loss 0.2277:  77%|█████████████████     | 23199/30000 [2:27:10<31:56,  3.55it/s]11/11/2019 18:19:54 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 18:19:54 - INFO - __main__ -     global_step = 5800\n",
      "11/11/2019 18:19:54 - INFO - __main__ -     train loss = 0.2277\n",
      "11/11/2019 18:20:07 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 18:20:07 - INFO - __main__ -     Num examples = 2939\n",
      "11/11/2019 18:20:07 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 18:23:06 - INFO - __main__ -     eval_F1 = 0.7826079774620256\n",
      "11/11/2019 18:23:06 - INFO - __main__ -     eval_loss = 0.39153652704290803\n",
      "11/11/2019 18:23:06 - INFO - __main__ -     global_step = 5800\n",
      "11/11/2019 18:23:06 - INFO - __main__ -     loss = 0.2277\n",
      "================================================================================\n",
      "loss 0.1728:  80%|█████████████████▌    | 23999/30000 [2:33:39<28:07,  3.56it/s]11/11/2019 18:26:24 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 18:26:24 - INFO - __main__ -     global_step = 6000\n",
      "11/11/2019 18:26:24 - INFO - __main__ -     train loss = 0.1728\n",
      "11/11/2019 18:26:36 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 18:26:36 - INFO - __main__ -     Num examples = 2939\n",
      "11/11/2019 18:26:36 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 18:29:36 - INFO - __main__ -     eval_F1 = 0.7968169289236352\n",
      "11/11/2019 18:29:36 - INFO - __main__ -     eval_loss = 0.4078985435978299\n",
      "11/11/2019 18:29:36 - INFO - __main__ -     global_step = 6000\n",
      "11/11/2019 18:29:36 - INFO - __main__ -     loss = 0.1728\n",
      "================================================================================\n",
      "loss 0.1546:  83%|██████████████████▏   | 24799/30000 [2:40:08<22:41,  3.82it/s]11/11/2019 18:32:52 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 18:32:52 - INFO - __main__ -     global_step = 6200\n",
      "11/11/2019 18:32:52 - INFO - __main__ -     train loss = 0.1546\n",
      "11/11/2019 18:33:05 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 18:33:05 - INFO - __main__ -     Num examples = 2939\n",
      "11/11/2019 18:33:05 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 18:36:05 - INFO - __main__ -     eval_F1 = 0.791512754874882\n",
      "11/11/2019 18:36:05 - INFO - __main__ -     eval_loss = 0.42001100589010504\n",
      "11/11/2019 18:36:05 - INFO - __main__ -     global_step = 6200\n",
      "11/11/2019 18:36:05 - INFO - __main__ -     loss = 0.1546\n",
      "================================================================================\n",
      "loss 0.1127:  85%|██████████████████▊   | 25599/30000 [2:46:37<19:18,  3.80it/s]11/11/2019 18:39:22 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 18:39:22 - INFO - __main__ -     global_step = 6400\n",
      "11/11/2019 18:39:22 - INFO - __main__ -     train loss = 0.1127\n",
      "11/11/2019 18:39:34 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 18:39:34 - INFO - __main__ -     Num examples = 2939\n",
      "11/11/2019 18:39:34 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 18:42:34 - INFO - __main__ -     eval_F1 = 0.7955517704820901\n",
      "11/11/2019 18:42:34 - INFO - __main__ -     eval_loss = 0.41131269066564496\n",
      "11/11/2019 18:42:34 - INFO - __main__ -     global_step = 6400\n",
      "11/11/2019 18:42:34 - INFO - __main__ -     loss = 0.1127\n",
      "================================================================================\n",
      "loss 0.112:  88%|████████████████████▏  | 26399/30000 [2:53:07<15:43,  3.82it/s]11/11/2019 18:45:51 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 18:45:51 - INFO - __main__ -     global_step = 6600\n",
      "11/11/2019 18:45:51 - INFO - __main__ -     train loss = 0.112\n",
      "11/11/2019 18:46:04 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 18:46:04 - INFO - __main__ -     Num examples = 2939\n",
      "11/11/2019 18:46:04 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 18:49:04 - INFO - __main__ -     eval_F1 = 0.8004042964694523\n",
      "11/11/2019 18:49:04 - INFO - __main__ -     eval_loss = 0.4329715314050836\n",
      "11/11/2019 18:49:04 - INFO - __main__ -     global_step = 6600\n",
      "11/11/2019 18:49:04 - INFO - __main__ -     loss = 0.112\n",
      "================================================================================\n",
      "Best F1 0.8004042964694523\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.1218:  91%|███████████████████▉  | 27199/30000 [2:59:47<13:07,  3.56it/s]11/11/2019 18:52:31 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 18:52:31 - INFO - __main__ -     global_step = 6800\n",
      "11/11/2019 18:52:31 - INFO - __main__ -     train loss = 0.1218\n",
      "11/11/2019 18:52:44 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 18:52:44 - INFO - __main__ -     Num examples = 2939\n",
      "11/11/2019 18:52:44 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 18:55:44 - INFO - __main__ -     eval_F1 = 0.8030355307253009\n",
      "11/11/2019 18:55:44 - INFO - __main__ -     eval_loss = 0.4185651078700058\n",
      "11/11/2019 18:55:44 - INFO - __main__ -     global_step = 6800\n",
      "11/11/2019 18:55:44 - INFO - __main__ -     loss = 0.1218\n",
      "================================================================================\n",
      "Best F1 0.8030355307253009\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.1048:  93%|████████████████████▌ | 27999/30000 [3:06:28<08:55,  3.74it/s]11/11/2019 18:59:13 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 18:59:13 - INFO - __main__ -     global_step = 7000\n",
      "11/11/2019 18:59:13 - INFO - __main__ -     train loss = 0.1048\n",
      "11/11/2019 18:59:26 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 18:59:26 - INFO - __main__ -     Num examples = 2939\n",
      "11/11/2019 18:59:26 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 19:02:26 - INFO - __main__ -     eval_F1 = 0.8047640702086638\n",
      "11/11/2019 19:02:26 - INFO - __main__ -     eval_loss = 0.43283481240993543\n",
      "11/11/2019 19:02:26 - INFO - __main__ -     global_step = 7000\n",
      "11/11/2019 19:02:26 - INFO - __main__ -     loss = 0.1048\n",
      "================================================================================\n",
      "Best F1 0.8047640702086638\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.0839:  96%|█████████████████████ | 28799/30000 [3:13:10<05:31,  3.62it/s]11/11/2019 19:05:55 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 19:05:55 - INFO - __main__ -     global_step = 7200\n",
      "11/11/2019 19:05:55 - INFO - __main__ -     train loss = 0.0839\n",
      "11/11/2019 19:06:08 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 19:06:08 - INFO - __main__ -     Num examples = 2939\n",
      "11/11/2019 19:06:08 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 19:09:08 - INFO - __main__ -     eval_F1 = 0.8012623787245684\n",
      "11/11/2019 19:09:08 - INFO - __main__ -     eval_loss = 0.43581807868735445\n",
      "11/11/2019 19:09:08 - INFO - __main__ -     global_step = 7200\n",
      "11/11/2019 19:09:08 - INFO - __main__ -     loss = 0.0839\n",
      "================================================================================\n",
      "loss 0.0801:  99%|█████████████████████▋| 29599/30000 [3:19:42<01:49,  3.65it/s]11/11/2019 19:12:27 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 19:12:27 - INFO - __main__ -     global_step = 7400\n",
      "11/11/2019 19:12:27 - INFO - __main__ -     train loss = 0.0801\n",
      "11/11/2019 19:12:40 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 19:12:40 - INFO - __main__ -     Num examples = 2939\n",
      "11/11/2019 19:12:40 - INFO - __main__ -     Batch size = 48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11/2019 19:15:40 - INFO - __main__ -     eval_F1 = 0.8020980267194789\n",
      "11/11/2019 19:15:40 - INFO - __main__ -     eval_loss = 0.4415797358078341\n",
      "11/11/2019 19:15:40 - INFO - __main__ -     global_step = 7400\n",
      "11/11/2019 19:15:40 - INFO - __main__ -     loss = 0.0801\n",
      "================================================================================\n",
      "loss 0.0682: 100%|██████████████████████| 30000/30000 [3:24:35<00:00,  2.44it/s]\n",
      "11/11/2019 19:17:19 - INFO - pytorch_transformers.modeling_utils -   loading weights file ../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_2/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "Traceback (most recent call last):\n",
      "  File \"./run_bert_2562_last2embedding_cls.py\", line 841, in <module>\n",
      "    main()\n",
      "  File \"./run_bert_2562_last2embedding_cls.py\", line 757, in main\n",
      "    logits = model(input_ids=input_ids, token_type_ids=segment_ids, attention_mask=input_mask).detach().cpu().numpy()\n",
      "  File \"/home/haizhi/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/haizhi/workspace/TODO/2019互联网新闻/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 1119, in forward\n",
      "    attention_mask=flat_attention_mask, head_mask=head_mask)\n",
      "  File \"/home/haizhi/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/haizhi/workspace/TODO/2019互联网新闻/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 727, in forward\n",
      "    head_mask=head_mask)\n",
      "  File \"/home/haizhi/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/haizhi/workspace/TODO/2019互联网新闻/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 440, in forward\n",
      "    layer_outputs = layer_module(hidden_states, attention_mask, head_mask[i])\n",
      "  File \"/home/haizhi/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/haizhi/workspace/TODO/2019互联网新闻/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 420, in forward\n",
      "    intermediate_output = self.intermediate(attention_output)\n",
      "  File \"/home/haizhi/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/haizhi/workspace/TODO/2019互联网新闻/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 392, in forward\n",
      "    hidden_states = self.intermediate_act_fn(hidden_states)\n",
      "  File \"/home/haizhi/workspace/TODO/2019互联网新闻/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 142, in gelu\n",
      "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 384.00 MiB (GPU 0; 10.92 GiB total capacity; 9.35 GiB already allocated; 191.00 MiB free; 810.30 MiB cached)\n"
     ]
    }
   ],
   "source": [
    "!python ./run_bert_2562_last2embedding_cls.py \\\n",
    "--model_type bert \\\n",
    "--model_name_or_path ../model/chinese_roberta_wwm_large_ext_pytorch  \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_test \\\n",
    "--data_dir ../data/data_StratifiedKFold_42/data_origin_2 \\\n",
    "--output_dir ../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_2 \\\n",
    "--max_seq_length 512 \\\n",
    "--split_num 1 \\\n",
    "--lstm_hidden_size 512 \\\n",
    "--lstm_layers 1 \\\n",
    "--lstm_dropout 0.1 \\\n",
    "--eval_steps 200 \\\n",
    "--per_gpu_train_batch_size 4 \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--warmup_steps 0 \\\n",
    "--per_gpu_eval_batch_size 48 \\\n",
    "--learning_rate 1e-5 \\\n",
    "--adam_epsilon 1e-6 \\\n",
    "--weight_decay 0 \\\n",
    "--train_steps 30000 \\\n",
    "--freeze 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11/2019 19:17:41 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "11/11/2019 19:17:41 - INFO - pytorch_transformers.tokenization_utils -   Model name '../model/chinese_roberta_wwm_large_ext_pytorch' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming '../model/chinese_roberta_wwm_large_ext_pytorch' is a path or url to a directory containing tokenizer files.\n",
      "11/11/2019 19:17:41 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../model/chinese_roberta_wwm_large_ext_pytorch/added_tokens.json. We won't load it.\n",
      "11/11/2019 19:17:41 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../model/chinese_roberta_wwm_large_ext_pytorch/special_tokens_map.json. We won't load it.\n",
      "11/11/2019 19:17:41 - INFO - pytorch_transformers.tokenization_utils -   loading file ../model/chinese_roberta_wwm_large_ext_pytorch/vocab.txt\n",
      "11/11/2019 19:17:41 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/11/2019 19:17:41 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/11/2019 19:17:41 - INFO - pytorch_transformers.modeling_utils -   loading configuration file ../model/chinese_roberta_wwm_large_ext_pytorch/config.json\n",
      "11/11/2019 19:17:41 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 3,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "11/11/2019 19:17:41 - INFO - pytorch_transformers.modeling_utils -   loading weights file ../model/chinese_roberta_wwm_large_ext_pytorch/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "11/11/2019 19:17:46 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification_last2embedding_cls not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "11/11/2019 19:17:46 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification_last2embedding_cls: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "11/11/2019 19:17:49 - INFO - __main__ -   ** RAW EXAMPLE **\n",
      "11/11/2019 19:17:49 - INFO - __main__ -   content: ['过', '去', '一', '年', '的', '江', '歌', '悲', '剧', '，', '这', '几', '日', '再', '次', '刷', '屏', '：', '住', '在', '东', '京', '都', '中', '野', '区', '的', '中', '国', '女', '留', '学', '生', '江', '歌', '，', '收', '留', '了', '被', '前', '男', '友', '陈', '世', '锋', '恶', '意', '纠', '缠', '的', '闺', '蜜', '刘', '鑫', '，', '两', '人', '在', '回', '到', '江', '歌', '公', '寓', '楼', '时', '，', '陈', '世', '锋', '已', '经', '等', '在', '楼', '下', '，', '叫', '嚣', '着', '要', '刘', '鑫', '给', '自', '己', '一', '个', '说', '法', '（', '男', '友', '此', '时', '的', '情', '绪', '处', '于', '濒', '临', '崩', '溃', '的', '状', '态', '）', '。', '江', '歌', '为', '了', '保', '护', '刘', '鑫', '，', '就', '让', '她', '先', '进', '了', '房', '间', '，', '自', '己', '拦', '在', '外', '面', '要', '求', '陈', '世', '锋', '离', '开', '。', '结', '果', '江', '歌', '被', '陈', '世', '锋', '用', '刀', '多', '处', '刺', '伤', '脖', '子', '和', '胸', '部', '，', '刀', '刀', '毙', '命', '，', '残', '忍', '至', '极', '，', '最', '终', '因', '失', '血', '过', '多', '丧', '生', '。', '十', '几', '刀', '，', '刘', '鑫', '躲', '在', '屋', '里', '，', '躲', '在', '门', '后', '，', '亲', '耳', '听', '着', '闺', '蜜', '江', '歌', '的', '声', '声', '求', '助', '及', '惨', '叫', '，', '却', '始', '终', '没', '有', '打', '开', '门', '。', '连', '邻', '居', '都', '听', '到', '呼', '救', '纷', '纷', '开', '门', '查', '看', '究', '竟', '，', '那', '扇', '可', '以', '救', '命', '的', '们', '始', '终', '没', '有', '打', '开', '。', '江', '歌', '死', '后', '，', '刘', '鑫', '面', '对', '警', '方', '的', '询', '问', '，', '称', '自', '己', '一', '无', '所', '知', '，', '什', '么', '都', '没', '有', '听', '见', '，', '不', '肯', '出', '来', '指', '证', '凶', '手', '、', '为', '江', '歌', '伸', '冤', '，', '只', '想', '着', '撇', '清', '关', '系', '、', '澄', '清', '自', '己', '，', '任', '江', '歌', '遭', '外', '界', '议', '论', '指', '摘', '，', '甚', '至', '拒', '绝', '同', '江', '歌', '妈', '妈', '联', '系', '，', '自', '己', '做', '了', '新', '头', '发', '买', '了', '新', '包', '包', '快', '快', '乐', '乐', '地', '过', '着', '自', '己', '的', '生', '活', '，', '暗', '自', '庆', '幸', '自', '己', '终', '于', '摆', '脱', '了', '那', '个', '纠', '缠', '自', '己', '的', '前', '男', '友', '。', '迫', '于', '舆', '论', '的', '压', '力', '，', '刘', '鑫', '终', '于', '出', '现', '了', '。', '江', '歌', '为', '刘', '鑫', '失', '去', '了', '生', '命', '，', '可', '在', '刘', '鑫', '眼', '里', '，', '江', '歌', '的', '一', '条', '命', '抵', '不', '过', '自', '己', '的', '名', '声', '，', '她', '只', '在', '意', '自', '己', '及', '家', '人', '的', '生', '活', '受', '到', '了', '严', '重', '的', '影', '响', '，', '认', '为', '自', '己', '知', '道', '错', '了', '江', '歌', '妈', '妈', '就', '该', '原', '谅', '她', '了', '，', '不', '应', '该', '揪', '着', '她', '不', '放', '。', '甚', '至', '在', '事', '搁', '一', '年', '后', '首', '次', '与', '她', '死', '者', '母', '亲', '见', '面', '，', '还', '身', '穿', '艳', '色', '裤', '子', '，', '不', '摘', '帽', '子', '丝', '毫', '没', '有', '尊', '重', '。', '江', '歌', '遇', '害', '至', '今', '，', '她', '每', '一', '次', '出', '现', '在', '网', '络', '那', '头', '，', '都', '是', '在', '为', '自', '己', '争', '辩', '，', '为', '自', '己', '的', '利', '益', '作', '斗', '争', '。', '如', '果', '说', '，', '江', '歌', '的', '正', '直', '与', '善', '良', '，', '是', '来', '自', '江', '歌', '妈', '妈', '的', '言', '传', '身', '教', '。', '那', '刘', '鑫', '的', '自', '私', '与', '冷', '漠', '呢', '？', '在', '刘', '鑫', '妈', '妈', '与', '江', '歌', '妈', '妈', '的', '争', '执', '中', '可', '见', '一', '斑', '，', '刘', '鑫', '妈', '妈', '说', '：', '[UNK]', '她', '命', '短', '了', '，', '她', '不', '是', '为', '了', '俺', '闺', '女', '！', '[UNK]', '江', '歌', '妈', '妈', '后', '悔', '吗', '？', '一', '定', '后', '悔', '，', '后', '悔', '自', '己', '教', '女', '儿', '这', '样', '善', '良', '，', '让', '她', '付', '出', '了', '生', '命', '的', '代', '价', '去', '为', '别', '人', '的', '过', '错', '买', '单', '。', '曾', '经', '还', '有', '一', '个', '事', '件', '，', '一', '位', '爸', '爸', '带', '着', '自', '己', '的', '孩', '子', '去', '河', '边', '钓', '鱼', '，', '遇', '到', '一', '位', '带', '着', '孩', '子', '玩', '的', '妈', '妈', '。', '虽', '然', '并', '不', '相', '识', '，', '但', '是', '两', '个', '孩', '子', '很', '快', '打', '成', '了', '一', '片', '，', '玩', '得', '不', '亦', '乐', '乎', '。', '谁', '知', '突', '然', '两', '个', '孩', '子', '掉', '进', '了', '河', '里', '。', '这', '位', '爸', '爸', '二', '话', '没', '说', '，', '立', '刻', '下', '河', '去', '救', '孩', '子', '。', '这', '个', '时', '候', '他', '的', '孩', '子', '离', '岸', '边', '近', '，', '另', '一', '个', '孩', '子', '已', '经', '飘', '得', '比', '较', '远', '了', '。', '孩', '子', '妈', '妈', '因', '为', '不', '会', '游', '泳', '，', '一', '直', '在', '岸', '边', '喊', '话', '，', '救', '救', '她', '的', '孩', '子', '。', '这', '位', '爸', '爸', '稍', '一', '犹', '豫', '，', '还', '是', '奋', '力', '向', '着', '自', '己', '的', '孩', '子', '游', '过', '去', '，', '将', '自', '己', '的', '孩', '子', '拖', '到', '岸', '边', '，', '这', '才', '向', '着', '另', '外', '一', '个', '孩', '子', '游', '去', '。', '可', '是', '遗', '憾', '的', '是', '，', '另', '外', '一', '个', '孩', '子', '被', '救', '上', '岸', '，', '却', '已', '停', '止', '了', '呼', '吸', '。', '这', '位', '妈', '妈', '悲', '痛', '欲', '绝', '，', '不', '停', '地', '埋', '怨', '这', '位', '父', '亲', '：', '[UNK]', '我', '的', '孩', '子', '就', '在', '距', '离', '岸', '边', '不', '远', '处', '，', '你', '一', '伸', '手', '就', '可', '以', '抓', '得', '到', '，', '为', '什', '么', '你', '要', '游', '得', '那', '么', '远', '先', '救', '自', '己', '的', '孩', '子', '？', '你', '完', '全', '可', '以', '救', '起', '我', '的', '孩', '子', '再', '救', '你', '的', '孩', '子', '，', '你', '为', '什', '么', '这', '么', '没', '人', '性', '，', '为', '什', '么', '为', '什', '么', '[UNK]', '[UNK]', '[UNK]', '这', '是', '人', '的', '本', '能', '，', '看', '到', '自', '己', '的', '孩', '子', '落', '水', '，', '作', '为', '父', '母', '肯', '定', '会', '不', '顾', '一', '切', '地', '去', '救', '他', '，', '更', '何', '况', '是', '他', '自', '己', '的', '孩', '子', '离', '得', '近', '，', '他', '先', '救', '自', '己', '的', '孩', '子', '于', '情', '于', '理', '都', '说', '得', '过', '去', '。', '不', '说', '别', '的', '，', '世', '界', '上', '的', '人', '都', '是', '自', '私', '的', '，', '对', '于', '自', '己', '的', '孩', '子', '，', '更', '是', '如', '此', '，', '如', '果', '这', '位', '父', '亲', '不', '去', '救', '自', '己', '儿', '子', '，', '而', '是', '救', '了', '互', '不', '相', '识', '的', '小', '孩', '，', '万', '一', '孩', '子', '也', '遭', '遇', '了', '不', '幸', '，', '那', '么', '，', '他', '的', '家', '庭', '呢', '？', '他', '后', '半', '辈', '子', '呢', '？', '谁', '来', '为', '他', '破', '碎', '的', '家', '庭', '负', '责', '？', '孩', '子', '，', '舍', '己', '为', '人', '是', '很', '伟', '大', '，', '可', '是', '你', '的', '生', '命', '同', '样', '宝', '贵', '，', '同', '样', '值', '得', '珍', '惜', '。', '我', '们', '不', '能', '容', '许', '你', '为', '了', '别', '的', '任', '何', '人', '，', '去', '做', '以', '身', '犯', '险', '的', '事', '情', '，', '付', '出', '生', '命', '的', '代', '价', '去', '承', '担', '别', '人', '的', '过', '错', '或', '失', '误', '。', '告', '诉', '孩', '子', '，', '善', '良', '的', '前', '提', '是', '懂', '得', '保', '护', '自', '己', '！', '在', '这', '四', '种', '情', '况', '下', '，', '收', '起', '你', '的', '善', '良', '1', '.', '当', '你', '弱', '势', '而', 'ta', '强', '势', '时', '有', '一', '天', '，', '有', '位', '妈', '妈', '去', '学', '校', '接', '孩', '子', '迟', '到', '了', '，', '一', '个', '人', '贩', '子', '抓', '住', '机', '会', '，', '趁', '机', '哄', '骗', '她', '的', '孩', '子', '。', '骗', '子', '说', '：', '[UNK]', '哎', '呀', '，', '小', '朋', '友', '，', '我', '东', '西', '丢', '在', '厕', '所', '了', '，', '现', '在', '你', '能', '不', '能', '帮', '我', '去', '找', '一', '下', '。', '[UNK]', '她', '是', '想', '把', '孩', '子', '引', '到', '厕', '所', '里', '，', '好', '让', '同', '伴', '下', '手', '。', '结', '果', '她', '连', '续', '问', '了', '三', '遍', '，', '这', '个', '小', '孩', '子', '一', '直', '摇', '头', '，', '她', '恼', '了', '，', '你', '这', '孩', '子', '，', '没', '学', '过', '什', '么', '叫', '助', '人', '为', '乐', '吗', '？', '小', '孩', '瞪', '了', '他', '一', '眼', '，', '跑', '到', '老', '师', '身', '边', '去', '了', '，', '人', '贩', '子', '看', '这', '个', '孩', '子', '机', '警', '，', '马', '上', '转', '换', '目', '标', '。', '没', '到', '十', '分', '钟', '，', '他', '们', '骗', '到', '另', '一', '个', '小', '孩', '儿', '，', '正', '打', '算', '开', '车', '走', '人', '的', '时', '候', '，', '被', '警', '察', '抓', '了', '个', '正', '着', '。', '原', '来', '这', '个', '小', '孩', '子', '跑', '到', '老', '师', '身', '边', '，', '很', '笃', '定', '地', '跟', '老', '师', '说', '，', '这', '几', '个', '人', '是', '坏', '人', '。', '老', '师', '半', '信', '半', '疑', '，', '但', '是', '为', '了', '安', '全', '着', '想', '，', '还', '是', '报', '了', '警', '。', '结', '果', '没', '想', '到', '，', '他', '们', '真', '的', '是', '坏', '人', '。', '老', '师', '就', '问', '这', '个', '小', '孩', '，', '你', '怎', '么', '知', '道', '他', '们', '是', '坏', '人', '的', '。', '小', '孩', '子', '说', '：', '[UNK]', '妈', '妈', '说', '过', '，', '如', '果', '有', '大', '人', '找', '你', '帮', '忙', '，', '千', '万', '不', '要', '理', '他', '，', '因', '为', '如', '果', '大', '人', '遇', '到', '解', '决', '不', '了', '的', '困', '难', '，', '一', '定', '会', '寻', '求', '成', '人', '帮', '忙', '，', '而', '不', '是', '比', '他', '还', '弱', '小', '的', '孩', '子', '。', '[UNK]', '成', '年', '男', '子', '一', '般', '不', '会', '找', '孕', '妇', '帮', '忙', '，', '健', '壮', '的', '成', '年', '人', '一', '般', '不', '会', '让', '老', '人', '帮', '忙', '，', '大', '人', '一', '般', '不', '会', '找', '孩', '子', '帮', '忙', '，', '寻', '求', '帮', '助', '一', '定', '是', '因', '为', '你', '比', '他', '强', '，', '所', '以', '他', '才', '找', '你', '帮', '忙', '。', '如', '果', '你', '本', '就', '是', '弱', '势', '，', '而', '强', '势', '的', '一', '方', '反', '而', '要', '你', '帮', '忙', '，', '那', '么', '说', '明', '他', '一', '定', '另', '有', '所', '图', '！', '这', '时', '候', '，', '请', '务', '必', '收', '起', '你', '的', '善', '良', '！', '2', '.', '避', '免', '进', '入', '封', '闭', '环', '境', '四', '年', '前', '，', '佳', '木', '斯', '市', '桦', '南', '县', '，', '曾', '发', '生', '一', '起', '惨', '案', '。', '一', '名', '刚', '刚', '17', '岁', '的', '年', '轻', '女', '护', '士', '，', '在', '路', '上', '看', '到', '一', '名', '孕', '妇', '跌', '倒', '，', '急', '忙', '上', '前', '搀', '扶', '，', '并', '将', '孕', '妇', '送', '回', '家', '中', '。', '当', '天', '下', '午', '3', '点', '15', '分', '，', '女', '护', '士', '给', '朋', '友', '发', '来', '微', '信', '：', '送', '一', '名', '孕', '妇', '阿', '姨', '，', '到', '她', '家', '了', '[UNK]', '[UNK]', '让', '所', '有', '人', '都', '没', '想', '到', '的', '是', '，', '这', '条', '充', '满', '爱', '心', '的', '留', '言', '，', '成', '为', '善', '良', '女', '孩', '留', '在', '人', '世', '间', '最', '后', '的', '信', '息', '。', '善', '良', '的', '女', '孩', '失', '踪', '，', '警', '方', '四', '处', '侦', '查', '，', '不', '久', '捕', '获', '邪', '恶', '的', '孕', '妇', '谭', '某', '[UNK]', '[UNK]', '网', '络', '上', '的', '新', '闻', '称', '，', '此', '女', '为', '讨', '好', '丈', '夫', '，', '假', '装', '在', '街', '头', '跌', '倒', '，', '引', '诱', '善', '良', '女', '孩', '，', '拐', '到', '家', '里', '让', '丈', '夫', '伤', '害', '。', '毫', '无', '防', '范', '的', '女', '孩', '，', '送', '其', '至', '出', '租', '屋', '后', '，', '被', '孕', '妇', '谭', '某', '以', '一', '瓶', '掺', '了', '安', '眠', '药', '的', '酸', '奶', '迷', '昏', '，', '而', '后', '将', '善', '良', '的', '姑', '娘', '杀', '害', '。', '有', '天', '下', '雨', '，', '有', '个', '孕', '妇', '见', '门', '口', '有', '个', '乞', '丐', '在', '淋', '雨', '，', '于', '心', '不', '忍', '，', '于', '是', '请', '他', '进', '来', '歇', '歇', '脚', '。', '乞', '丐', '感', '恩', '戴', '德', '地', '进', '了', '她', '的', '家', '，', '在', '换', '了', '干', '净', '的', '衣', '服', '，', '吃', '饱', '饭', '之', '后', '，', '他', '发', '现', '只', '有', '孕', '妇', '一', '个', '人', '在', '家', '，', '于', '是', '威', '胁', '孕', '妇', '给', '他', '一', '笔', '生', '活', '费', '。', '在', '这', '样', '封', '闭', '的', '环', '境', '下', '，', '只', '有', '他', '们', '两', '人', '在', '，', '她', '失', '去', '了', '讨', '价', '还', '价', '的', '余', '地', '，', '为', '了', '避', '免', '遭', '受', '更', '大', '的', '伤', '害', '，', '破', '财', '消', '灾', '，', '付', '了', '一', '万', '块', '钱', '给', '乞', '丐', '。', '当', '一', '个', '场', '合', '，', '从', '开', '放', '变', '成', '封', '闭', '，', '缺', '少', '了', '大', '家', '的', '监', '督', '，', '很', '难', '想', '象', '，', '人', '到', '底', '会', '做', '出', '什', '么', '事', '情', '。', '在', '人', '前', '可', '能', '是', '谦', '谦', '君', '子', '，', '但', '是', '到', '了', '人', '后', '，', '明', '显', '你', '弱', '人', '强', '的', '封', '闭', '环', '境', '中', '，', '你', '只', '能', '予', '取', '予', '求', '。', '所', '以', '，', '助', '人', '为', '乐', '可', '以', '，', '但', '是', '尽', '量', '避', '免', '在', '封', '闭', '的', '空', '间', '里', '。', '这', '时', '候', '，', '请', '务', '必', '收', '起', '你', '的', '善', '良', '！', '3', '.', '有', '时', '候', '善', '良', '也', '会', '坏', '事', '有', '个', '广', '为', '人', '知', '的', '故', '事', '。', '游', '客', '在', '禁', '猎', '区', '保', '护', '组', '织', '的', '护', '送', '下', '，', '来', '到', '了', '神', '秘', '的', '可', '可', '西', '里', '。', '一', '只', '可', '爱', '的', '小', '藏', '羚', '羊', '跑', '了', '来', '，', '好', '奇', '的', '探', '头', '探', '脑', '。', '可', '爱', '的', '小', '生', '灵', '，', '游', '客', '们', '顿', '时', '骚', '动', '起', '来', '，', '纷', '纷', '上', '前', '拍', '照', '，', '拿', '出', '食', '物', '和', '饮', '水', '，', '要', '喂', '这', '只', '可', '爱', '的', '小', '动', '物', '。', '突', '然', '之', '间', '，', '一', '声', '怒', '吼', '响', '起', '：', '[UNK]', '滚', '开', '！', '[UNK]', '禁', '猎', '区', '的', '保', '护', '队', '长', '冲', '过', '来', '，', '打', '跑', '可', '爱', '的', '小', '藏', '羚', '羊', '，', '不', '许', '游', '客', '们', '喂', '食', '。', '游', '客', '怒', '了', '：', '[UNK]', '你', '这', '是', '干', '什', '么', '？', '怎', '么', '可', '以', '如', '此', '粗', '暴', '的', '对', '待', '小', '动', '物', '。', '我', '们', '爱', '护', '这', '些', '小', '动', '物', '，', '又', '有', '什', '么', '不', '对', '？', '[UNK]', '[UNK]', '你', '们', '这', '是', '在', '造', '孽', '！', '[UNK]', '保', '护', '队', '长', '喝', '斥', '道', '：', '[UNK]', '如', '果', '你', '们', '对', '待', '野', '生', '动', '物', '太', '友', '善', '，', '它', '们', '就', '会', '以', '为', '人', '类', '都', '是', '善', '良', '的', '，', '一', '旦', '遭', '遇', '盗', '猎', '者', '，', '就', '有', '可', '能', '惨', '遭', '猎', '杀', '。', '[UNK]', '游', '客', '们', '惊', '呆', '了', '。', '[UNK]', '[UNK]', '你', '的', '善', '良', '，', '必', '须', '要', '经', '得', '起', '人', '心', '的', '复', '杂', '！', '看', '起', '来', '是', '善', '良', '的', '事', '情', '，', '却', '可', '能', '因', '为', '人', '的', '复', '杂', '贪', '婪', '而', '变', '成', '坏', '事', '，', '行', '善', '务', '必', '要', '考', '察', '清', '楚', '，', '你', '的', '善', '良', '到', '底', '会', '造', '成', '什', '么', '样', '的', '后', '果', '。', '这', '时', '候', '，', '请', '务', '必', '收', '起', '你', '的', '善', '良', '！', '4', '.', '升', '米', '恩', '，', '斗', '米', '仇', '从', '前', '，', '有', '两', '户', '人', '家', '是', '邻', '居', '，', '平', '时', '关', '系', '还', '不', '错', '。', '其', '中', '一', '家', '人', '因', '为', '能', '干', '些', '，', '家', '中', '要', '富', '裕', '的', '多', '。', '这', '两', '家', '本', '来', '没', '有', '什', '么', '恩', '怨', '的', '，', '可', '是', '，', '这', '一', '年', '，', '老', '天', '爷', '发', '怒', '，', '降', '下', '了', '灾', '祸', '，', '田', '中', '颗', '粒', '无', '收', '。', '这', '穷', '的', '一', '家', '没', '有', '了', '收', '成', '，', '只', '好', '躺', '着', '等', '死', '。', '这', '个', '时', '候', '，', '富', '的', '一', '家', '买', '到', '了', '很', '多', '粮', '食', '，', '想', '着', '大', '家', '邻', '居', '的', '，', '就', '给', '穷', '的', '一', '家', '送', '去', '了', '一', '升', '米', '，', '救', '了', '急', '。', '这', '穷', '的', '一', '家', '非', '常', '感', '激', '富', '人', '，', '认', '为', '这', '真', '是', '救', '命', '的', '恩', '人', '呀', '！', '熬', '过', '最', '艰', '苦', '的', '时', '刻', '后', '，', '穷', '人', '就', '前', '往', '感', '谢', '富', '人', '。', '说', '话', '间', '，', '谈', '起', '明', '年', '的', '种', '子', '还', '没', '有', '着', '落', '，', '富', '的', '一', '家', '慷', '慨', '地', '说', '：', '这', '样', '吧', '，', '我', '这', '里', '的', '粮', '食', '还', '有', '很', '多', '，', '你', '就', '再', '拿', '去', '一', '斗', '吧', '。', '这', '穷', '的', '千', '恩', '万', '谢', '地', '拿', '着', '一', '斗', '米', '回', '家', '了', '。', '回', '家', '后', '，', '他', '的', '兄', '弟', '说', '了', '，', '这', '斗', '米', '能', '做', '什', '么', '？', '除', '了', '吃', '以', '外', '，', '根', '本', '就', '不', '够', '我', '们', '明', '年', '地', '里', '的', '种', '子', '，', '这', '个', '富', '人', '太', '过', '分', '了', '，', '既', '然', '你', '这', '么', '有', '钱', '，', '就', '应', '该', '多', '送', '我', '们', '一', '些', '粮', '食', '和', '钱', '，', '才', '给', '这', '么', '一', '点', '，', '真', '是', '坏', '的', '很', '。', '这', '话', '传', '到', '了', '富', '人', '耳', '朵', '里', '，', '他', '很', '生', '气', '，', '心', '想', '，', '我', '白', '白', '送', '你', '这', '么', '多', '的', '粮', '食', '，', '你', '不', '仅', '不', '感', '谢', '我', '，', '还', '把', '我', '当', '仇', '人', '一', '样', '忌', '恨', '，', '真', '不', '是', '人', '。', '于', '是', '，', '本', '来', '关', '系', '不', '错', '的', '两', '家', '人', '。', '从', '此', '就', '成', '了', '仇', '人', '，', '老', '死', '不', '相', '往', '来', '。', '一', '个', '人', '饥', '寒', '交', '迫', '的', '时', '候', '，', '你', '给', '他', '一', '碗', '米', '，', '就', '是', '解', '决', '了', '他', '的', '大', '问', '题', '，', '他', '会', '感', '恩', '不', '尽', '。', '但', '是', '，', '你', '如', '果', '继', '续', '给', '他', '米', '，', '他', '就', '会', '觉', '得', '理', '所', '当', '然', '了', '。', '一', '碗', '米', '不', '够', '，', '两', '碗', '米', '不', '够', '，', '三', '碗', '四', '碗', '还', '是', '觉', '得', '你', '只', '给', '了', '沧', '海', '一', '粟', '。', '生', '活', '里', '常', '有', '这', '样', '的', '事', '，', '第', '一', '次', '为', '一', '个', '人', '提', '供', '帮', '助', '时', '，', '他', '会', '对', '你', '心', '存', '感', '激', '，', '第', '二', '次', '他', '的', '感', '恩', '心', '理', '就', '会', '淡', '化', '，', '到', '了', 'n', '次', '以', '后', '，', '他', '简', '直', '就', '理', '直', '气', '壮', '地', '认', '为', '这', '都', '是', '你', '应', '该', '为', '他', '做', '的', '，', '甚', '至', '当', '没', '有', '了', '这', '种', '帮', '助', '时', '，', '他', '会', '对', '施', '助', '者', '心', '存', '怨', '恨', '。', '所', '以', '，', '人', '的', '善', '良', '一', '定', '要', '有', '度', '！', '当', '一', '个', '人', '不', '思', '进', '取', '，', '一', '味', '索', '取', '帮', '助', '时', '，', '请', '及', '时', '收', '起', '你', '的', '善', '良', '！', '这', '时', '候', '，', '请', '务', '必', '收', '起', '你', '的', '善', '良', '！', '我', '们', '不', '愿', '意', '世', '间', '满', '是', '冷', '漠', '之', '人', '，', '我', '们', '也', '不', '愿', '意', '教', '育', '孩', '子', '成', '为', '利', '己', '主', '义', '者', '，', '但', '是', '世', '间', '险', '恶', '，', '有', '些', '时', '候', '选', '择', '自', '保', '无', '可', '厚', '非', '。', '对', '于', '孩', '子', '来', '说', '，', '他', '们', '还', '不', '能', '很', '好', '地', '判', '断', '事', '情', '的', '真', '假', '，', '作', '为', '父', '母', '，', '我', '们', '要', '教', '会', '孩', '子', '：', '善', '良', '没', '有', '错', '，', '但', '是', '要', '有', '底', '线', '，', '人', '性', '复', '杂', '，', '教', '会', '孩', '子', '懂', '得', '先', '保', '护', '自', '己', '，', '才', '能', '考', '虑', '善', '良', '以', '及', '其', '他', '。', '成', '功', '的', '父', '母', '每', '时', '每', '刻', '都', '会', '注', '重', '优', '质', '信', '息', '的', '分', '享', '，', '把', '好', '的', '亲', '子', '教', '育', '文', '章', '分', '享', '给', '身', '边', '的', '朋', '友', '，', '也', '许', '就', '是', '这', '样', '一', '个', '简', '单', '的', '动', '作', '，', '数', '亿', '的', '孩', '子', '会', '更', '加', '健', '康', '、', '快', '乐', '的', '成', '长', '！', '-', 'end', '-', '下', '面', '的', '内', '容', '你', '可', '能', '会', '喜', '欢', '↓↓↓', '（', '公', '众', '号', '回', '复', '[UNK]', '爱', '娃', '[UNK]', '即', '可', '查', '看', '文', '章', '）', '育', '儿', '福', '利', '（', '公', '益', '父', '母', '课', '堂', '！', '育', '儿', '问', '题', '解', '答', '！', '儿', '童', '能', '力', '测', '评', '）', '育', '儿', '干', '货', '精', '选', '（', '如', '：', '如', '何', '夸', '孩', '子', '？', '如', '何', '培', '养', '阅', '读', '？', '如', '何', '过', '渡', '幼', '儿', '园', '？', '）', '父', '母', '成', '长', '记', '（', '如', '：', '犹', '太', '人', '的', '家', '教', '！', '经', '验', '说', 'vs', '科', '学', '说', '！', '家', '庭', '教', '育', '如', '何', '培', '养', '出', '成', '功', '的', '孩', '子', '？', '）', '万', '千', '妈', '妈', '推', '荐', '（', '如', '何', '进', '行', '优', '质', '早', '教', '？', '如', '何', '选', '择', '合', '适', '孩', '子', '的', '早', '教', '课', '程', '？', '早', '教', '精', '品', '课', '程', '推', '荐', '！', '）', '最', '牛', '早', '教', '↓', '卓', '越', '七', '田', '中', '国', '首', '家', '效', '果', '可', '衡', '量', '的', '早', '教', '机', '构', '，', '深', '圳', '东', '莞', '十', '多', '家', '分', '校', '，', '为', '近', '百', '家', '早', '教', '机', '构', '提', '供', '师', '资', '培', '训', '！', '专', '注', '0', '-', '9', '岁', '孩', '子', '学', '习', '潜', '能', '、', '性', '格', '潜', '能', '的', '科', '学', '育', '儿', '方', '案', '。', '您', '身', '边', '的', '早', '教', '专', '家', '邀', '您', '一', '起', '科', '学', '育', '儿', '育', '儿', '亲', '子', '教', '育', '↓', '深', '圳', '早', '教', '专', '家', '觉', '得', '文', '章', '还', '不', '错', '，', '分', '享', '给', '需', '要', '的', '朋', '友', '吧', '！', '点', '击', '阅', '读', '原', '文', '↓↓↓', '，', '免', '费', '获', '取', '价', '值', '480', '元', '儿', '童', '生', '理', '发', '育', '测', '评']\n",
      "11/11/2019 19:17:49 - INFO - __main__ -   *** Example ***\n",
      "11/11/2019 19:17:49 - INFO - __main__ -   idx: 0\n",
      "11/11/2019 19:17:49 - INFO - __main__ -   guid: 7640a5589bc7486ca199eeeb38af79dd\n",
      "11/11/2019 19:17:49 - INFO - __main__ -   tokens: [CLS] 江 歌 事 件 : 教 会 孩 子 ， 善 良 的 同 时 更 要 懂 得 保 护 自 己 ! [SEP] 利 己 主 义 者 ， 但 是 世 间 险 恶 ， 有 些 时 候 选 择 自 保 无 可 厚 非 。 对 于 孩 子 来 说 ， 他 们 还 不 能 很 好 地 判 断 事 情 的 真 假 ， 作 为 父 母 ， 我 们 要 教 会 孩 子 ： 善 良 没 有 错 ， 但 是 要 有 底 线 ， 人 性 复 杂 ， 教 会 孩 子 懂 得 先 保 护 自 己 ， 才 能 考 虑 善 良 以 及 其 他 。 成 功 的 父 母 每 时 每 刻 都 会 注 重 优 质 信 息 的 分 享 ， 把 好 的 亲 子 教 育 文 章 分 享 给 身 边 的 朋 友 ， 也 许 就 是 这 样 一 个 简 单 的 动 作 ， 数 亿 的 孩 子 会 更 加 健 康 、 快 乐 的 成 长 ！ - end - 下 面 的 内 容 你 可 能 会 喜 欢 ↓↓↓ （ 公 众 号 回 复 [UNK] 爱 娃 [UNK] 即 可 查 看 文 章 ） 育 儿 福 利 （ 公 益 父 母 课 堂 ！ 育 儿 问 题 解 答 ！ 儿 童 能 力 测 评 ） 育 儿 干 货 精 选 （ 如 ： 如 何 夸 孩 子 ？ 如 何 培 养 阅 读 ？ 如 何 过 渡 幼 儿 园 ？ ） 父 母 成 长 记 （ 如 ： 犹 太 人 的 家 教 ！ 经 验 说 vs 科 学 说 ！ 家 庭 教 育 如 何 培 养 出 成 功 的 孩 子 ？ ） 万 千 妈 妈 推 荐 （ 如 何 进 行 优 质 早 教 ？ 如 何 选 择 合 适 孩 子 的 早 教 课 程 ？ 早 教 精 品 课 程 推 荐 ！ ） 最 牛 早 教 ↓ 卓 越 七 田 中 国 首 家 效 果 可 衡 量 的 早 教 机 构 ， 深 圳 东 莞 十 多 家 分 校 ， 为 近 百 家 早 教 机 构 提 供 师 资 培 训 ！ 专 注 0 - 9 岁 孩 子 学 习 潜 能 、 性 格 潜 能 的 科 学 育 儿 方 案 。 您 身 边 的 早 教 专 家 邀 您 一 起 科 学 育 儿 育 儿 亲 子 教 育 ↓ 深 圳 早 教 专 家 觉 得 文 章 还 不 错 ， 分 享 给 需 要 的 朋 友 吧 ！ 点 击 阅 读 原 文 ↓↓↓ ， 免 费 获 取 价 值 480 元 儿 童 生 理 发 育 测 [SEP]\n",
      "11/11/2019 19:17:49 - INFO - __main__ -   input_ids: 101 3736 3625 752 816 131 3136 833 2111 2094 8024 1587 5679 4638 1398 3198 3291 6206 2743 2533 924 2844 5632 2346 106 102 1164 2346 712 721 5442 8024 852 3221 686 7313 7372 2626 8024 3300 763 3198 952 6848 2885 5632 924 3187 1377 1331 7478 511 2190 754 2111 2094 3341 6432 8024 800 812 6820 679 5543 2523 1962 1765 1161 3171 752 2658 4638 4696 969 8024 868 711 4266 3678 8024 2769 812 6206 3136 833 2111 2094 8038 1587 5679 3766 3300 7231 8024 852 3221 6206 3300 2419 5296 8024 782 2595 1908 3325 8024 3136 833 2111 2094 2743 2533 1044 924 2844 5632 2346 8024 2798 5543 5440 5991 1587 5679 809 1350 1071 800 511 2768 1216 4638 4266 3678 3680 3198 3680 1174 6963 833 3800 7028 831 6574 928 2622 4638 1146 775 8024 2828 1962 4638 779 2094 3136 5509 3152 4995 1146 775 5314 6716 6804 4638 3301 1351 8024 738 6387 2218 3221 6821 3416 671 702 5042 1296 4638 1220 868 8024 3144 783 4638 2111 2094 833 3291 1217 978 2434 510 2571 727 4638 2768 7270 8013 118 9931 118 678 7481 4638 1079 2159 872 1377 5543 833 1599 3614 9010 8020 1062 830 1384 1726 1908 100 4263 2015 100 1315 1377 3389 4692 3152 4995 8021 5509 1036 4886 1164 8020 1062 4660 4266 3678 6440 1828 8013 5509 1036 7309 7579 6237 5031 8013 1036 4997 5543 1213 3844 6397 8021 5509 1036 2397 6573 5125 6848 8020 1963 8038 1963 862 1930 2111 2094 8043 1963 862 1824 1075 7325 6438 8043 1963 862 6814 3941 2405 1036 1736 8043 8021 4266 3678 2768 7270 6381 8020 1963 8038 4310 1922 782 4638 2157 3136 8013 5307 7741 6432 8349 4906 2110 6432 8013 2157 2431 3136 5509 1963 862 1824 1075 1139 2768 1216 4638 2111 2094 8043 8021 674 1283 1968 1968 2972 5773 8020 1963 862 6822 6121 831 6574 3193 3136 8043 1963 862 6848 2885 1394 6844 2111 2094 4638 3193 3136 6440 4923 8043 3193 3136 5125 1501 6440 4923 2972 5773 8013 8021 3297 4281 3193 3136 371 1294 6632 673 4506 704 1744 7674 2157 3126 3362 1377 6130 7030 4638 3193 3136 3322 3354 8024 3918 1766 691 5806 1282 1914 2157 1146 3413 8024 711 6818 4636 2157 3193 3136 3322 3354 2990 897 2360 6598 1824 6378 8013 683 3800 121 118 130 2259 2111 2094 2110 739 4052 5543 510 2595 3419 4052 5543 4638 4906 2110 5509 1036 3175 3428 511 2644 6716 6804 4638 3193 3136 683 2157 6913 2644 671 6629 4906 2110 5509 1036 5509 1036 779 2094 3136 5509 371 3918 1766 3193 3136 683 2157 6230 2533 3152 4995 6820 679 7231 8024 1146 775 5314 7444 6206 4638 3301 1351 1416 8013 4157 1140 7325 6438 1333 3152 9010 8024 1048 6589 5815 1357 817 966 9482 1039 1036 4997 4495 4415 1355 5509 3844 102\n",
      "11/11/2019 19:17:49 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/11/2019 19:17:49 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/11/2019 19:17:49 - INFO - __main__ -   label: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11/2019 19:18:38 - INFO - __main__ -   ***** Running training *****\n",
      "11/11/2019 19:18:38 - INFO - __main__ -     Num examples = 11758\n",
      "11/11/2019 19:18:38 - INFO - __main__ -     Batch size = 4\n",
      "11/11/2019 19:18:38 - INFO - __main__ -     Num steps = 30000\n",
      "  0%|                                                 | 0/30000 [00:00<?, ?it/s]11/11/2019 19:18:51 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 19:18:51 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 19:18:51 - INFO - __main__ -     Batch size = 48\n",
      "/home/haizhi/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "11/11/2019 19:21:44 - INFO - __main__ -     eval_F1 = 0.2114200353972265\n",
      "11/11/2019 19:21:44 - INFO - __main__ -     eval_loss = 1.3298926016976755\n",
      "11/11/2019 19:21:44 - INFO - __main__ -     global_step = 0\n",
      "================================================================================\n",
      "Best F1 0.2114200353972265\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.6823:   3%|▋                       | 799/30000 [06:29<2:08:34,  3.79it/s]11/11/2019 19:25:08 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 19:25:08 - INFO - __main__ -     global_step = 200\n",
      "11/11/2019 19:25:08 - INFO - __main__ -     train loss = 0.6823\n",
      "loss 0.4849:   5%|█▏                     | 1599/30000 [09:42<2:08:20,  3.69it/s]11/11/2019 19:28:21 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 19:28:21 - INFO - __main__ -     global_step = 400\n",
      "11/11/2019 19:28:21 - INFO - __main__ -     train loss = 0.4849\n",
      "loss 0.4844:   8%|█▊                     | 2399/30000 [12:56<2:01:17,  3.79it/s]11/11/2019 19:31:35 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 19:31:35 - INFO - __main__ -     global_step = 600\n",
      "11/11/2019 19:31:35 - INFO - __main__ -     train loss = 0.4844\n",
      "loss 0.4543:  11%|██▍                    | 3199/30000 [16:10<2:01:31,  3.68it/s]11/11/2019 19:34:49 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 19:34:49 - INFO - __main__ -     global_step = 800\n",
      "11/11/2019 19:34:49 - INFO - __main__ -     train loss = 0.4543\n",
      "loss 0.4642:  13%|███                    | 3999/30000 [19:24<1:53:56,  3.80it/s]11/11/2019 19:38:03 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 19:38:03 - INFO - __main__ -     global_step = 1000\n",
      "11/11/2019 19:38:03 - INFO - __main__ -     train loss = 0.4642\n",
      "loss 0.4082:  16%|███▋                   | 4799/30000 [22:38<1:56:13,  3.61it/s]11/11/2019 19:41:17 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 19:41:17 - INFO - __main__ -     global_step = 1200\n",
      "11/11/2019 19:41:17 - INFO - __main__ -     train loss = 0.4082\n",
      "loss 0.3965:  19%|████▎                  | 5599/30000 [25:53<1:48:21,  3.75it/s]11/11/2019 19:44:32 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 19:44:32 - INFO - __main__ -     global_step = 1400\n",
      "11/11/2019 19:44:32 - INFO - __main__ -     train loss = 0.3965\n",
      "loss 0.3823:  21%|████▉                  | 6399/30000 [29:08<1:46:25,  3.70it/s]11/11/2019 19:47:47 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 19:47:47 - INFO - __main__ -     global_step = 1600\n",
      "11/11/2019 19:47:47 - INFO - __main__ -     train loss = 0.3823\n",
      "loss 0.4315:  24%|█████▌                 | 7199/30000 [32:23<1:44:18,  3.64it/s]11/11/2019 19:51:02 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 19:51:02 - INFO - __main__ -     global_step = 1800\n",
      "11/11/2019 19:51:02 - INFO - __main__ -     train loss = 0.4315\n",
      "loss 0.4421:  27%|██████▏                | 7999/30000 [35:38<1:40:19,  3.66it/s]11/11/2019 19:54:17 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 19:54:17 - INFO - __main__ -     global_step = 2000\n",
      "11/11/2019 19:54:17 - INFO - __main__ -     train loss = 0.4421\n",
      "loss 0.33:  29%|███████▎                 | 8799/30000 [38:54<1:34:07,  3.75it/s]11/11/2019 19:57:33 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 19:57:33 - INFO - __main__ -     global_step = 2200\n",
      "11/11/2019 19:57:33 - INFO - __main__ -     train loss = 0.33\n",
      "loss 0.4218:  32%|███████▎               | 9599/30000 [42:11<1:28:57,  3.82it/s]11/11/2019 20:00:50 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 20:00:50 - INFO - __main__ -     global_step = 2400\n",
      "11/11/2019 20:00:50 - INFO - __main__ -     train loss = 0.4218\n",
      "loss 0.4153:  35%|███████▋              | 10399/30000 [45:28<1:31:24,  3.57it/s]11/11/2019 20:04:07 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 20:04:07 - INFO - __main__ -     global_step = 2600\n",
      "11/11/2019 20:04:07 - INFO - __main__ -     train loss = 0.4153\n",
      "loss 0.3437:  37%|████████▏             | 11199/30000 [48:45<1:27:51,  3.57it/s]11/11/2019 20:07:24 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 20:07:24 - INFO - __main__ -     global_step = 2800\n",
      "11/11/2019 20:07:24 - INFO - __main__ -     train loss = 0.3437\n",
      "11/11/2019 20:07:37 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 20:07:37 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 20:07:37 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 20:10:36 - INFO - __main__ -     eval_F1 = 0.7842408658891605\n",
      "11/11/2019 20:10:36 - INFO - __main__ -     eval_loss = 0.33336992207313737\n",
      "11/11/2019 20:10:36 - INFO - __main__ -     global_step = 2800\n",
      "11/11/2019 20:10:36 - INFO - __main__ -     loss = 0.3437\n",
      "================================================================================\n",
      "Best F1 0.7842408658891605\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.3938:  40%|████████▊             | 11999/30000 [55:24<1:18:33,  3.82it/s]11/11/2019 20:14:03 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 20:14:03 - INFO - __main__ -     global_step = 3000\n",
      "11/11/2019 20:14:03 - INFO - __main__ -     train loss = 0.3938\n",
      "11/11/2019 20:14:16 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 20:14:16 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 20:14:16 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 20:17:16 - INFO - __main__ -     eval_F1 = 0.7995737642699683\n",
      "11/11/2019 20:17:16 - INFO - __main__ -     eval_loss = 0.3546514172227152\n",
      "11/11/2019 20:17:16 - INFO - __main__ -     global_step = 3000\n",
      "11/11/2019 20:17:16 - INFO - __main__ -     loss = 0.3938\n",
      "================================================================================\n",
      "Best F1 0.7995737642699683\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.2752:  43%|████████▌           | 12799/30000 [1:02:05<1:15:42,  3.79it/s]11/11/2019 20:20:44 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 20:20:44 - INFO - __main__ -     global_step = 3200\n",
      "11/11/2019 20:20:44 - INFO - __main__ -     train loss = 0.2752\n",
      "11/11/2019 20:20:56 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 20:20:56 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 20:20:56 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 20:23:55 - INFO - __main__ -     eval_F1 = 0.7910543828195983\n",
      "11/11/2019 20:23:55 - INFO - __main__ -     eval_loss = 0.3467661120117672\n",
      "11/11/2019 20:23:55 - INFO - __main__ -     global_step = 3200\n",
      "11/11/2019 20:23:55 - INFO - __main__ -     loss = 0.2752\n",
      "================================================================================\n",
      "loss 0.2481:  45%|█████████           | 13599/30000 [1:08:33<1:13:35,  3.71it/s]11/11/2019 20:27:11 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 20:27:11 - INFO - __main__ -     global_step = 3400\n",
      "11/11/2019 20:27:11 - INFO - __main__ -     train loss = 0.2481\n",
      "11/11/2019 20:27:24 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 20:27:24 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 20:27:24 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 20:30:22 - INFO - __main__ -     eval_F1 = 0.7367762467211413\n",
      "11/11/2019 20:30:22 - INFO - __main__ -     eval_loss = 0.35489410735786925\n",
      "11/11/2019 20:30:22 - INFO - __main__ -     global_step = 3400\n",
      "11/11/2019 20:30:22 - INFO - __main__ -     loss = 0.2481\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.2239:  48%|█████████▌          | 14399/30000 [1:14:59<1:08:34,  3.79it/s]11/11/2019 20:33:38 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 20:33:38 - INFO - __main__ -     global_step = 3600\n",
      "11/11/2019 20:33:38 - INFO - __main__ -     train loss = 0.2239\n",
      "11/11/2019 20:33:51 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 20:33:51 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 20:33:51 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 20:36:49 - INFO - __main__ -     eval_F1 = 0.7942365649185014\n",
      "11/11/2019 20:36:49 - INFO - __main__ -     eval_loss = 0.4006804879754782\n",
      "11/11/2019 20:36:49 - INFO - __main__ -     global_step = 3600\n",
      "11/11/2019 20:36:49 - INFO - __main__ -     loss = 0.2239\n",
      "================================================================================\n",
      "loss 0.2361:  51%|██████████▏         | 15199/30000 [1:21:27<1:07:27,  3.66it/s]11/11/2019 20:40:06 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 20:40:06 - INFO - __main__ -     global_step = 3800\n",
      "11/11/2019 20:40:06 - INFO - __main__ -     train loss = 0.2361\n",
      "11/11/2019 20:40:18 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 20:40:18 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 20:40:18 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 20:43:16 - INFO - __main__ -     eval_F1 = 0.7972536010974961\n",
      "11/11/2019 20:43:16 - INFO - __main__ -     eval_loss = 0.3865452298534013\n",
      "11/11/2019 20:43:16 - INFO - __main__ -     global_step = 3800\n",
      "11/11/2019 20:43:16 - INFO - __main__ -     loss = 0.2361\n",
      "================================================================================\n",
      "loss 0.2236:  53%|██████████▋         | 15999/30000 [1:27:53<1:05:25,  3.57it/s]11/11/2019 20:46:32 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 20:46:32 - INFO - __main__ -     global_step = 4000\n",
      "11/11/2019 20:46:32 - INFO - __main__ -     train loss = 0.2236\n",
      "11/11/2019 20:46:45 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 20:46:45 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 20:46:45 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 20:49:43 - INFO - __main__ -     eval_F1 = 0.8023243421585841\n",
      "11/11/2019 20:49:43 - INFO - __main__ -     eval_loss = 0.3752882768880696\n",
      "11/11/2019 20:49:43 - INFO - __main__ -     global_step = 4000\n",
      "11/11/2019 20:49:43 - INFO - __main__ -     loss = 0.2236\n",
      "================================================================================\n",
      "Best F1 0.8023243421585841\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.2168:  56%|███████████▏        | 16799/30000 [1:34:32<1:00:28,  3.64it/s]11/11/2019 20:53:11 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 20:53:11 - INFO - __main__ -     global_step = 4200\n",
      "11/11/2019 20:53:11 - INFO - __main__ -     train loss = 0.2168\n",
      "11/11/2019 20:53:24 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 20:53:24 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 20:53:24 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 20:56:23 - INFO - __main__ -     eval_F1 = 0.8036809656726035\n",
      "11/11/2019 20:56:23 - INFO - __main__ -     eval_loss = 0.3756269663797631\n",
      "11/11/2019 20:56:23 - INFO - __main__ -     global_step = 4200\n",
      "11/11/2019 20:56:23 - INFO - __main__ -     loss = 0.2168\n",
      "================================================================================\n",
      "Best F1 0.8036809656726035\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.2029:  59%|████████████▉         | 17599/30000 [1:41:12<54:34,  3.79it/s]11/11/2019 20:59:51 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 20:59:51 - INFO - __main__ -     global_step = 4400\n",
      "11/11/2019 20:59:51 - INFO - __main__ -     train loss = 0.2029\n",
      "11/11/2019 21:00:04 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 21:00:04 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 21:00:04 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 21:03:03 - INFO - __main__ -     eval_F1 = 0.802443788509049\n",
      "11/11/2019 21:03:03 - INFO - __main__ -     eval_loss = 0.35212817914303274\n",
      "11/11/2019 21:03:03 - INFO - __main__ -     global_step = 4400\n",
      "11/11/2019 21:03:03 - INFO - __main__ -     loss = 0.2029\n",
      "================================================================================\n",
      "loss 0.1805:  61%|█████████████▍        | 18399/30000 [1:47:42<53:48,  3.59it/s]11/11/2019 21:06:21 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 21:06:21 - INFO - __main__ -     global_step = 4600\n",
      "11/11/2019 21:06:21 - INFO - __main__ -     train loss = 0.1805\n",
      "11/11/2019 21:06:34 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 21:06:34 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 21:06:34 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 21:09:33 - INFO - __main__ -     eval_F1 = 0.7864773864773865\n",
      "11/11/2019 21:09:33 - INFO - __main__ -     eval_loss = 0.3978881460103777\n",
      "11/11/2019 21:09:33 - INFO - __main__ -     global_step = 4600\n",
      "11/11/2019 21:09:33 - INFO - __main__ -     loss = 0.1805\n",
      "================================================================================\n",
      "loss 0.2447:  64%|██████████████        | 19199/30000 [1:54:12<47:24,  3.80it/s]11/11/2019 21:12:51 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 21:12:51 - INFO - __main__ -     global_step = 4800\n",
      "11/11/2019 21:12:51 - INFO - __main__ -     train loss = 0.2447\n",
      "11/11/2019 21:13:04 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 21:13:04 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 21:13:04 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 21:16:03 - INFO - __main__ -     eval_F1 = 0.7951149932743827\n",
      "11/11/2019 21:16:03 - INFO - __main__ -     eval_loss = 0.36558128821273006\n",
      "11/11/2019 21:16:03 - INFO - __main__ -     global_step = 4800\n",
      "11/11/2019 21:16:03 - INFO - __main__ -     loss = 0.2447\n",
      "================================================================================\n",
      "loss 0.222:  67%|███████████████▎       | 19999/30000 [2:00:43<43:55,  3.79it/s]11/11/2019 21:19:22 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 21:19:22 - INFO - __main__ -     global_step = 5000\n",
      "11/11/2019 21:19:22 - INFO - __main__ -     train loss = 0.222\n",
      "11/11/2019 21:19:35 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 21:19:35 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 21:19:35 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 21:22:33 - INFO - __main__ -     eval_F1 = 0.8043564565037339\n",
      "11/11/2019 21:22:33 - INFO - __main__ -     eval_loss = 0.36557629512202355\n",
      "11/11/2019 21:22:33 - INFO - __main__ -     global_step = 5000\n",
      "11/11/2019 21:22:33 - INFO - __main__ -     loss = 0.222\n",
      "================================================================================\n",
      "Best F1 0.8043564565037339\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.1659:  69%|███████████████▎      | 20799/30000 [2:07:21<40:58,  3.74it/s]11/11/2019 21:26:00 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 21:26:00 - INFO - __main__ -     global_step = 5200\n",
      "11/11/2019 21:26:00 - INFO - __main__ -     train loss = 0.1659\n",
      "11/11/2019 21:26:13 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 21:26:13 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 21:26:13 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 21:29:11 - INFO - __main__ -     eval_F1 = 0.7881547977301615\n",
      "11/11/2019 21:29:11 - INFO - __main__ -     eval_loss = 0.40056010161436373\n",
      "11/11/2019 21:29:11 - INFO - __main__ -     global_step = 5200\n",
      "11/11/2019 21:29:11 - INFO - __main__ -     loss = 0.1659\n",
      "================================================================================\n",
      "loss 0.2347:  72%|███████████████▊      | 21599/30000 [2:13:50<38:30,  3.64it/s]11/11/2019 21:32:28 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 21:32:28 - INFO - __main__ -     global_step = 5400\n",
      "11/11/2019 21:32:28 - INFO - __main__ -     train loss = 0.2347\n",
      "11/11/2019 21:32:41 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 21:32:41 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 21:32:41 - INFO - __main__ -     Batch size = 48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11/2019 21:35:39 - INFO - __main__ -     eval_F1 = 0.8032551616656997\n",
      "11/11/2019 21:35:39 - INFO - __main__ -     eval_loss = 0.3801314340483758\n",
      "11/11/2019 21:35:39 - INFO - __main__ -     global_step = 5400\n",
      "11/11/2019 21:35:39 - INFO - __main__ -     loss = 0.2347\n",
      "================================================================================\n",
      "loss 0.241:  75%|█████████████████▏     | 22399/30000 [2:20:17<33:25,  3.79it/s]11/11/2019 21:38:56 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 21:38:56 - INFO - __main__ -     global_step = 5600\n",
      "11/11/2019 21:38:56 - INFO - __main__ -     train loss = 0.241\n",
      "11/11/2019 21:39:09 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 21:39:09 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 21:39:09 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 21:42:07 - INFO - __main__ -     eval_F1 = 0.8014841084441168\n",
      "11/11/2019 21:42:07 - INFO - __main__ -     eval_loss = 0.3485690583024294\n",
      "11/11/2019 21:42:07 - INFO - __main__ -     global_step = 5600\n",
      "11/11/2019 21:42:07 - INFO - __main__ -     loss = 0.241\n",
      "================================================================================\n",
      "loss 0.2045:  77%|█████████████████     | 23199/30000 [2:26:44<31:48,  3.56it/s]11/11/2019 21:45:23 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 21:45:23 - INFO - __main__ -     global_step = 5800\n",
      "11/11/2019 21:45:23 - INFO - __main__ -     train loss = 0.2045\n",
      "11/11/2019 21:45:36 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 21:45:36 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 21:45:36 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 21:48:34 - INFO - __main__ -     eval_F1 = 0.8056914365603444\n",
      "11/11/2019 21:48:34 - INFO - __main__ -     eval_loss = 0.368543200345049\n",
      "11/11/2019 21:48:34 - INFO - __main__ -     global_step = 5800\n",
      "11/11/2019 21:48:34 - INFO - __main__ -     loss = 0.2045\n",
      "================================================================================\n",
      "Best F1 0.8056914365603444\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.19:  80%|███████████████████▏    | 23999/30000 [2:33:22<26:50,  3.73it/s]11/11/2019 21:52:01 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 21:52:01 - INFO - __main__ -     global_step = 6000\n",
      "11/11/2019 21:52:01 - INFO - __main__ -     train loss = 0.19\n",
      "11/11/2019 21:52:14 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 21:52:14 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 21:52:14 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 21:55:12 - INFO - __main__ -     eval_F1 = 0.8052157243699362\n",
      "11/11/2019 21:55:12 - INFO - __main__ -     eval_loss = 0.3629407361149788\n",
      "11/11/2019 21:55:12 - INFO - __main__ -     global_step = 6000\n",
      "11/11/2019 21:55:12 - INFO - __main__ -     loss = 0.19\n",
      "================================================================================\n",
      "loss 0.1514:  83%|██████████████████▏   | 24799/30000 [2:39:51<23:12,  3.74it/s]11/11/2019 21:58:30 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 21:58:30 - INFO - __main__ -     global_step = 6200\n",
      "11/11/2019 21:58:30 - INFO - __main__ -     train loss = 0.1514\n",
      "11/11/2019 21:58:43 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 21:58:43 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 21:58:43 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 22:01:42 - INFO - __main__ -     eval_F1 = 0.7990418642775583\n",
      "11/11/2019 22:01:42 - INFO - __main__ -     eval_loss = 0.3565852584076985\n",
      "11/11/2019 22:01:42 - INFO - __main__ -     global_step = 6200\n",
      "11/11/2019 22:01:42 - INFO - __main__ -     loss = 0.1514\n",
      "================================================================================\n",
      "loss 0.1393:  85%|██████████████████▊   | 25599/30000 [2:46:21<19:22,  3.78it/s]11/11/2019 22:05:00 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 22:05:00 - INFO - __main__ -     global_step = 6400\n",
      "11/11/2019 22:05:00 - INFO - __main__ -     train loss = 0.1393\n",
      "11/11/2019 22:05:13 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 22:05:13 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 22:05:13 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 22:08:11 - INFO - __main__ -     eval_F1 = 0.801515309928722\n",
      "11/11/2019 22:08:11 - INFO - __main__ -     eval_loss = 0.3985163960981393\n",
      "11/11/2019 22:08:11 - INFO - __main__ -     global_step = 6400\n",
      "11/11/2019 22:08:11 - INFO - __main__ -     loss = 0.1393\n",
      "================================================================================\n",
      "loss 0.0937:  88%|███████████████████▎  | 26399/30000 [2:52:51<16:28,  3.64it/s]11/11/2019 22:11:30 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 22:11:30 - INFO - __main__ -     global_step = 6600\n",
      "11/11/2019 22:11:30 - INFO - __main__ -     train loss = 0.0937\n",
      "11/11/2019 22:11:43 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 22:11:43 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 22:11:43 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 22:14:41 - INFO - __main__ -     eval_F1 = 0.7887951532019031\n",
      "11/11/2019 22:14:41 - INFO - __main__ -     eval_loss = 0.4020436896131404\n",
      "11/11/2019 22:14:41 - INFO - __main__ -     global_step = 6600\n",
      "11/11/2019 22:14:41 - INFO - __main__ -     loss = 0.0937\n",
      "================================================================================\n",
      "loss 0.0917:  91%|███████████████████▉  | 27199/30000 [2:59:21<13:06,  3.56it/s]11/11/2019 22:18:00 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 22:18:00 - INFO - __main__ -     global_step = 6800\n",
      "11/11/2019 22:18:00 - INFO - __main__ -     train loss = 0.0917\n",
      "11/11/2019 22:18:13 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 22:18:13 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 22:18:13 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 22:21:11 - INFO - __main__ -     eval_F1 = 0.800018788698034\n",
      "11/11/2019 22:21:11 - INFO - __main__ -     eval_loss = 0.4044611579287917\n",
      "11/11/2019 22:21:11 - INFO - __main__ -     global_step = 6800\n",
      "11/11/2019 22:21:11 - INFO - __main__ -     loss = 0.0917\n",
      "================================================================================\n",
      "loss 0.1057:  93%|████████████████████▌ | 27999/30000 [3:05:51<09:24,  3.54it/s]11/11/2019 22:24:30 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 22:24:30 - INFO - __main__ -     global_step = 7000\n",
      "11/11/2019 22:24:30 - INFO - __main__ -     train loss = 0.1057\n",
      "11/11/2019 22:24:43 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 22:24:43 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 22:24:43 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 22:27:41 - INFO - __main__ -     eval_F1 = 0.801626929446933\n",
      "11/11/2019 22:27:41 - INFO - __main__ -     eval_loss = 0.41967578070057976\n",
      "11/11/2019 22:27:41 - INFO - __main__ -     global_step = 7000\n",
      "11/11/2019 22:27:41 - INFO - __main__ -     loss = 0.1057\n",
      "================================================================================\n",
      "loss 0.1043:  96%|█████████████████████ | 28799/30000 [3:12:19<05:29,  3.65it/s]11/11/2019 22:30:58 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 22:30:58 - INFO - __main__ -     global_step = 7200\n",
      "11/11/2019 22:30:58 - INFO - __main__ -     train loss = 0.1043\n",
      "11/11/2019 22:31:11 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 22:31:11 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 22:31:11 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 22:34:09 - INFO - __main__ -     eval_F1 = 0.8011780100993763\n",
      "11/11/2019 22:34:09 - INFO - __main__ -     eval_loss = 0.415434135334386\n",
      "11/11/2019 22:34:09 - INFO - __main__ -     global_step = 7200\n",
      "11/11/2019 22:34:09 - INFO - __main__ -     loss = 0.1043\n",
      "================================================================================\n",
      "loss 0.1015:  99%|█████████████████████▋| 29599/30000 [3:18:46<01:52,  3.55it/s]11/11/2019 22:37:25 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 22:37:25 - INFO - __main__ -     global_step = 7400\n",
      "11/11/2019 22:37:25 - INFO - __main__ -     train loss = 0.1015\n",
      "11/11/2019 22:37:38 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 22:37:38 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 22:37:38 - INFO - __main__ -     Batch size = 48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11/2019 22:40:36 - INFO - __main__ -     eval_F1 = 0.8015834858687804\n",
      "11/11/2019 22:40:36 - INFO - __main__ -     eval_loss = 0.41254822597388296\n",
      "11/11/2019 22:40:36 - INFO - __main__ -     global_step = 7400\n",
      "11/11/2019 22:40:36 - INFO - __main__ -     loss = 0.1015\n",
      "================================================================================\n",
      "loss 0.078: 100%|███████████████████████| 30000/30000 [3:23:35<00:00,  2.46it/s]\n",
      "11/11/2019 22:42:14 - INFO - pytorch_transformers.modeling_utils -   loading weights file ../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_3/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "Traceback (most recent call last):\n",
      "  File \"./run_bert_2562_last2embedding_cls.py\", line 841, in <module>\n",
      "    main()\n",
      "  File \"./run_bert_2562_last2embedding_cls.py\", line 757, in main\n",
      "    logits = model(input_ids=input_ids, token_type_ids=segment_ids, attention_mask=input_mask).detach().cpu().numpy()\n",
      "  File \"/home/haizhi/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/haizhi/workspace/TODO/2019互联网新闻/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 1119, in forward\n",
      "    attention_mask=flat_attention_mask, head_mask=head_mask)\n",
      "  File \"/home/haizhi/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/haizhi/workspace/TODO/2019互联网新闻/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 727, in forward\n",
      "    head_mask=head_mask)\n",
      "  File \"/home/haizhi/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/haizhi/workspace/TODO/2019互联网新闻/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 440, in forward\n",
      "    layer_outputs = layer_module(hidden_states, attention_mask, head_mask[i])\n",
      "  File \"/home/haizhi/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/haizhi/workspace/TODO/2019互联网新闻/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 420, in forward\n",
      "    intermediate_output = self.intermediate(attention_output)\n",
      "  File \"/home/haizhi/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/haizhi/workspace/TODO/2019互联网新闻/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 392, in forward\n",
      "    hidden_states = self.intermediate_act_fn(hidden_states)\n",
      "  File \"/home/haizhi/workspace/TODO/2019互联网新闻/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 142, in gelu\n",
      "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 384.00 MiB (GPU 0; 10.92 GiB total capacity; 9.35 GiB already allocated; 191.00 MiB free; 810.30 MiB cached)\n"
     ]
    }
   ],
   "source": [
    "!python ./run_bert_2562_last2embedding_cls.py \\\n",
    "--model_type bert \\\n",
    "--model_name_or_path ../model/chinese_roberta_wwm_large_ext_pytorch  \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_test \\\n",
    "--data_dir ../data/data_StratifiedKFold_42/data_origin_3 \\\n",
    "--output_dir ../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_3 \\\n",
    "--max_seq_length 512 \\\n",
    "--split_num 1 \\\n",
    "--lstm_hidden_size 512 \\\n",
    "--lstm_layers 1 \\\n",
    "--lstm_dropout 0.1 \\\n",
    "--eval_steps 200 \\\n",
    "--per_gpu_train_batch_size 4 \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--warmup_steps 0 \\\n",
    "--per_gpu_eval_batch_size 48 \\\n",
    "--learning_rate 1e-5 \\\n",
    "--adam_epsilon 1e-6 \\\n",
    "--weight_decay 0 \\\n",
    "--train_steps 30000 \\\n",
    "--freeze 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11/2019 22:42:36 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "11/11/2019 22:42:36 - INFO - pytorch_transformers.tokenization_utils -   Model name '../model/chinese_roberta_wwm_large_ext_pytorch' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming '../model/chinese_roberta_wwm_large_ext_pytorch' is a path or url to a directory containing tokenizer files.\n",
      "11/11/2019 22:42:36 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../model/chinese_roberta_wwm_large_ext_pytorch/added_tokens.json. We won't load it.\n",
      "11/11/2019 22:42:36 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../model/chinese_roberta_wwm_large_ext_pytorch/special_tokens_map.json. We won't load it.\n",
      "11/11/2019 22:42:36 - INFO - pytorch_transformers.tokenization_utils -   loading file ../model/chinese_roberta_wwm_large_ext_pytorch/vocab.txt\n",
      "11/11/2019 22:42:36 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/11/2019 22:42:36 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/11/2019 22:42:36 - INFO - pytorch_transformers.modeling_utils -   loading configuration file ../model/chinese_roberta_wwm_large_ext_pytorch/config.json\n",
      "11/11/2019 22:42:36 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 3,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "11/11/2019 22:42:36 - INFO - pytorch_transformers.modeling_utils -   loading weights file ../model/chinese_roberta_wwm_large_ext_pytorch/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "11/11/2019 22:42:41 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification_last2embedding_cls not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "11/11/2019 22:42:41 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification_last2embedding_cls: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "11/11/2019 22:42:44 - INFO - __main__ -   ** RAW EXAMPLE **\n",
      "11/11/2019 22:42:44 - INFO - __main__ -   content: ['这', '几', '天', '看', '了', '有', '人', '举', '报', '施', '某', '某', '的', '贴', '子', '，', '经', '与', '举', '报', '人', '联', '系', '证', '实', '，', '是', '宣', '某', '当', '天', '中', '午', '请', '举', '报', '人', '和', '枪', '手', '喝', '酒', '后', '，', '晚', '上', '才', '发', '的', '贴', '子', '！', '本', '人', '不', '去', '讨', '论', '前', '二', '天', '的', '举', '报', '，', '相', '信', '总', '归', '会', '有', '说', '法', '的', '！', '今', '天', '一', '看', '施', '全', '军', '2017', '年', '1', '月', '2', '日', '实', '名', '举', '报', '上', '黄', '镇', '宣', '国', '才', '的', '贴', '子', '（', '仍', '被', '锁', '定', '禁', '止', '评', '论', '）', '已', '经', '正', '好', '一', '整', '年', '了', '=', '750', ')', 'window', '.', 'open', '(', \"'\", 'http', ':', '/', '/', 'img', '.', 'js', '##ly', '##001', '.', 'com', '/', 'at', '##ta', '##ch', '##ment', '/', 'mon', '_', '180', '##1', '/', '4', '_', '291', '##08', '##5', '_', 'c', '##79', '##6', '##a', '##6', '##a', '##86', '##e', '##17', '##12', '##1', '.', 'jpg', '?', '123', \"'\", ')', ';', '\"', 'on', '##load', '=', '\"', 'if', '(', 'this', '.', 'off', '##set', '##wi', '##dt', '##h', '>', \"'\", '750', \"'\", ')', 'this', '.', 'wi', '##dt', '##h', '=', \"'\", '750', \"'\", ';', '\"', 'sr', '##c', '=', '\"', 'http', ':', '/', '/', 'img', '.', 'js', '##ly', '##001', '.', 'com', '/', 'at', '##ta', '##ch', '##ment', '/', 'mon', '_', '180', '##1', '/', '4', '_', '291', '##08', '##5', '_', 'c', '##79', '##6', '##a', '##6', '##a', '##86', '##e', '##17', '##12', '##1', '.', 'jpg', '?', '123', '\"', 'style', '=', '\"', 'max', '-', 'wi', '##dt', '##h', ':', '750', '##px', ';', '\"', '/', '>', '图', '片', ':', '/', 'home', '/', 'al', '##ida', '##ta', '/', 'www', '/', 'data', '/', 'tm', '##p', '/', 'q', '##fu', '##pl', '##oa', '##d', '/', '4', '_', '291', '##08', '##5', '_', '151', '##49', '##81', '##47', '##14', '##78', '##95', '##2', '.', 'jpg', '施', '全', '军', '实', '名', '举', '报', '50', '天', '后', '，', '上', '黄', '镇', '党', '委', '政', '府', '回', '复', '如', '下', '图', '：', '=', '750', ')', 'window', '.', 'open', '(', \"'\", 'http', ':', '/', '/', 'img', '.', 'js', '##ly', '##001', '.', 'com', '/', 'at', '##ta', '##ch', '##ment', '/', 'mon', '_', '180', '##1', '/', '4', '_', '291', '##08', '##5', '_', 'a9', '##b', '##11', '##b', '##7', '##ea', '##2', '##b', '##1', '##ce', '##9', '.', 'jpg', '?', '90', \"'\", ')', ';', '\"', 'on', '##load', '=', '\"', 'if', '(', 'this', '.', 'off', '##set', '##wi', '##dt', '##h', '>', \"'\", '750', \"'\", ')', 'this', '.', 'wi', '##dt', '##h', '=', \"'\", '750', \"'\", ';', '\"', 'sr', '##c', '=', '\"', 'http', ':', '/', '/', 'img', '.', 'js', '##ly', '##001', '.', 'com', '/', 'at', '##ta', '##ch', '##ment', '/', 'mon', '_', '180', '##1', '/', '4', '_', '291', '##08', '##5', '_', 'a9', '##b', '##11', '##b', '##7', '##ea', '##2', '##b', '##1', '##ce', '##9', '.', 'jpg', '?', '90', '\"', 'style', '=', '\"', 'max', '-', 'wi', '##dt', '##h', ':', '750', '##px', ';', '\"', '/', '>', '图', '片', ':', '/', 'home', '/', 'al', '##ida', '##ta', '/', 'www', '/', 'data', '/', 'tm', '##p', '/', 'q', '##fu', '##pl', '##oa', '##d', '/', '4', '_', '291', '##08', '##5', '_', '151', '##49', '##81', '##47', '##26', '##31', '##66', '##8', '.', 'jpg', '=', '750', ')', 'window', '.', 'open', '(', \"'\", 'http', ':', '/', '/', 'img', '.', 'js', '##ly', '##001', '.', 'com', '/', 'at', '##ta', '##ch', '##ment', '/', 'mon', '_', '180', '##1', '/', '4', '_', '291', '##08', '##5', '_', '9', '##cd', '##e', '##9', '##b', '##39', '##43', '##fe', '##20', '##c', '.', 'jpg', '?', '75', \"'\", ')', ';', '\"', 'on', '##load', '=', '\"', 'if', '(', 'this', '.', 'off', '##set', '##wi', '##dt', '##h', '>', \"'\", '750', \"'\", ')', 'this', '.', 'wi', '##dt', '##h', '=', \"'\", '750', \"'\", ';', '\"', 'sr', '##c', '=', '\"', 'http', ':', '/', '/', 'img', '.', 'js', '##ly', '##001', '.', 'com', '/', 'at', '##ta', '##ch', '##ment', '/', 'mon', '_', '180', '##1', '/', '4', '_', '291', '##08', '##5', '_', '9', '##cd', '##e', '##9', '##b', '##39', '##43', '##fe', '##20', '##c', '.', 'jpg', '?', '75', '\"', 'style', '=', '\"', 'max', '-', 'wi', '##dt', '##h', ':', '750', '##px', ';', '\"', '/', '>', '图', '片', ':', '/', 'home', '/', 'al', '##ida', '##ta', '/', 'www', '/', 'data', '/', 'tm', '##p', '/', 'q', '##fu', '##pl', '##oa', '##d', '/', '4', '_', '291', '##08', '##5', '_', '151', '##49', '##81', '##47', '##23', '##53', '##07', '##5', '.', 'jpg', '一', '年', '的', '贴', '子', '，', '再', '次', '被', '网', '友', '顶', '起', '来', '后', '，', '才', '发', '现', '施', '某', '几', '天', '前', '回', '复', '网', '友', '的', '处', '理', '结', '果', '竟', '如', '下', '图', '：', '=', '750', ')', 'window', '.', 'open', '(', \"'\", 'http', ':', '/', '/', 'img', '.', 'js', '##ly', '##001', '.', 'com', '/', 'at', '##ta', '##ch', '##ment', '/', 'mon', '_', '180', '##1', '/', '4', '_', '291', '##08', '##5', '_', '9', '##d', '##32', '##ee', '##57', '##27', '##60', '##d', '##85', '.', 'jpg', '?', '131', \"'\", ')', ';', '\"', 'on', '##load', '=', '\"', 'if', '(', 'this', '.', 'off', '##set', '##wi', '##dt', '##h', '>', \"'\", '750', \"'\", ')', 'this', '.', 'wi', '##dt', '##h', '=', \"'\", '750', \"'\", ';', '\"', 'sr', '##c', '=', '\"', 'http', ':', '/', '/', 'img', '.', 'js', '##ly', '##001', '.', 'com', '/', 'at', '##ta', '##ch', '##ment', '/', 'mon', '_', '180', '##1', '/', '4', '_', '291', '##08', '##5', '_', '9', '##d', '##32', '##ee', '##57', '##27', '##60', '##d', '##85', '.', 'jpg', '?', '131', '\"', 'style', '=', '\"', 'max', '-', 'wi', '##dt', '##h', ':', '750', '##px', ';', '\"', '/', '>', '图', '片', ':', '/', 'home', '/', 'al', '##ida', '##ta', '/', 'www', '/', 'data', '/', 'tm', '##p', '/', 'q', '##fu', '##pl', '##oa', '##d', '/', '4', '_', '291', '##08', '##5', '_', '151', '##49', '##81', '##47', '##35', '##47', '##17', '##2', '.', 'jpg', '现', '责', '问', '张', '涛', '书', '记', '：', '1', '、', '宣', '国', '才', '被', '举', '报', '这', '么', '多', '问', '题', '，', '什', '么', '时', '候', '有', '答', '复', '。', '2', '、', '宣', '国', '才', '被', '举', '报', '后', '，', '为', '什', '么', '被', '立', '刻', '免', '了', '村', '书', '记', '职', '务', '？', '为', '什', '么', '又', '被', '安', '排', '到', '城', '管', '队', '[UNK]', '吃', '空', '响', '[UNK]', '，', '自', '己', '却', '天', '天', '在', '我', '们', '水', '泥', '厂', '上', '班', '赚', '黑', '钱', '？', '3', '、', '这', '几', '个', '月', '，', '水', '泥', '每', '吨', '近', '200', '元', '纯', '利', '润', '，', '还', '供', '不', '应', '求', '，', '宣', '国', '才', '还', '清', '上', '黄', '政', '府', '担', '保', '借', '给', '宣', '国', '才', '代', '付', '振', '东', '厂', '工', '资', '社', '保', '的', '钱', '了', '吗', '？', '4', '、', '据', '了', '解', '宣', '国', '才', '占', '他', '人', '企', '业', '经', '营', '，', '又', '欠', '税', '52', '.', '16', '万', '元', '、', '欠', '社', '保', '32', '.', '76', '万', '元', '、', '应', '该', '还', '欠', '了', '职', '工', '工', '资', '几', '十', '万', '，', '上', '黄', '政', '府', '打', '算', '替', '宣', '国', '才', '担', '保', '还', '是', '归', '还', '？', '5', '、', '我', '们', '厂', '合', '法', '会', '计', '和', '老', '板', '被', '判', '刑', '四', '到', '六', '年', '，', '现', '在', '服', '刑', '。', '厂', '子', '给', '宣', '国', '才', '强', '占', '，', '宣', '国', '才', '每', '天', '赚', '20', '多', '万', '净', '利', '润', '，', '却', '对', '外', '宣', '称', '天', '天', '亏', '本', '！', '等', '咱', '老', '板', '刑', '满', '回', '厂', '，', '宣', '国', '才', '给', '咱', '厂', '[UNK]', '天', '天', '亏', '[UNK]', '可', '能', '要', '[UNK]', '亏', '[UNK]', '的', '几', '千', '万', '元', '，', '甚', '至', '几', '个', '亿', '，', '张', '涛', '书', '记', '您', '承', '担', '还', '是', '上', '黄', '政', '府', '承', '担', '？', '当', '初', '可', '是', '您', '亲', '自', '把', '厂', '交', '给', '宣', '国', '才', '生', '产', '的', '！', '希', '望', '徐', '市', '长', '看', '到', '本', '贴', '后', '能', '像', '批', '示', '263', '、', '批', '示', '违', '建', '等', '民', '生', '问', '题', '一', '样', '，', '关', '注', '一', '下', '我', '们', '水', '泥', '厂', '的', '将', '来', '！', '也', '请', '徐', '市', '长', '抽', '日', '理', '万', '机', '之', '空', '亲', '自', '约', '谈', '一', '下', '当', '事', '人', '（', '特', '别', '是', '那', '位', '施', '站', '长', '）', '，', '千', '万', '不', '能', '听', '取', '一', '面', '之', '辞', '！']\n",
      "11/11/2019 22:42:44 - INFO - __main__ -   *** Example ***\n",
      "11/11/2019 22:42:44 - INFO - __main__ -   idx: 0\n",
      "11/11/2019 22:42:44 - INFO - __main__ -   guid: 7a3dd79f90ee419da87190cff60f7a86\n",
      "11/11/2019 22:42:44 - INFO - __main__ -   tokens: [CLS] 问 责 领 导 ( 上 黄 镇 党 委 书 记 张 涛 ， 宣 国 才 真 能 一 手 遮 天 吗 ？ ) [SEP] ##57 ##27 ##60 ##d ##85 . jpg ? 131 \" style = \" max - wi ##dt ##h : 750 ##px ; \" / > 图 片 : / home / al ##ida ##ta / www / data / tm ##p / q ##fu ##pl ##oa ##d / 4 _ 291 ##08 ##5 _ 151 ##49 ##81 ##47 ##35 ##47 ##17 ##2 . jpg 现 责 问 张 涛 书 记 ： 1 、 宣 国 才 被 举 报 这 么 多 问 题 ， 什 么 时 候 有 答 复 。 2 、 宣 国 才 被 举 报 后 ， 为 什 么 被 立 刻 免 了 村 书 记 职 务 ？ 为 什 么 又 被 安 排 到 城 管 队 [UNK] 吃 空 响 [UNK] ， 自 己 却 天 天 在 我 们 水 泥 厂 上 班 赚 黑 钱 ？ 3 、 这 几 个 月 ， 水 泥 每 吨 近 200 元 纯 利 润 ， 还 供 不 应 求 ， 宣 国 才 还 清 上 黄 政 府 担 保 借 给 宣 国 才 代 付 振 东 厂 工 资 社 保 的 钱 了 吗 ？ 4 、 据 了 解 宣 国 才 占 他 人 企 业 经 营 ， 又 欠 税 52 . 16 万 元 、 欠 社 保 32 . 76 万 元 、 应 该 还 欠 了 职 工 工 资 几 十 万 ， 上 黄 政 府 打 算 替 宣 国 才 担 保 还 是 归 还 ？ 5 、 我 们 厂 合 法 会 计 和 老 板 被 判 刑 四 到 六 年 ， 现 在 服 刑 。 厂 子 给 宣 国 才 强 占 ， 宣 国 才 每 天 赚 20 多 万 净 利 润 ， 却 对 外 宣 称 天 天 亏 本 ！ 等 咱 老 板 刑 满 回 厂 ， 宣 国 才 给 咱 厂 [UNK] 天 天 亏 [UNK] 可 能 要 [UNK] 亏 [UNK] 的 几 千 万 元 ， 甚 至 几 个 亿 ， 张 涛 书 记 您 承 担 还 是 上 黄 政 府 承 担 ？ 当 初 可 是 您 亲 自 把 厂 交 给 宣 国 才 生 产 的 ！ 希 望 徐 市 长 看 到 本 贴 后 能 像 批 示 263 、 批 示 违 建 等 民 生 问 题 一 样 ， 关 注 一 下 我 们 水 泥 厂 的 将 来 ！ 也 请 徐 市 长 抽 日 理 万 机 之 空 亲 自 约 谈 一 下 当 事 人 （ 特 别 是 那 位 施 站 长 ） ， 千 万 不 能 听 取 一 面 之 辞 [SEP]\n",
      "11/11/2019 22:42:44 - INFO - __main__ -   input_ids: 101 7309 6569 7566 2193 113 677 7942 7252 1054 1999 741 6381 2476 3875 8024 2146 1744 2798 4696 5543 671 2797 6902 1921 1408 8043 114 102 9647 8976 8581 8168 9169 119 9248 136 9403 107 8969 134 107 8621 118 8541 12672 8199 131 9180 10605 132 107 120 135 1745 4275 131 120 8563 120 9266 12708 8383 120 8173 120 9000 120 9908 8187 120 159 12043 12569 11355 8168 120 125 142 11777 9153 8157 142 9564 9500 9313 9050 8852 9050 8408 8144 119 9248 4385 6569 7309 2476 3875 741 6381 8038 122 510 2146 1744 2798 6158 715 2845 6821 720 1914 7309 7579 8024 784 720 3198 952 3300 5031 1908 511 123 510 2146 1744 2798 6158 715 2845 1400 8024 711 784 720 6158 4989 1174 1048 749 3333 741 6381 5466 1218 8043 711 784 720 1348 6158 2128 2961 1168 1814 5052 7339 100 1391 4958 1510 100 8024 5632 2346 1316 1921 1921 1762 2769 812 3717 3799 1322 677 4408 6611 7946 7178 8043 124 510 6821 1126 702 3299 8024 3717 3799 3680 1417 6818 8185 1039 5283 1164 3883 8024 6820 897 679 2418 3724 8024 2146 1744 2798 6820 3926 677 7942 3124 2424 2857 924 955 5314 2146 1744 2798 807 802 2920 691 1322 2339 6598 4852 924 4638 7178 749 1408 8043 125 510 2945 749 6237 2146 1744 2798 1304 800 782 821 689 5307 5852 8024 1348 3612 4925 8247 119 8121 674 1039 510 3612 4852 924 8211 119 8399 674 1039 510 2418 6421 6820 3612 749 5466 2339 2339 6598 1126 1282 674 8024 677 7942 3124 2424 2802 5050 3296 2146 1744 2798 2857 924 6820 3221 2495 6820 8043 126 510 2769 812 1322 1394 3791 833 6369 1469 5439 3352 6158 1161 1152 1724 1168 1063 2399 8024 4385 1762 3302 1152 511 1322 2094 5314 2146 1744 2798 2487 1304 8024 2146 1744 2798 3680 1921 6611 8113 1914 674 1112 1164 3883 8024 1316 2190 1912 2146 4917 1921 1921 755 3315 8013 5023 1493 5439 3352 1152 4007 1726 1322 8024 2146 1744 2798 5314 1493 1322 100 1921 1921 755 100 1377 5543 6206 100 755 100 4638 1126 1283 674 1039 8024 4493 5635 1126 702 783 8024 2476 3875 741 6381 2644 2824 2857 6820 3221 677 7942 3124 2424 2824 2857 8043 2496 1159 1377 3221 2644 779 5632 2828 1322 769 5314 2146 1744 2798 4495 772 4638 8013 2361 3307 2528 2356 7270 4692 1168 3315 6585 1400 5543 1008 2821 4850 10864 510 2821 4850 6824 2456 5023 3696 4495 7309 7579 671 3416 8024 1068 3800 671 678 2769 812 3717 3799 1322 4638 2199 3341 8013 738 6435 2528 2356 7270 2853 3189 4415 674 3322 722 4958 779 5632 5276 6448 671 678 2496 752 782 8020 4294 1166 3221 6929 855 3177 4991 7270 8021 8024 1283 674 679 5543 1420 1357 671 7481 722 6791 102\n",
      "11/11/2019 22:42:44 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/11/2019 22:42:44 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/11/2019 22:42:44 - INFO - __main__ -   label: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11/2019 22:43:33 - INFO - __main__ -   ***** Running training *****\n",
      "11/11/2019 22:43:33 - INFO - __main__ -     Num examples = 11758\n",
      "11/11/2019 22:43:33 - INFO - __main__ -     Batch size = 4\n",
      "11/11/2019 22:43:33 - INFO - __main__ -     Num steps = 30000\n",
      "  0%|                                                 | 0/30000 [00:00<?, ?it/s]11/11/2019 22:43:46 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 22:43:46 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 22:43:46 - INFO - __main__ -     Batch size = 48\n",
      "/home/haizhi/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "11/11/2019 22:46:40 - INFO - __main__ -     eval_F1 = 0.20079195349582182\n",
      "11/11/2019 22:46:40 - INFO - __main__ -     eval_loss = 1.342591678903949\n",
      "11/11/2019 22:46:40 - INFO - __main__ -     global_step = 0\n",
      "================================================================================\n",
      "Best F1 0.20079195349582182\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.7099:   3%|▋                       | 799/30000 [06:28<2:13:26,  3.65it/s]11/11/2019 22:50:02 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 22:50:02 - INFO - __main__ -     global_step = 200\n",
      "11/11/2019 22:50:02 - INFO - __main__ -     train loss = 0.7099\n",
      "loss 0.5912:   5%|█▏                     | 1599/30000 [09:41<2:07:11,  3.72it/s]11/11/2019 22:53:15 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 22:53:15 - INFO - __main__ -     global_step = 400\n",
      "11/11/2019 22:53:15 - INFO - __main__ -     train loss = 0.5912\n",
      "loss 0.482:   8%|█▉                      | 2399/30000 [12:54<2:07:22,  3.61it/s]11/11/2019 22:56:28 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 22:56:28 - INFO - __main__ -     global_step = 600\n",
      "11/11/2019 22:56:28 - INFO - __main__ -     train loss = 0.482\n",
      "loss 0.4366:  11%|██▍                    | 3199/30000 [16:08<1:58:52,  3.76it/s]11/11/2019 22:59:42 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 22:59:42 - INFO - __main__ -     global_step = 800\n",
      "11/11/2019 22:59:42 - INFO - __main__ -     train loss = 0.4366\n",
      "loss 0.3864:  13%|███                    | 3999/30000 [19:22<1:52:21,  3.86it/s]11/11/2019 23:02:56 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 23:02:56 - INFO - __main__ -     global_step = 1000\n",
      "11/11/2019 23:02:56 - INFO - __main__ -     train loss = 0.3864\n",
      "loss 0.3729:  16%|███▋                   | 4799/30000 [22:37<1:50:07,  3.81it/s]11/11/2019 23:06:11 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 23:06:11 - INFO - __main__ -     global_step = 1200\n",
      "11/11/2019 23:06:11 - INFO - __main__ -     train loss = 0.3729\n",
      "loss 0.4086:  19%|████▎                  | 5599/30000 [25:53<1:52:54,  3.60it/s]11/11/2019 23:09:27 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 23:09:27 - INFO - __main__ -     global_step = 1400\n",
      "11/11/2019 23:09:27 - INFO - __main__ -     train loss = 0.4086\n",
      "loss 0.4045:  21%|████▉                  | 6399/30000 [29:08<1:42:47,  3.83it/s]11/11/2019 23:12:42 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 23:12:42 - INFO - __main__ -     global_step = 1600\n",
      "11/11/2019 23:12:42 - INFO - __main__ -     train loss = 0.4045\n",
      "loss 0.3734:  24%|█████▌                 | 7199/30000 [32:23<1:41:52,  3.73it/s]11/11/2019 23:15:57 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 23:15:57 - INFO - __main__ -     global_step = 1800\n",
      "11/11/2019 23:15:57 - INFO - __main__ -     train loss = 0.3734\n",
      "loss 0.3932:  27%|██████▏                | 7999/30000 [35:39<1:37:01,  3.78it/s]11/11/2019 23:19:13 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 23:19:13 - INFO - __main__ -     global_step = 2000\n",
      "11/11/2019 23:19:13 - INFO - __main__ -     train loss = 0.3932\n",
      "loss 0.3739:  29%|██████▋                | 8799/30000 [38:54<1:35:56,  3.68it/s]11/11/2019 23:22:28 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 23:22:28 - INFO - __main__ -     global_step = 2200\n",
      "11/11/2019 23:22:28 - INFO - __main__ -     train loss = 0.3739\n",
      "loss 0.3568:  32%|███████▎               | 9599/30000 [42:08<1:31:10,  3.73it/s]11/11/2019 23:25:42 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 23:25:42 - INFO - __main__ -     global_step = 2400\n",
      "11/11/2019 23:25:42 - INFO - __main__ -     train loss = 0.3568\n",
      "loss 0.3918:  35%|███████▋              | 10399/30000 [45:23<1:29:47,  3.64it/s]11/11/2019 23:28:57 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 23:28:57 - INFO - __main__ -     global_step = 2600\n",
      "11/11/2019 23:28:57 - INFO - __main__ -     train loss = 0.3918\n",
      "loss 0.343:  37%|████████▌              | 11199/30000 [48:37<1:22:45,  3.79it/s]11/11/2019 23:32:11 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 23:32:11 - INFO - __main__ -     global_step = 2800\n",
      "11/11/2019 23:32:11 - INFO - __main__ -     train loss = 0.343\n",
      "11/11/2019 23:32:25 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 23:32:25 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 23:32:25 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 23:35:23 - INFO - __main__ -     eval_F1 = 0.7825181634484971\n",
      "11/11/2019 23:35:23 - INFO - __main__ -     eval_loss = 0.37835647580364057\n",
      "11/11/2019 23:35:23 - INFO - __main__ -     global_step = 2800\n",
      "11/11/2019 23:35:23 - INFO - __main__ -     loss = 0.343\n",
      "================================================================================\n",
      "Best F1 0.7825181634484971\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.3658:  40%|████████▊             | 11999/30000 [55:15<1:19:27,  3.78it/s]11/11/2019 23:38:49 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 23:38:49 - INFO - __main__ -     global_step = 3000\n",
      "11/11/2019 23:38:49 - INFO - __main__ -     train loss = 0.3658\n",
      "11/11/2019 23:39:02 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 23:39:02 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 23:39:02 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 23:42:00 - INFO - __main__ -     eval_F1 = 0.7745555642266293\n",
      "11/11/2019 23:42:00 - INFO - __main__ -     eval_loss = 0.36840412633553626\n",
      "11/11/2019 23:42:00 - INFO - __main__ -     global_step = 3000\n",
      "11/11/2019 23:42:00 - INFO - __main__ -     loss = 0.3658\n",
      "================================================================================\n",
      "loss 0.3201:  43%|████████▌           | 12799/30000 [1:01:42<1:18:26,  3.65it/s]11/11/2019 23:45:16 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 23:45:16 - INFO - __main__ -     global_step = 3200\n",
      "11/11/2019 23:45:16 - INFO - __main__ -     train loss = 0.3201\n",
      "11/11/2019 23:45:29 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 23:45:29 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 23:45:29 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 23:48:27 - INFO - __main__ -     eval_F1 = 0.7949629075289398\n",
      "11/11/2019 23:48:27 - INFO - __main__ -     eval_loss = 0.38437510807547837\n",
      "11/11/2019 23:48:27 - INFO - __main__ -     global_step = 3200\n",
      "11/11/2019 23:48:27 - INFO - __main__ -     loss = 0.3201\n",
      "================================================================================\n",
      "Best F1 0.7949629075289398\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.2973:  45%|█████████           | 13599/30000 [1:08:19<1:16:51,  3.56it/s]11/11/2019 23:51:53 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 23:51:53 - INFO - __main__ -     global_step = 3400\n",
      "11/11/2019 23:51:53 - INFO - __main__ -     train loss = 0.2973\n",
      "11/11/2019 23:52:06 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 23:52:06 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 23:52:06 - INFO - __main__ -     Batch size = 48\n",
      "11/11/2019 23:55:05 - INFO - __main__ -     eval_F1 = 0.7815524615544763\n",
      "11/11/2019 23:55:05 - INFO - __main__ -     eval_loss = 0.356012057032316\n",
      "11/11/2019 23:55:05 - INFO - __main__ -     global_step = 3400\n",
      "11/11/2019 23:55:05 - INFO - __main__ -     loss = 0.2973\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.2309:  48%|█████████▌          | 14399/30000 [1:14:47<1:09:03,  3.77it/s]11/11/2019 23:58:21 - INFO - __main__ -   ***** Report result *****\n",
      "11/11/2019 23:58:21 - INFO - __main__ -     global_step = 3600\n",
      "11/11/2019 23:58:21 - INFO - __main__ -     train loss = 0.2309\n",
      "11/11/2019 23:58:35 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/11/2019 23:58:35 - INFO - __main__ -     Num examples = 2938\n",
      "11/11/2019 23:58:35 - INFO - __main__ -     Batch size = 48\n",
      "11/12/2019 00:01:34 - INFO - __main__ -     eval_F1 = 0.7972559146954682\n",
      "11/12/2019 00:01:34 - INFO - __main__ -     eval_loss = 0.37722855355710755\n",
      "11/12/2019 00:01:34 - INFO - __main__ -     global_step = 3600\n",
      "11/12/2019 00:01:34 - INFO - __main__ -     loss = 0.2309\n",
      "================================================================================\n",
      "Best F1 0.7972559146954682\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.2533:  51%|██████████▏         | 15199/30000 [1:21:27<1:04:37,  3.82it/s]11/12/2019 00:05:01 - INFO - __main__ -   ***** Report result *****\n",
      "11/12/2019 00:05:01 - INFO - __main__ -     global_step = 3800\n",
      "11/12/2019 00:05:01 - INFO - __main__ -     train loss = 0.2533\n",
      "11/12/2019 00:05:14 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/12/2019 00:05:14 - INFO - __main__ -     Num examples = 2938\n",
      "11/12/2019 00:05:14 - INFO - __main__ -     Batch size = 48\n",
      "11/12/2019 00:08:13 - INFO - __main__ -     eval_F1 = 0.7953229109101829\n",
      "11/12/2019 00:08:13 - INFO - __main__ -     eval_loss = 0.39467352301242853\n",
      "11/12/2019 00:08:13 - INFO - __main__ -     global_step = 3800\n",
      "11/12/2019 00:08:13 - INFO - __main__ -     loss = 0.2533\n",
      "================================================================================\n",
      "loss 0.164:  53%|███████████▏         | 15999/30000 [1:27:58<1:03:44,  3.66it/s]11/12/2019 00:11:32 - INFO - __main__ -   ***** Report result *****\n",
      "11/12/2019 00:11:32 - INFO - __main__ -     global_step = 4000\n",
      "11/12/2019 00:11:32 - INFO - __main__ -     train loss = 0.164\n",
      "11/12/2019 00:11:45 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/12/2019 00:11:45 - INFO - __main__ -     Num examples = 2938\n",
      "11/12/2019 00:11:45 - INFO - __main__ -     Batch size = 48\n",
      "11/12/2019 00:14:45 - INFO - __main__ -     eval_F1 = 0.7930227563550475\n",
      "11/12/2019 00:14:45 - INFO - __main__ -     eval_loss = 0.4335171932414655\n",
      "11/12/2019 00:14:45 - INFO - __main__ -     global_step = 4000\n",
      "11/12/2019 00:14:45 - INFO - __main__ -     loss = 0.164\n",
      "================================================================================\n",
      "loss 0.2009:  56%|███████████▏        | 16799/30000 [1:34:29<1:01:58,  3.55it/s]11/12/2019 00:18:03 - INFO - __main__ -   ***** Report result *****\n",
      "11/12/2019 00:18:03 - INFO - __main__ -     global_step = 4200\n",
      "11/12/2019 00:18:03 - INFO - __main__ -     train loss = 0.2009\n",
      "11/12/2019 00:18:17 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/12/2019 00:18:17 - INFO - __main__ -     Num examples = 2938\n",
      "11/12/2019 00:18:17 - INFO - __main__ -     Batch size = 48\n",
      "11/12/2019 00:21:16 - INFO - __main__ -     eval_F1 = 0.7778637771296516\n",
      "11/12/2019 00:21:16 - INFO - __main__ -     eval_loss = 0.46078802921598955\n",
      "11/12/2019 00:21:16 - INFO - __main__ -     global_step = 4200\n",
      "11/12/2019 00:21:16 - INFO - __main__ -     loss = 0.2009\n",
      "================================================================================\n",
      "loss 0.1995:  59%|████████████▉         | 17599/30000 [1:41:01<57:59,  3.56it/s]11/12/2019 00:24:35 - INFO - __main__ -   ***** Report result *****\n",
      "11/12/2019 00:24:35 - INFO - __main__ -     global_step = 4400\n",
      "11/12/2019 00:24:35 - INFO - __main__ -     train loss = 0.1995\n",
      "11/12/2019 00:24:48 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/12/2019 00:24:48 - INFO - __main__ -     Num examples = 2938\n",
      "11/12/2019 00:24:48 - INFO - __main__ -     Batch size = 48\n",
      "11/12/2019 00:27:48 - INFO - __main__ -     eval_F1 = 0.7855157390560704\n",
      "11/12/2019 00:27:48 - INFO - __main__ -     eval_loss = 0.4457900928663871\n",
      "11/12/2019 00:27:48 - INFO - __main__ -     global_step = 4400\n",
      "11/12/2019 00:27:48 - INFO - __main__ -     loss = 0.1995\n",
      "================================================================================\n",
      "loss 0.2302:  61%|█████████████▍        | 18399/30000 [1:47:32<52:59,  3.65it/s]11/12/2019 00:31:06 - INFO - __main__ -   ***** Report result *****\n",
      "11/12/2019 00:31:06 - INFO - __main__ -     global_step = 4600\n",
      "11/12/2019 00:31:06 - INFO - __main__ -     train loss = 0.2302\n",
      "11/12/2019 00:31:19 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/12/2019 00:31:19 - INFO - __main__ -     Num examples = 2938\n",
      "11/12/2019 00:31:19 - INFO - __main__ -     Batch size = 48\n",
      "11/12/2019 00:34:19 - INFO - __main__ -     eval_F1 = 0.790238432083162\n",
      "11/12/2019 00:34:19 - INFO - __main__ -     eval_loss = 0.39701897693016835\n",
      "11/12/2019 00:34:19 - INFO - __main__ -     global_step = 4600\n",
      "11/12/2019 00:34:19 - INFO - __main__ -     loss = 0.2302\n",
      "================================================================================\n",
      "loss 0.1994:  64%|██████████████        | 19199/30000 [1:54:03<47:08,  3.82it/s]11/12/2019 00:37:37 - INFO - __main__ -   ***** Report result *****\n",
      "11/12/2019 00:37:37 - INFO - __main__ -     global_step = 4800\n",
      "11/12/2019 00:37:37 - INFO - __main__ -     train loss = 0.1994\n",
      "11/12/2019 00:37:50 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/12/2019 00:37:50 - INFO - __main__ -     Num examples = 2938\n",
      "11/12/2019 00:37:50 - INFO - __main__ -     Batch size = 48\n",
      "11/12/2019 00:40:49 - INFO - __main__ -     eval_F1 = 0.794407894148962\n",
      "11/12/2019 00:40:49 - INFO - __main__ -     eval_loss = 0.4414569547159537\n",
      "11/12/2019 00:40:49 - INFO - __main__ -     global_step = 4800\n",
      "11/12/2019 00:40:49 - INFO - __main__ -     loss = 0.1994\n",
      "================================================================================\n",
      "loss 0.2181:  67%|██████████████▋       | 19999/30000 [2:00:33<45:47,  3.64it/s]11/12/2019 00:44:07 - INFO - __main__ -   ***** Report result *****\n",
      "11/12/2019 00:44:07 - INFO - __main__ -     global_step = 5000\n",
      "11/12/2019 00:44:07 - INFO - __main__ -     train loss = 0.2181\n",
      "11/12/2019 00:44:20 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/12/2019 00:44:20 - INFO - __main__ -     Num examples = 2938\n",
      "11/12/2019 00:44:20 - INFO - __main__ -     Batch size = 48\n",
      "11/12/2019 00:47:19 - INFO - __main__ -     eval_F1 = 0.7736070729792841\n",
      "11/12/2019 00:47:19 - INFO - __main__ -     eval_loss = 0.41109024727296445\n",
      "11/12/2019 00:47:19 - INFO - __main__ -     global_step = 5000\n",
      "11/12/2019 00:47:19 - INFO - __main__ -     loss = 0.2181\n",
      "================================================================================\n",
      "loss 0.2061:  69%|███████████████▎      | 20799/30000 [2:07:02<42:13,  3.63it/s]11/12/2019 00:50:36 - INFO - __main__ -   ***** Report result *****\n",
      "11/12/2019 00:50:36 - INFO - __main__ -     global_step = 5200\n",
      "11/12/2019 00:50:36 - INFO - __main__ -     train loss = 0.2061\n",
      "11/12/2019 00:50:50 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/12/2019 00:50:50 - INFO - __main__ -     Num examples = 2938\n",
      "11/12/2019 00:50:50 - INFO - __main__ -     Batch size = 48\n",
      "11/12/2019 00:53:49 - INFO - __main__ -     eval_F1 = 0.7865229694423727\n",
      "11/12/2019 00:53:49 - INFO - __main__ -     eval_loss = 0.4036649054276847\n",
      "11/12/2019 00:53:49 - INFO - __main__ -     global_step = 5200\n",
      "11/12/2019 00:53:49 - INFO - __main__ -     loss = 0.2061\n",
      "================================================================================\n",
      "loss 0.1833:  72%|███████████████▊      | 21599/30000 [2:13:32<37:56,  3.69it/s]11/12/2019 00:57:06 - INFO - __main__ -   ***** Report result *****\n",
      "11/12/2019 00:57:06 - INFO - __main__ -     global_step = 5400\n",
      "11/12/2019 00:57:06 - INFO - __main__ -     train loss = 0.1833\n",
      "11/12/2019 00:57:19 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/12/2019 00:57:19 - INFO - __main__ -     Num examples = 2938\n",
      "11/12/2019 00:57:19 - INFO - __main__ -     Batch size = 48\n",
      "11/12/2019 01:00:18 - INFO - __main__ -     eval_F1 = 0.7935918276600588\n",
      "11/12/2019 01:00:18 - INFO - __main__ -     eval_loss = 0.42361037318985306\n",
      "11/12/2019 01:00:18 - INFO - __main__ -     global_step = 5400\n",
      "11/12/2019 01:00:18 - INFO - __main__ -     loss = 0.1833\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.1823:  75%|████████████████▍     | 22399/30000 [2:20:01<33:45,  3.75it/s]11/12/2019 01:03:35 - INFO - __main__ -   ***** Report result *****\n",
      "11/12/2019 01:03:35 - INFO - __main__ -     global_step = 5600\n",
      "11/12/2019 01:03:35 - INFO - __main__ -     train loss = 0.1823\n",
      "11/12/2019 01:03:49 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/12/2019 01:03:49 - INFO - __main__ -     Num examples = 2938\n",
      "11/12/2019 01:03:49 - INFO - __main__ -     Batch size = 48\n",
      "11/12/2019 01:06:49 - INFO - __main__ -     eval_F1 = 0.790386769410675\n",
      "11/12/2019 01:06:49 - INFO - __main__ -     eval_loss = 0.4332723853839261\n",
      "11/12/2019 01:06:49 - INFO - __main__ -     global_step = 5600\n",
      "11/12/2019 01:06:49 - INFO - __main__ -     loss = 0.1823\n",
      "================================================================================\n",
      "loss 0.1806:  77%|█████████████████     | 23199/30000 [2:26:34<31:20,  3.62it/s]11/12/2019 01:10:08 - INFO - __main__ -   ***** Report result *****\n",
      "11/12/2019 01:10:08 - INFO - __main__ -     global_step = 5800\n",
      "11/12/2019 01:10:08 - INFO - __main__ -     train loss = 0.1806\n",
      "11/12/2019 01:10:21 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/12/2019 01:10:21 - INFO - __main__ -     Num examples = 2938\n",
      "11/12/2019 01:10:21 - INFO - __main__ -     Batch size = 48\n",
      "11/12/2019 01:13:20 - INFO - __main__ -     eval_F1 = 0.8021051135172081\n",
      "11/12/2019 01:13:20 - INFO - __main__ -     eval_loss = 0.422350108503334\n",
      "11/12/2019 01:13:20 - INFO - __main__ -     global_step = 5800\n",
      "11/12/2019 01:13:20 - INFO - __main__ -     loss = 0.1806\n",
      "================================================================================\n",
      "Best F1 0.8021051135172081\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.1892:  80%|█████████████████▌    | 23999/30000 [2:33:14<27:45,  3.60it/s]11/12/2019 01:16:48 - INFO - __main__ -   ***** Report result *****\n",
      "11/12/2019 01:16:48 - INFO - __main__ -     global_step = 6000\n",
      "11/12/2019 01:16:48 - INFO - __main__ -     train loss = 0.1892\n",
      "11/12/2019 01:17:02 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/12/2019 01:17:02 - INFO - __main__ -     Num examples = 2938\n",
      "11/12/2019 01:17:02 - INFO - __main__ -     Batch size = 48\n",
      "11/12/2019 01:20:01 - INFO - __main__ -     eval_F1 = 0.7947258791991413\n",
      "11/12/2019 01:20:01 - INFO - __main__ -     eval_loss = 0.41177067833562053\n",
      "11/12/2019 01:20:01 - INFO - __main__ -     global_step = 6000\n",
      "11/12/2019 01:20:01 - INFO - __main__ -     loss = 0.1892\n",
      "================================================================================\n",
      "loss 0.1518:  83%|██████████████████▏   | 24799/30000 [2:39:45<22:52,  3.79it/s]11/12/2019 01:23:19 - INFO - __main__ -   ***** Report result *****\n",
      "11/12/2019 01:23:19 - INFO - __main__ -     global_step = 6200\n",
      "11/12/2019 01:23:19 - INFO - __main__ -     train loss = 0.1518\n",
      "11/12/2019 01:23:32 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/12/2019 01:23:32 - INFO - __main__ -     Num examples = 2938\n",
      "11/12/2019 01:23:32 - INFO - __main__ -     Batch size = 48\n",
      "11/12/2019 01:26:32 - INFO - __main__ -     eval_F1 = 0.7952097140252202\n",
      "11/12/2019 01:26:32 - INFO - __main__ -     eval_loss = 0.4203534780310527\n",
      "11/12/2019 01:26:32 - INFO - __main__ -     global_step = 6200\n",
      "11/12/2019 01:26:32 - INFO - __main__ -     loss = 0.1518\n",
      "================================================================================\n",
      "loss 0.1494:  85%|██████████████████▊   | 25599/30000 [2:46:17<19:15,  3.81it/s]11/12/2019 01:29:51 - INFO - __main__ -   ***** Report result *****\n",
      "11/12/2019 01:29:51 - INFO - __main__ -     global_step = 6400\n",
      "11/12/2019 01:29:51 - INFO - __main__ -     train loss = 0.1494\n",
      "11/12/2019 01:30:04 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/12/2019 01:30:04 - INFO - __main__ -     Num examples = 2938\n",
      "11/12/2019 01:30:04 - INFO - __main__ -     Batch size = 48\n",
      "11/12/2019 01:33:04 - INFO - __main__ -     eval_F1 = 0.7744345178514744\n",
      "11/12/2019 01:33:04 - INFO - __main__ -     eval_loss = 0.42266527329000736\n",
      "11/12/2019 01:33:04 - INFO - __main__ -     global_step = 6400\n",
      "11/12/2019 01:33:04 - INFO - __main__ -     loss = 0.1494\n",
      "================================================================================\n",
      "loss 0.1248:  88%|███████████████████▎  | 26399/30000 [2:52:47<15:46,  3.81it/s]11/12/2019 01:36:21 - INFO - __main__ -   ***** Report result *****\n",
      "11/12/2019 01:36:21 - INFO - __main__ -     global_step = 6600\n",
      "11/12/2019 01:36:21 - INFO - __main__ -     train loss = 0.1248\n",
      "11/12/2019 01:36:34 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/12/2019 01:36:34 - INFO - __main__ -     Num examples = 2938\n",
      "11/12/2019 01:36:34 - INFO - __main__ -     Batch size = 48\n",
      "11/12/2019 01:39:33 - INFO - __main__ -     eval_F1 = 0.7987953498961439\n",
      "11/12/2019 01:39:33 - INFO - __main__ -     eval_loss = 0.4358564613597287\n",
      "11/12/2019 01:39:33 - INFO - __main__ -     global_step = 6600\n",
      "11/12/2019 01:39:33 - INFO - __main__ -     loss = 0.1248\n",
      "================================================================================\n",
      "loss 0.109:  91%|████████████████████▊  | 27199/30000 [2:59:16<13:05,  3.57it/s]11/12/2019 01:42:50 - INFO - __main__ -   ***** Report result *****\n",
      "11/12/2019 01:42:50 - INFO - __main__ -     global_step = 6800\n",
      "11/12/2019 01:42:50 - INFO - __main__ -     train loss = 0.109\n",
      "11/12/2019 01:43:04 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/12/2019 01:43:04 - INFO - __main__ -     Num examples = 2938\n",
      "11/12/2019 01:43:04 - INFO - __main__ -     Batch size = 48\n",
      "11/12/2019 01:46:03 - INFO - __main__ -     eval_F1 = 0.7967876588620681\n",
      "11/12/2019 01:46:03 - INFO - __main__ -     eval_loss = 0.46013829763227654\n",
      "11/12/2019 01:46:03 - INFO - __main__ -     global_step = 6800\n",
      "11/12/2019 01:46:03 - INFO - __main__ -     loss = 0.109\n",
      "================================================================================\n",
      "loss 0.0689:  93%|████████████████████▌ | 27999/30000 [3:05:46<09:14,  3.61it/s]11/12/2019 01:49:20 - INFO - __main__ -   ***** Report result *****\n",
      "11/12/2019 01:49:20 - INFO - __main__ -     global_step = 7000\n",
      "11/12/2019 01:49:20 - INFO - __main__ -     train loss = 0.0689\n",
      "11/12/2019 01:49:34 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/12/2019 01:49:34 - INFO - __main__ -     Num examples = 2938\n",
      "11/12/2019 01:49:34 - INFO - __main__ -     Batch size = 48\n",
      "11/12/2019 01:52:32 - INFO - __main__ -     eval_F1 = 0.8031610844910076\n",
      "11/12/2019 01:52:32 - INFO - __main__ -     eval_loss = 0.4646761827321062\n",
      "11/12/2019 01:52:32 - INFO - __main__ -     global_step = 7000\n",
      "11/12/2019 01:52:32 - INFO - __main__ -     loss = 0.0689\n",
      "================================================================================\n",
      "Best F1 0.8031610844910076\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.0832:  96%|█████████████████████ | 28799/30000 [3:12:25<05:22,  3.72it/s]11/12/2019 01:55:59 - INFO - __main__ -   ***** Report result *****\n",
      "11/12/2019 01:55:59 - INFO - __main__ -     global_step = 7200\n",
      "11/12/2019 01:55:59 - INFO - __main__ -     train loss = 0.0832\n",
      "11/12/2019 01:56:12 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/12/2019 01:56:12 - INFO - __main__ -     Num examples = 2938\n",
      "11/12/2019 01:56:12 - INFO - __main__ -     Batch size = 48\n",
      "11/12/2019 01:59:12 - INFO - __main__ -     eval_F1 = 0.804364010516205\n",
      "11/12/2019 01:59:12 - INFO - __main__ -     eval_loss = 0.4702574521633646\n",
      "11/12/2019 01:59:12 - INFO - __main__ -     global_step = 7200\n",
      "11/12/2019 01:59:12 - INFO - __main__ -     loss = 0.0832\n",
      "================================================================================\n",
      "Best F1 0.804364010516205\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.1242:  99%|█████████████████████▋| 29599/30000 [3:19:06<01:49,  3.66it/s]11/12/2019 02:02:40 - INFO - __main__ -   ***** Report result *****\n",
      "11/12/2019 02:02:40 - INFO - __main__ -     global_step = 7400\n",
      "11/12/2019 02:02:40 - INFO - __main__ -     train loss = 0.1242\n",
      "11/12/2019 02:02:54 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/12/2019 02:02:54 - INFO - __main__ -     Num examples = 2938\n",
      "11/12/2019 02:02:54 - INFO - __main__ -     Batch size = 48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/12/2019 02:05:53 - INFO - __main__ -     eval_F1 = 0.8047367461511529\n",
      "11/12/2019 02:05:53 - INFO - __main__ -     eval_loss = 0.4704312004990155\n",
      "11/12/2019 02:05:53 - INFO - __main__ -     global_step = 7400\n",
      "11/12/2019 02:05:53 - INFO - __main__ -     loss = 0.1242\n",
      "================================================================================\n",
      "Best F1 0.8047367461511529\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.1081: 100%|██████████████████████| 30000/30000 [3:24:09<00:00,  2.45it/s]\n",
      "11/12/2019 02:07:43 - INFO - pytorch_transformers.modeling_utils -   loading weights file ../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_4/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "Traceback (most recent call last):\n",
      "  File \"./run_bert_2562_last2embedding_cls.py\", line 841, in <module>\n",
      "    main()\n",
      "  File \"./run_bert_2562_last2embedding_cls.py\", line 757, in main\n",
      "    logits = model(input_ids=input_ids, token_type_ids=segment_ids, attention_mask=input_mask).detach().cpu().numpy()\n",
      "  File \"/home/haizhi/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/haizhi/workspace/TODO/2019互联网新闻/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 1119, in forward\n",
      "    attention_mask=flat_attention_mask, head_mask=head_mask)\n",
      "  File \"/home/haizhi/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/haizhi/workspace/TODO/2019互联网新闻/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 727, in forward\n",
      "    head_mask=head_mask)\n",
      "  File \"/home/haizhi/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/haizhi/workspace/TODO/2019互联网新闻/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 440, in forward\n",
      "    layer_outputs = layer_module(hidden_states, attention_mask, head_mask[i])\n",
      "  File \"/home/haizhi/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/haizhi/workspace/TODO/2019互联网新闻/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 420, in forward\n",
      "    intermediate_output = self.intermediate(attention_output)\n",
      "  File \"/home/haizhi/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/haizhi/workspace/TODO/2019互联网新闻/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 392, in forward\n",
      "    hidden_states = self.intermediate_act_fn(hidden_states)\n",
      "  File \"/home/haizhi/workspace/TODO/2019互联网新闻/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 142, in gelu\n",
      "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 384.00 MiB (GPU 0; 10.92 GiB total capacity; 9.35 GiB already allocated; 191.00 MiB free; 810.30 MiB cached)\n"
     ]
    }
   ],
   "source": [
    "!python ./run_bert_2562_last2embedding_cls.py \\\n",
    "--model_type bert \\\n",
    "--model_name_or_path ../model/chinese_roberta_wwm_large_ext_pytorch  \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_test \\\n",
    "--data_dir ../data/data_StratifiedKFold_42/data_origin_4 \\\n",
    "--output_dir ../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_4 \\\n",
    "--max_seq_length 512 \\\n",
    "--split_num 1 \\\n",
    "--lstm_hidden_size 512 \\\n",
    "--lstm_layers 1 \\\n",
    "--lstm_dropout 0.1 \\\n",
    "--eval_steps 200 \\\n",
    "--per_gpu_train_batch_size 4 \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--warmup_steps 0 \\\n",
    "--per_gpu_eval_batch_size 48 \\\n",
    "--learning_rate 1e-5 \\\n",
    "--adam_epsilon 1e-6 \\\n",
    "--weight_decay 0 \\\n",
    "--train_steps 30000 \\\n",
    "--freeze 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/12/2019 09:57:19 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "11/12/2019 09:57:19 - INFO - pytorch_transformers.tokenization_utils -   Model name '../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_0' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming '../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_0' is a path or url to a directory containing tokenizer files.\n",
      "11/12/2019 09:57:19 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_0/added_tokens.json. We won't load it.\n",
      "11/12/2019 09:57:19 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_0/special_tokens_map.json. We won't load it.\n",
      "11/12/2019 09:57:19 - INFO - pytorch_transformers.tokenization_utils -   loading file ../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_0/vocab.txt\n",
      "11/12/2019 09:57:19 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/12/2019 09:57:19 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/12/2019 09:57:19 - INFO - pytorch_transformers.modeling_utils -   loading configuration file ../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_0/config.json\n",
      "11/12/2019 09:57:19 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 3,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "11/12/2019 09:57:19 - INFO - pytorch_transformers.modeling_utils -   loading weights file ../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_0/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "11/12/2019 09:57:36 - INFO - pytorch_transformers.modeling_utils -   loading weights file ../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_0/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "dev 0.8148304076709233\n",
      "/home/haizhi/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "test 0.06813008130081301\n"
     ]
    }
   ],
   "source": [
    "!cp ../model/chinese_roberta_wwm_large_ext_pytorch/vocab.txt \\\n",
    "../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_0/vocab.txt\n",
    "!cp ../model/chinese_roberta_wwm_large_ext_pytorch/config.json \\\n",
    "../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_0/config.json\n",
    "\n",
    "!python ./run_bert_2562_last2embedding_cls.py \\\n",
    "--model_type bert \\\n",
    "--model_name_or_path ../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_0  \\\n",
    "--do_test \\\n",
    "--data_dir ../data/data_StratifiedKFold_42/data_origin_0 \\\n",
    "--output_dir ../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_0 \\\n",
    "--max_seq_length 512 \\\n",
    "--split_num 1 \\\n",
    "--lstm_hidden_size 512 \\\n",
    "--lstm_layers 1 \\\n",
    "--lstm_dropout 0.1 \\\n",
    "--eval_steps 200 \\\n",
    "--per_gpu_train_batch_size 4 \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--warmup_steps 0 \\\n",
    "--per_gpu_eval_batch_size 64 \\\n",
    "--learning_rate 1e-5 \\\n",
    "--adam_epsilon 1e-6 \\\n",
    "--weight_decay 0 \\\n",
    "--train_steps 30000 \\\n",
    "--freeze 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/12/2019 10:08:43 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "11/12/2019 10:08:43 - INFO - pytorch_transformers.tokenization_utils -   Model name '../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming '../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_1' is a path or url to a directory containing tokenizer files.\n",
      "11/12/2019 10:08:43 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_1/added_tokens.json. We won't load it.\n",
      "11/12/2019 10:08:43 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_1/special_tokens_map.json. We won't load it.\n",
      "11/12/2019 10:08:43 - INFO - pytorch_transformers.tokenization_utils -   loading file ../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_1/vocab.txt\n",
      "11/12/2019 10:08:43 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/12/2019 10:08:43 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/12/2019 10:08:43 - INFO - pytorch_transformers.modeling_utils -   loading configuration file ../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_1/config.json\n",
      "11/12/2019 10:08:43 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 3,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "11/12/2019 10:08:43 - INFO - pytorch_transformers.modeling_utils -   loading weights file ../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_1/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "11/12/2019 10:09:00 - INFO - pytorch_transformers.modeling_utils -   loading weights file ../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_1/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "dev 0.807653769267959\n",
      "/home/haizhi/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "test 0.07110787525785706\n"
     ]
    }
   ],
   "source": [
    "!cp ../model/chinese_roberta_wwm_large_ext_pytorch/vocab.txt \\\n",
    "../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_1/vocab.txt\n",
    "!cp ../model/chinese_roberta_wwm_large_ext_pytorch/config.json \\\n",
    "../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_1/config.json\n",
    "\n",
    "!python ./run_bert_2562_last2embedding_cls.py \\\n",
    "--model_type bert \\\n",
    "--model_name_or_path ../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_1  \\\n",
    "--do_test \\\n",
    "--data_dir ../data/data_StratifiedKFold_42/data_origin_1 \\\n",
    "--output_dir ../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_1 \\\n",
    "--max_seq_length 512 \\\n",
    "--split_num 1 \\\n",
    "--lstm_hidden_size 512 \\\n",
    "--lstm_layers 1 \\\n",
    "--lstm_dropout 0.1 \\\n",
    "--eval_steps 200 \\\n",
    "--per_gpu_train_batch_size 4 \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--warmup_steps 0 \\\n",
    "--per_gpu_eval_batch_size 64 \\\n",
    "--learning_rate 1e-5 \\\n",
    "--adam_epsilon 1e-6 \\\n",
    "--weight_decay 0 \\\n",
    "--train_steps 30000 \\\n",
    "--freeze 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/12/2019 10:20:11 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "11/12/2019 10:20:11 - INFO - pytorch_transformers.tokenization_utils -   Model name '../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_2' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming '../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_2' is a path or url to a directory containing tokenizer files.\n",
      "11/12/2019 10:20:11 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_2/added_tokens.json. We won't load it.\n",
      "11/12/2019 10:20:11 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_2/special_tokens_map.json. We won't load it.\n",
      "11/12/2019 10:20:11 - INFO - pytorch_transformers.tokenization_utils -   loading file ../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_2/vocab.txt\n",
      "11/12/2019 10:20:11 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/12/2019 10:20:11 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/12/2019 10:20:11 - INFO - pytorch_transformers.modeling_utils -   loading configuration file ../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_2/config.json\n",
      "11/12/2019 10:20:11 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 3,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "11/12/2019 10:20:11 - INFO - pytorch_transformers.modeling_utils -   loading weights file ../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_2/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "11/12/2019 10:20:28 - INFO - pytorch_transformers.modeling_utils -   loading weights file ../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_2/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "dev 0.8047640702086638\n",
      "/home/haizhi/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "test 0.06578517793013386\n"
     ]
    }
   ],
   "source": [
    "!cp ../model/chinese_roberta_wwm_large_ext_pytorch/vocab.txt \\\n",
    "../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_2/vocab.txt\n",
    "!cp ../model/chinese_roberta_wwm_large_ext_pytorch/config.json \\\n",
    "../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_2/config.json\n",
    "\n",
    "!python ./run_bert_2562_last2embedding_cls.py \\\n",
    "--model_type bert \\\n",
    "--model_name_or_path ../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_2  \\\n",
    "--do_test \\\n",
    "--data_dir ../data/data_StratifiedKFold_42/data_origin_2 \\\n",
    "--output_dir ../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_2 \\\n",
    "--max_seq_length 512 \\\n",
    "--split_num 1 \\\n",
    "--lstm_hidden_size 512 \\\n",
    "--lstm_layers 1 \\\n",
    "--lstm_dropout 0.1 \\\n",
    "--eval_steps 200 \\\n",
    "--per_gpu_train_batch_size 4 \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--warmup_steps 0 \\\n",
    "--per_gpu_eval_batch_size 64 \\\n",
    "--learning_rate 1e-5 \\\n",
    "--adam_epsilon 1e-6 \\\n",
    "--weight_decay 0 \\\n",
    "--train_steps 30000 \\\n",
    "--freeze 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/12/2019 10:31:40 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "11/12/2019 10:31:40 - INFO - pytorch_transformers.tokenization_utils -   Model name '../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_3' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming '../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_3' is a path or url to a directory containing tokenizer files.\n",
      "11/12/2019 10:31:40 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_3/added_tokens.json. We won't load it.\n",
      "11/12/2019 10:31:40 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_3/special_tokens_map.json. We won't load it.\n",
      "11/12/2019 10:31:40 - INFO - pytorch_transformers.tokenization_utils -   loading file ../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_3/vocab.txt\n",
      "11/12/2019 10:31:40 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/12/2019 10:31:40 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/12/2019 10:31:40 - INFO - pytorch_transformers.modeling_utils -   loading configuration file ../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_3/config.json\n",
      "11/12/2019 10:31:40 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 3,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "11/12/2019 10:31:40 - INFO - pytorch_transformers.modeling_utils -   loading weights file ../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_3/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "11/12/2019 10:31:57 - INFO - pytorch_transformers.modeling_utils -   loading weights file ../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_3/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "dev 0.8056914365603444\n",
      "/home/haizhi/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "test 0.08224180360403271\n"
     ]
    }
   ],
   "source": [
    "!cp ../model/chinese_roberta_wwm_large_ext_pytorch/vocab.txt \\\n",
    "../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_3/vocab.txt\n",
    "!cp ../model/chinese_roberta_wwm_large_ext_pytorch/config.json \\\n",
    "../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_3/config.json\n",
    "\n",
    "!python ./run_bert_2562_last2embedding_cls.py \\\n",
    "--model_type bert \\\n",
    "--model_name_or_path ../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_3  \\\n",
    "--do_test \\\n",
    "--data_dir ../data/data_StratifiedKFold_42/data_origin_3 \\\n",
    "--output_dir ../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_3 \\\n",
    "--max_seq_length 512 \\\n",
    "--split_num 1 \\\n",
    "--lstm_hidden_size 512 \\\n",
    "--lstm_layers 1 \\\n",
    "--lstm_dropout 0.1 \\\n",
    "--eval_steps 200 \\\n",
    "--per_gpu_train_batch_size 4 \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--warmup_steps 0 \\\n",
    "--per_gpu_eval_batch_size 64 \\\n",
    "--learning_rate 1e-5 \\\n",
    "--adam_epsilon 1e-6 \\\n",
    "--weight_decay 0 \\\n",
    "--train_steps 30000 \\\n",
    "--freeze 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/12/2019 10:43:12 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "11/12/2019 10:43:12 - INFO - pytorch_transformers.tokenization_utils -   Model name '../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_4' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming '../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_4' is a path or url to a directory containing tokenizer files.\n",
      "11/12/2019 10:43:12 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_4/added_tokens.json. We won't load it.\n",
      "11/12/2019 10:43:12 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_4/special_tokens_map.json. We won't load it.\n",
      "11/12/2019 10:43:12 - INFO - pytorch_transformers.tokenization_utils -   loading file ../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_4/vocab.txt\n",
      "11/12/2019 10:43:12 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/12/2019 10:43:12 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/12/2019 10:43:12 - INFO - pytorch_transformers.modeling_utils -   loading configuration file ../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_4/config.json\n",
      "11/12/2019 10:43:12 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 3,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "11/12/2019 10:43:12 - INFO - pytorch_transformers.modeling_utils -   loading weights file ../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_4/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "11/12/2019 10:43:29 - INFO - pytorch_transformers.modeling_utils -   loading weights file ../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_4/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "dev 0.8047367461511529\n",
      "/home/haizhi/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "test 0.06994934143870314\n"
     ]
    }
   ],
   "source": [
    "!cp ../model/chinese_roberta_wwm_large_ext_pytorch/vocab.txt \\\n",
    "../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_4/vocab.txt\n",
    "!cp ../model/chinese_roberta_wwm_large_ext_pytorch/config.json \\\n",
    "../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_4/config.json\n",
    "\n",
    "!python ./run_bert_2562_last2embedding_cls.py \\\n",
    "--model_type bert \\\n",
    "--model_name_or_path ../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_4  \\\n",
    "--do_test \\\n",
    "--data_dir ../data/data_StratifiedKFold_42/data_origin_4 \\\n",
    "--output_dir ../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_4 \\\n",
    "--max_seq_length 512 \\\n",
    "--split_num 1 \\\n",
    "--lstm_hidden_size 512 \\\n",
    "--lstm_layers 1 \\\n",
    "--lstm_dropout 0.1 \\\n",
    "--eval_steps 200 \\\n",
    "--per_gpu_train_batch_size 4 \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--warmup_steps 0 \\\n",
    "--per_gpu_eval_batch_size 64 \\\n",
    "--learning_rate 1e-5 \\\n",
    "--adam_epsilon 1e-6 \\\n",
    "--weight_decay 0 \\\n",
    "--train_steps 30000 \\\n",
    "--freeze 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提交"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12159446052945852\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df = pd.read_csv('../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_4/sub.csv')\n",
    "df = df[['id']]\n",
    "df['0'] = 0\n",
    "df['1'] = 0\n",
    "df['2'] = 0\n",
    "\n",
    "k = 5 \n",
    "for i in [0,1,2,3,4]:\n",
    "    temp=pd.read_csv('../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_{}/sub.csv'.format(i))\n",
    "    df['0']+=temp['label_0']/k\n",
    "    df['1']+=temp['label_1']/k\n",
    "    df['2']+=temp['label_2']/k\n",
    "print(df['0'].mean())\n",
    "\n",
    "df['label']=np.argmax(df[['0','1','2']].values,-1)\n",
    "df[['id','label']].to_csv('../model/roberta_wwm_large_512_1_last2embedding_cls/roberta_wwm_large_512_1_last2embedding_cls_sub.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
