{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/22/2019 11:27:56 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "11/22/2019 11:27:56 - INFO - pytorch_transformers.tokenization_utils -   Model name '../model/chinese_roberta_wwm_large_ext_pytorch' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming '../model/chinese_roberta_wwm_large_ext_pytorch' is a path or url to a directory containing tokenizer files.\n",
      "11/22/2019 11:27:56 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../model/chinese_roberta_wwm_large_ext_pytorch/added_tokens.json. We won't load it.\n",
      "11/22/2019 11:27:56 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../model/chinese_roberta_wwm_large_ext_pytorch/special_tokens_map.json. We won't load it.\n",
      "11/22/2019 11:27:56 - INFO - pytorch_transformers.tokenization_utils -   loading file ../model/chinese_roberta_wwm_large_ext_pytorch/vocab.txt\n",
      "11/22/2019 11:27:56 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/22/2019 11:27:56 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/22/2019 11:27:56 - INFO - pytorch_transformers.modeling_utils -   loading configuration file ../model/chinese_roberta_wwm_large_ext_pytorch/config.json\n",
      "11/22/2019 11:27:56 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 3,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "11/22/2019 11:27:56 - INFO - pytorch_transformers.modeling_utils -   loading weights file ../model/chinese_roberta_wwm_large_ext_pytorch/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "11/22/2019 11:28:02 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'W.0.weight', 'W.0.bias', 'gru.0.weight_ih_l0', 'gru.0.weight_hh_l0', 'gru.0.bias_ih_l0', 'gru.0.bias_hh_l0', 'gru.0.weight_ih_l0_reverse', 'gru.0.weight_hh_l0_reverse', 'gru.0.bias_ih_l0_reverse', 'gru.0.bias_hh_l0_reverse']\n",
      "11/22/2019 11:28:02 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "11/22/2019 11:28:03 - INFO - __main__ -   *** Example ***\n",
      "11/22/2019 11:28:03 - INFO - __main__ -   idx: 0\n",
      "11/22/2019 11:28:03 - INFO - __main__ -   guid: 7a3dd79f90ee419da87190cff60f7a86\n",
      "11/22/2019 11:28:03 - INFO - __main__ -   tokens: [CLS] 问 责 领 导 ( 上 黄 镇 党 委 书 记 张 涛 ， 宣 国 才 真 能 一 手 遮 天 吗 ？ ) [SEP] 这 几 天 看 了 有 人 举 报 施 某 某 的 贴 子 ， 经 与 举 报 人 联 系 证 实 ， 是 宣 某 当 天 中 午 请 举 报 人 和 枪 手 喝 酒 后 ， 晚 上 才 发 的 贴 子 ！ 本 人 不 去 讨 论 前 二 天 的 举 报 ， 相 信 总 归 会 有 说 法 的 ！ 今 天 一 看 施 全 军 2017 年 1 月 2 日 实 名 举 报 上 黄 镇 宣 国 才 的 贴 子 （ 仍 被 锁 定 禁 止 评 论 ） 已 经 正 好 一 整 年 了 = 750 ) window . open ( ' http : / / img . js ##ly ##001 . com / at ##ta ##ch ##ment / mon _ 180 ##1 / 4 _ 291 ##08 ##5 _ c ##79 ##6 ##a ##6 ##a ##86 ##e ##17 ##12 ##1 . jpg ? 123 ' ) ; \" on ##load = \" if ( this . off ##set ##wi ##dt ##h > ' 750 ' ) this . wi ##dt ##h = ' 750 ' ; \" sr ##c = \" http : / / img . js ##ly ##001 . com / at ##ta ##ch ##ment / mon _ [SEP]\n",
      "11/22/2019 11:28:03 - INFO - __main__ -   input_ids: 101 7309 6569 7566 2193 113 677 7942 7252 1054 1999 741 6381 2476 3875 8024 2146 1744 2798 4696 5543 671 2797 6902 1921 1408 8043 114 102 6821 1126 1921 4692 749 3300 782 715 2845 3177 3378 3378 4638 6585 2094 8024 5307 680 715 2845 782 5468 5143 6395 2141 8024 3221 2146 3378 2496 1921 704 1286 6435 715 2845 782 1469 3366 2797 1600 6983 1400 8024 3241 677 2798 1355 4638 6585 2094 8013 3315 782 679 1343 6374 6389 1184 753 1921 4638 715 2845 8024 4685 928 2600 2495 833 3300 6432 3791 4638 8013 791 1921 671 4692 3177 1059 1092 8109 2399 122 3299 123 3189 2141 1399 715 2845 677 7942 7252 2146 1744 2798 4638 6585 2094 8020 793 6158 7219 2137 4881 3632 6397 6389 8021 2347 5307 3633 1962 671 3146 2399 749 134 9180 114 12158 119 8893 113 112 8184 131 120 120 11412 119 9016 8436 9942 119 8134 120 8243 8383 8370 8631 120 8556 142 8420 8148 120 125 142 11777 9153 8157 142 145 9495 8158 8139 8158 8139 9219 8154 8408 8455 8148 119 9248 136 8604 112 114 132 107 8281 11713 134 107 8898 113 8554 119 9594 9852 10958 12672 8199 135 112 9180 112 114 8554 119 8541 12672 8199 134 112 9180 112 132 107 12109 8177 134 107 8184 131 120 120 11412 119 9016 8436 9942 119 8134 120 8243 8383 8370 8631 120 8556 142 102\n",
      "11/22/2019 11:28:03 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/22/2019 11:28:03 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/22/2019 11:28:03 - INFO - __main__ -   label: 2\n",
      "11/22/2019 11:28:03 - INFO - __main__ -   *** Example ***\n",
      "11/22/2019 11:28:03 - INFO - __main__ -   idx: 0\n",
      "11/22/2019 11:28:03 - INFO - __main__ -   guid: 7a3dd79f90ee419da87190cff60f7a86\n",
      "11/22/2019 11:28:03 - INFO - __main__ -   tokens: [CLS] 问 责 领 导 ( 上 黄 镇 党 委 书 记 张 涛 ， 宣 国 才 真 能 一 手 遮 天 吗 ？ ) [SEP] / www / data / tm ##p / q ##fu ##pl ##oa ##d / 4 _ 291 ##08 ##5 _ 151 ##49 ##81 ##47 ##23 ##53 ##07 ##5 . jpg 一 年 的 贴 子 ， 再 次 被 网 友 顶 起 来 后 ， 才 发 现 施 某 几 天 前 回 复 网 友 的 处 理 结 果 竟 如 下 图 ： = 750 ) window . open ( ' http : / / img . js ##ly ##001 . com / at ##ta ##ch ##ment / mon _ 180 ##1 / 4 _ 291 ##08 ##5 _ 9 ##d ##32 ##ee ##57 ##27 ##60 ##d ##85 . jpg ? 131 ' ) ; \" on ##load = \" if ( this . off ##set ##wi ##dt ##h > ' 750 ' ) this . wi ##dt ##h = ' 750 ' ; \" sr ##c = \" http : / / img . js ##ly ##001 . com / at ##ta ##ch ##ment / mon _ 180 ##1 / 4 _ 291 ##08 ##5 _ 9 ##d ##32 ##ee ##57 ##27 ##60 ##d ##85 . jpg ? 131 \" style = \" max - wi ##dt ##h : 750 ##px ; \" / > 图 片 : / home / al ##ida ##ta / www / data / tm [SEP]\n",
      "11/22/2019 11:28:03 - INFO - __main__ -   input_ids: 101 7309 6569 7566 2193 113 677 7942 7252 1054 1999 741 6381 2476 3875 8024 2146 1744 2798 4696 5543 671 2797 6902 1921 1408 8043 114 102 120 8173 120 9000 120 9908 8187 120 159 12043 12569 11355 8168 120 125 142 11777 9153 8157 142 9564 9500 9313 9050 8748 9310 9131 8157 119 9248 671 2399 4638 6585 2094 8024 1086 3613 6158 5381 1351 7553 6629 3341 1400 8024 2798 1355 4385 3177 3378 1126 1921 1184 1726 1908 5381 1351 4638 1905 4415 5310 3362 4994 1963 678 1745 8038 134 9180 114 12158 119 8893 113 112 8184 131 120 120 11412 119 9016 8436 9942 119 8134 120 8243 8383 8370 8631 120 8556 142 8420 8148 120 125 142 11777 9153 8157 142 130 8168 8709 8854 9647 8976 8581 8168 9169 119 9248 136 9403 112 114 132 107 8281 11713 134 107 8898 113 8554 119 9594 9852 10958 12672 8199 135 112 9180 112 114 8554 119 8541 12672 8199 134 112 9180 112 132 107 12109 8177 134 107 8184 131 120 120 11412 119 9016 8436 9942 119 8134 120 8243 8383 8370 8631 120 8556 142 8420 8148 120 125 142 11777 9153 8157 142 130 8168 8709 8854 9647 8976 8581 8168 9169 119 9248 136 9403 107 8969 134 107 8621 118 8541 12672 8199 131 9180 10605 132 107 120 135 1745 4275 131 120 8563 120 9266 12708 8383 120 8173 120 9000 120 9908 102\n",
      "11/22/2019 11:28:03 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/22/2019 11:28:03 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/22/2019 11:28:03 - INFO - __main__ -   label: 2\n"
     ]
    }
   ],
   "source": [
    "!python ./run_bert_2562.py \\\n",
    "--model_type bert \\\n",
    "--model_name_or_path ../model/chinese_roberta_wwm_large_ext_pytorch  \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_test \\\n",
    "--data_dir ../data/data_StratifiedKFold_42/data_origin_0 \\\n",
    "--output_dir ../model/roberta_wwm_large_2562_gru1_42/roberta_wwm_large_2562_gru1_0 \\\n",
    "--max_seq_length 256 \\\n",
    "--split_num 2 \\\n",
    "--lstm_hidden_size 512 \\\n",
    "--lstm_layers 1 \\\n",
    "--lstm_dropout 0.1 \\\n",
    "--eval_steps 200 \\\n",
    "--per_gpu_train_batch_size 4 \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--warmup_steps 0 \\\n",
    "--per_gpu_eval_batch_size 32 \\\n",
    "--learning_rate 5e-6 \\\n",
    "--adam_epsilon 1e-6 \\\n",
    "--weight_decay 0 \\\n",
    "--train_steps 20000 \\\n",
    "--freeze 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base:  首尾\n",
    "!python ./run_bert_2562.py \\\n",
    "--model_type bert \\\n",
    "--model_name_or_path ../model/chinese_roberta_wwm_large_ext_pytorch  \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_test \\\n",
    "--data_dir ../data/data_StratifiedKFold_42/data_origin_1 \\\n",
    "--output_dir ../model/roberta_wwm_large_2562_gru1_42/roberta_wwm_large_2562_gru1_1 \\\n",
    "--max_seq_length 256 \\\n",
    "--split_num 2 \\\n",
    "--lstm_hidden_size 512 \\\n",
    "--lstm_layers 1 \\\n",
    "--lstm_dropout 0.1 \\\n",
    "--eval_steps 200 \\\n",
    "--per_gpu_train_batch_size 4 \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--warmup_steps 0 \\\n",
    "--per_gpu_eval_batch_size 32 \\\n",
    "--learning_rate 5e-6 \\\n",
    "--adam_epsilon 1e-6 \\\n",
    "--weight_decay 0 \\\n",
    "--train_steps 20000 \\\n",
    "--freeze 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base:  首尾\n",
    "!python ./run_bert_2562.py \\\n",
    "--model_type bert \\\n",
    "--model_name_or_path ../model/chinese_roberta_wwm_large_ext_pytorch  \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_test \\\n",
    "--data_dir ../data/data_StratifiedKFold_42/data_origin_2 \\\n",
    "--output_dir ../model/roberta_wwm_large_2562_gru1_42/roberta_wwm_large_2562_gru1_2 \\\n",
    "--max_seq_length 256 \\\n",
    "--split_num 2 \\\n",
    "--lstm_hidden_size 512 \\\n",
    "--lstm_layers 1 \\\n",
    "--lstm_dropout 0.1 \\\n",
    "--eval_steps 200 \\\n",
    "--per_gpu_train_batch_size 4 \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--warmup_steps 0 \\\n",
    "--per_gpu_eval_batch_size 32 \\\n",
    "--learning_rate 5e-6 \\\n",
    "--adam_epsilon 1e-6 \\\n",
    "--weight_decay 0 \\\n",
    "--train_steps 20000 \\\n",
    "--freeze 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base:  首尾\n",
    "!python ./run_bert_2562.py \\\n",
    "--model_type bert \\\n",
    "--model_name_or_path ../model/chinese_roberta_wwm_large_ext_pytorch  \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_test \\\n",
    "--data_dir ../data/data_StratifiedKFold_42/data_origin_3 \\\n",
    "--output_dir ../model/roberta_wwm_large_2562_gru1_42/roberta_wwm_large_2562_gru1_3 \\\n",
    "--max_seq_length 256 \\\n",
    "--split_num 2 \\\n",
    "--lstm_hidden_size 512 \\\n",
    "--lstm_layers 1 \\\n",
    "--lstm_dropout 0.1 \\\n",
    "--eval_steps 200 \\\n",
    "--per_gpu_train_batch_size 4 \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--warmup_steps 0 \\\n",
    "--per_gpu_eval_batch_size 32 \\\n",
    "--learning_rate 5e-6 \\\n",
    "--adam_epsilon 1e-6 \\\n",
    "--weight_decay 0 \\\n",
    "--train_steps 20000 \\\n",
    "--freeze 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base:  首尾\n",
    "!python ./run_bert_2562.py \\\n",
    "--model_type bert \\\n",
    "--model_name_or_path ../model/chinese_roberta_wwm_large_ext_pytorch  \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_test \\\n",
    "--data_dir ../data/data_StratifiedKFold_42/data_origin_4 \\\n",
    "--output_dir ../model/roberta_wwm_large_2562_gru1_42/roberta_wwm_large_2562_gru1_4 \\\n",
    "--max_seq_length 256 \\\n",
    "--split_num 2 \\\n",
    "--lstm_hidden_size 512 \\\n",
    "--lstm_layers 1 \\\n",
    "--lstm_dropout 0.1 \\\n",
    "--eval_steps 200 \\\n",
    "--per_gpu_train_batch_size 4 \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--warmup_steps 0 \\\n",
    "--per_gpu_eval_batch_size 32 \\\n",
    "--learning_rate 5e-6 \\\n",
    "--adam_epsilon 1e-6 \\\n",
    "--weight_decay 0 \\\n",
    "--train_steps 20000 \\\n",
    "--freeze 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11444118823885764\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df = pd.read_csv('../model/roberta_wwm_large_2562_gru1_42/roberta_wwm_large_2562_gru1_0/sub.csv')\n",
    "df = df[['id']]\n",
    "df['0'] = 0\n",
    "df['1'] = 0\n",
    "df['2'] = 0\n",
    "\n",
    "k=5\n",
    "for i in [0,1,2,3,4]:\n",
    "    temp=pd.read_csv('../model/roberta_wwm_large_2562_gru1_42/roberta_wwm_large_2562_gru1_{}/sub.csv'.format(i))\n",
    "    df['0']+=temp['label_0']/k\n",
    "    df['1']+=temp['label_1']/k\n",
    "    df['2']+=temp['label_2']/k\n",
    "print(df['0'].mean())\n",
    "\n",
    "df['label']=np.argmax(df[['0','1','2']].values,-1)\n",
    "df[['id','label']].to_csv('../model/roberta_wwm_large_2562_gru1_42/roberta_wwm_large_2562_gru1_42_sub.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
