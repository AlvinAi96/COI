{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/13/2019 22:41:25 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "11/13/2019 22:41:25 - INFO - pytorch_transformers.tokenization_utils -   Model name '../model/chinese_roberta_wwm_large_ext_pytorch' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming '../model/chinese_roberta_wwm_large_ext_pytorch' is a path or url to a directory containing tokenizer files.\n",
      "11/13/2019 22:41:25 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../model/chinese_roberta_wwm_large_ext_pytorch/added_tokens.json. We won't load it.\n",
      "11/13/2019 22:41:25 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../model/chinese_roberta_wwm_large_ext_pytorch/special_tokens_map.json. We won't load it.\n",
      "11/13/2019 22:41:25 - INFO - pytorch_transformers.tokenization_utils -   loading file ../model/chinese_roberta_wwm_large_ext_pytorch/vocab.txt\n",
      "11/13/2019 22:41:25 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/13/2019 22:41:25 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/13/2019 22:41:25 - INFO - pytorch_transformers.modeling_utils -   loading configuration file ../model/chinese_roberta_wwm_large_ext_pytorch/config.json\n",
      "11/13/2019 22:41:25 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 3,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "11/13/2019 22:41:25 - INFO - pytorch_transformers.modeling_utils -   loading weights file ../model/chinese_roberta_wwm_large_ext_pytorch/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "11/13/2019 22:41:34 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification_last2embedding_cls not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "11/13/2019 22:41:34 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification_last2embedding_cls: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "11/13/2019 22:41:40 - INFO - __main__ -   ** RAW EXAMPLE **\n",
      "11/13/2019 22:41:40 - INFO - __main__ -   content: ['这', '几', '天', '看', '了', '有', '人', '举', '报', '施', '某', '某', '的', '贴', '子', '，', '经', '与', '举', '报', '人', '联', '系', '证', '实', '，', '是', '宣', '某', '当', '天', '中', '午', '请', '举', '报', '人', '和', '枪', '手', '喝', '酒', '后', '，', '晚', '上', '才', '发', '的', '贴', '子', '！', '本', '人', '不', '去', '讨', '论', '前', '二', '天', '的', '举', '报', '，', '相', '信', '总', '归', '会', '有', '说', '法', '的', '！', '今', '天', '一', '看', '施', '全', '军', '2017', '年', '1', '月', '2', '日', '实', '名', '举', '报', '上', '黄', '镇', '宣', '国', '才', '的', '贴', '子', '（', '仍', '被', '锁', '定', '禁', '止', '评', '论', '）', '已', '经', '正', '好', '一', '整', '年', '了', '750', '001', '##18', '##014', '##29', '##10', '##85', '##79', '##66', '##86', '##17', '##12', '##11', '##23', '750', '##750', '001', '##18', '##014', '##29', '##10', '##85', '##79', '##66', '##86', '##17', '##12', '##11', '##23', '750', '图', '片', '：', '429', '##10', '##85', '##15', '##14', '##98', '##14', '##71', '##47', '##89', '##52', '施', '全', '军', '实', '名', '举', '报', '50', '天', '后', '，', '上', '黄', '镇', '党', '委', '政', '府', '回', '复', '如', '下', '图', '：', '750', '001', '##18', '##014', '##29', '##10', '##85', '##91', '##17', '##21', '##99', '##0', '750', '##750', '001', '##18', '##014', '##29', '##10', '##85', '##91', '##17', '##21', '##99', '##0', '750', '图', '片', '：', '429', '##10', '##85', '##15', '##14', '##98', '##14', '##72', '##63', '##16', '##68', '750', '001', '##18', '##014', '##29', '##10', '##85', '##99', '##39', '##43', '##20', '##75', '750', '##750', '001', '##18', '##014', '##29', '##10', '##85', '##99', '##39', '##43', '##20', '##75', '750', '图', '片', '：', '429', '##10', '##85', '##15', '##14', '##98', '##14', '##72', '##35', '##30', '##75', '一', '年', '的', '贴', '子', '，', '再', '次', '被', '网', '友', '顶', '起', '来', '后', '，', '才', '发', '现', '施', '某', '几', '天', '前', '回', '复', '网', '友', '的', '处', '理', '结', '果', '竟', '如', '下', '图', '：', '750', '001', '##18', '##014', '##29', '##10', '##85', '##93', '##25', '##72', '##76', '##08', '##51', '##31', '750', '##750', '001', '##18', '##014', '##29', '##10', '##85', '##93', '##25', '##72', '##76', '##08', '##51', '##31', '750', '图', '片', '：', '429', '##10', '##85', '##15', '##14', '##98', '##14', '##73', '##54', '##71', '##72', '现', '责', '问', '张', '涛', '书', '记', '：', '1', '、', '宣', '国', '才', '被', '举', '报', '这', '么', '多', '问', '题', '，', '什', '么', '时', '候', '有', '答', '复', '。', '2', '、', '宣', '国', '才', '被', '举', '报', '后', '，', '为', '什', '么', '被', '立', '刻', '免', '了', '村', '书', '记', '职', '务', '？', '为', '什', '么', '又', '被', '安', '排', '到', '城', '管', '队', '[UNK]', '吃', '空', '响', '[UNK]', '，', '自', '己', '却', '天', '天', '在', '我', '们', '水', '泥', '厂', '上', '班', '赚', '黑', '钱', '？', '3', '、', '这', '几', '个', '月', '，', '水', '泥', '每', '吨', '近', '200', '元', '纯', '利', '润', '，', '还', '供', '不', '应', '求', '，', '宣', '国', '才', '还', '清', '上', '黄', '政', '府', '担', '保', '借', '给', '宣', '国', '才', '代', '付', '振', '东', '厂', '工', '资', '社', '保', '的', '钱', '了', '吗', '？', '4', '、', '据', '了', '解', '宣', '国', '才', '占', '他', '人', '企', '业', '经', '营', '，', '又', '欠', '税', '521', '##6', '万', '元', '、', '欠', '社', '保', '327', '##6', '万', '元', '、', '应', '该', '还', '欠', '了', '职', '工', '工', '资', '几', '十', '万', '，', '上', '黄', '政', '府', '打', '算', '替', '宣', '国', '才', '担', '保', '还', '是', '归', '还', '？', '5', '、', '我', '们', '厂', '合', '法', '会', '计', '和', '老', '板', '被', '判', '刑', '四', '到', '六', '年', '，', '现', '在', '服', '刑', '。', '厂', '子', '给', '宣', '国', '才', '强', '占', '，', '宣', '国', '才', '每', '天', '赚', '20', '多', '万', '净', '利', '润', '，', '却', '对', '外', '宣', '称', '天', '天', '亏', '本', '！', '等', '咱', '老', '板', '刑', '满', '回', '厂', '，', '宣', '国', '才', '给', '咱', '厂', '[UNK]', '天', '天', '亏', '[UNK]', '可', '能', '要', '[UNK]', '亏', '[UNK]', '的', '几', '千', '万', '元', '，', '甚', '至', '几', '个', '亿', '，', '张', '涛', '书', '记', '您', '承', '担', '还', '是', '上', '黄', '政', '府', '承', '担', '？', '当', '初', '可', '是', '您', '亲', '自', '把', '厂', '交', '给', '宣', '国', '才', '生', '产', '的', '！', '希', '望', '徐', '市', '长', '看', '到', '本', '贴', '后', '能', '像', '批', '示', '263', '、', '批', '示', '违', '建', '等', '民', '生', '问', '题', '一', '样', '，', '关', '注', '一', '下', '我', '们', '水', '泥', '厂', '的', '将', '来', '！', '也', '请', '徐', '市', '长', '抽', '日', '理', '万', '机', '之', '空', '亲', '自', '约', '谈', '一', '下', '当', '事', '人', '（', '特', '别', '是', '那', '位', '施', '站', '长', '）', '，', '千', '万', '不', '能', '听', '取', '一', '面', '之', '辞', '！']\n",
      "11/13/2019 22:41:40 - INFO - __main__ -   *** Example ***\n",
      "11/13/2019 22:41:40 - INFO - __main__ -   idx: 0\n",
      "11/13/2019 22:41:40 - INFO - __main__ -   guid: 7a3dd79f90ee419da87190cff60f7a86\n",
      "11/13/2019 22:41:40 - INFO - __main__ -   tokens: [CLS] 问 责 领 导 上 黄 镇 党 委 书 记 张 涛 ， 宣 国 才 真 能 一 手 遮 天 吗 ？ [SEP] 后 ， 才 发 现 施 某 几 天 前 回 复 网 友 的 处 理 结 果 竟 如 下 图 ： 750 001 ##18 ##014 ##29 ##10 ##85 ##93 ##25 ##72 ##76 ##08 ##51 ##31 750 ##750 001 ##18 ##014 ##29 ##10 ##85 ##93 ##25 ##72 ##76 ##08 ##51 ##31 750 图 片 ： 429 ##10 ##85 ##15 ##14 ##98 ##14 ##73 ##54 ##71 ##72 现 责 问 张 涛 书 记 ： 1 、 宣 国 才 被 举 报 这 么 多 问 题 ， 什 么 时 候 有 答 复 。 2 、 宣 国 才 被 举 报 后 ， 为 什 么 被 立 刻 免 了 村 书 记 职 务 ？ 为 什 么 又 被 安 排 到 城 管 队 [UNK] 吃 空 响 [UNK] ， 自 己 却 天 天 在 我 们 水 泥 厂 上 班 赚 黑 钱 ？ 3 、 这 几 个 月 ， 水 泥 每 吨 近 200 元 纯 利 润 ， 还 供 不 应 求 ， 宣 国 才 还 清 上 黄 政 府 担 保 借 给 宣 国 才 代 付 振 东 厂 工 资 社 保 的 钱 了 吗 ？ 4 、 据 了 解 宣 国 才 占 他 人 企 业 经 营 ， 又 欠 税 521 ##6 万 元 、 欠 社 保 327 ##6 万 元 、 应 该 还 欠 了 职 工 工 资 几 十 万 ， 上 黄 政 府 打 算 替 宣 国 才 担 保 还 是 归 还 ？ 5 、 我 们 厂 合 法 会 计 和 老 板 被 判 刑 四 到 六 年 ， 现 在 服 刑 。 厂 子 给 宣 国 才 强 占 ， 宣 国 才 每 天 赚 20 多 万 净 利 润 ， 却 对 外 宣 称 天 天 亏 本 ！ 等 咱 老 板 刑 满 回 厂 ， 宣 国 才 给 咱 厂 [UNK] 天 天 亏 [UNK] 可 能 要 [UNK] 亏 [UNK] 的 几 千 万 元 ， 甚 至 几 个 亿 ， 张 涛 书 记 您 承 担 还 是 上 黄 政 府 承 担 ？ 当 初 可 是 您 亲 自 把 厂 交 给 宣 国 才 生 产 的 ！ 希 望 徐 市 长 看 到 本 贴 后 能 像 批 示 263 、 批 示 违 建 等 民 生 问 题 一 样 ， 关 注 一 下 我 们 水 泥 厂 的 将 来 ！ 也 请 徐 市 长 抽 日 理 万 机 之 空 亲 自 约 谈 一 下 当 事 人 （ 特 别 是 那 位 施 站 长 ） ， 千 万 不 能 听 取 一 面 之 辞 [SEP]\n",
      "11/13/2019 22:41:40 - INFO - __main__ -   input_ids: 101 7309 6569 7566 2193 677 7942 7252 1054 1999 741 6381 2476 3875 8024 2146 1744 2798 4696 5543 671 2797 6902 1921 1408 8043 102 1400 8024 2798 1355 4385 3177 3378 1126 1921 1184 1726 1908 5381 1351 4638 1905 4415 5310 3362 4994 1963 678 1745 8038 9180 9263 8662 11365 8887 8311 9169 9676 8743 9492 9624 9153 9216 8805 9180 10969 9263 8662 11365 8887 8311 9169 9676 8743 9492 9624 9153 9216 8805 9180 1745 4275 8038 13249 8311 9169 8493 8717 9215 8717 9148 9488 9097 9492 4385 6569 7309 2476 3875 741 6381 8038 122 510 2146 1744 2798 6158 715 2845 6821 720 1914 7309 7579 8024 784 720 3198 952 3300 5031 1908 511 123 510 2146 1744 2798 6158 715 2845 1400 8024 711 784 720 6158 4989 1174 1048 749 3333 741 6381 5466 1218 8043 711 784 720 1348 6158 2128 2961 1168 1814 5052 7339 100 1391 4958 1510 100 8024 5632 2346 1316 1921 1921 1762 2769 812 3717 3799 1322 677 4408 6611 7946 7178 8043 124 510 6821 1126 702 3299 8024 3717 3799 3680 1417 6818 8185 1039 5283 1164 3883 8024 6820 897 679 2418 3724 8024 2146 1744 2798 6820 3926 677 7942 3124 2424 2857 924 955 5314 2146 1744 2798 807 802 2920 691 1322 2339 6598 4852 924 4638 7178 749 1408 8043 125 510 2945 749 6237 2146 1744 2798 1304 800 782 821 689 5307 5852 8024 1348 3612 4925 11411 8158 674 1039 510 3612 4852 924 12094 8158 674 1039 510 2418 6421 6820 3612 749 5466 2339 2339 6598 1126 1282 674 8024 677 7942 3124 2424 2802 5050 3296 2146 1744 2798 2857 924 6820 3221 2495 6820 8043 126 510 2769 812 1322 1394 3791 833 6369 1469 5439 3352 6158 1161 1152 1724 1168 1063 2399 8024 4385 1762 3302 1152 511 1322 2094 5314 2146 1744 2798 2487 1304 8024 2146 1744 2798 3680 1921 6611 8113 1914 674 1112 1164 3883 8024 1316 2190 1912 2146 4917 1921 1921 755 3315 8013 5023 1493 5439 3352 1152 4007 1726 1322 8024 2146 1744 2798 5314 1493 1322 100 1921 1921 755 100 1377 5543 6206 100 755 100 4638 1126 1283 674 1039 8024 4493 5635 1126 702 783 8024 2476 3875 741 6381 2644 2824 2857 6820 3221 677 7942 3124 2424 2824 2857 8043 2496 1159 1377 3221 2644 779 5632 2828 1322 769 5314 2146 1744 2798 4495 772 4638 8013 2361 3307 2528 2356 7270 4692 1168 3315 6585 1400 5543 1008 2821 4850 10864 510 2821 4850 6824 2456 5023 3696 4495 7309 7579 671 3416 8024 1068 3800 671 678 2769 812 3717 3799 1322 4638 2199 3341 8013 738 6435 2528 2356 7270 2853 3189 4415 674 3322 722 4958 779 5632 5276 6448 671 678 2496 752 782 8020 4294 1166 3221 6929 855 3177 4991 7270 8021 8024 1283 674 679 5543 1420 1357 671 7481 722 6791 102\n",
      "11/13/2019 22:41:40 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/13/2019 22:41:40 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/13/2019 22:41:40 - INFO - __main__ -   label: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/13/2019 22:43:13 - INFO - __main__ -   ***** Running training *****\n",
      "11/13/2019 22:43:13 - INFO - __main__ -     Num examples = 11755\n",
      "11/13/2019 22:43:13 - INFO - __main__ -     Batch size = 4\n",
      "11/13/2019 22:43:13 - INFO - __main__ -     Num steps = 30000\n",
      "  0%|                                                 | 0/30000 [00:00<?, ?it/s]11/13/2019 22:43:36 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/13/2019 22:43:36 - INFO - __main__ -     Num examples = 2941\n",
      "11/13/2019 22:43:36 - INFO - __main__ -     Batch size = 48\n",
      "/root/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "11/13/2019 22:45:41 - INFO - __main__ -     eval_F1 = 0.2074806812414847\n",
      "11/13/2019 22:45:41 - INFO - __main__ -     eval_loss = 1.3194139032594618\n",
      "11/13/2019 22:45:41 - INFO - __main__ -     global_step = 0\n",
      "================================================================================\n",
      "Best F1 0.2074806812414847\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.6513:   3%|▋                       | 799/30000 [04:40<1:27:35,  5.56it/s]11/13/2019 22:47:54 - INFO - __main__ -   ***** Report result *****\n",
      "11/13/2019 22:47:54 - INFO - __main__ -     global_step = 200\n",
      "11/13/2019 22:47:54 - INFO - __main__ -     train loss = 0.6513\n",
      "loss 0.4756:   5%|█▏                     | 1599/30000 [06:51<1:25:18,  5.55it/s]11/13/2019 22:50:05 - INFO - __main__ -   ***** Report result *****\n",
      "11/13/2019 22:50:05 - INFO - __main__ -     global_step = 400\n",
      "11/13/2019 22:50:05 - INFO - __main__ -     train loss = 0.4756\n",
      "loss 0.4929:   8%|█▊                     | 2399/30000 [09:02<1:23:05,  5.54it/s]11/13/2019 22:52:16 - INFO - __main__ -   ***** Report result *****\n",
      "11/13/2019 22:52:16 - INFO - __main__ -     global_step = 600\n",
      "11/13/2019 22:52:16 - INFO - __main__ -     train loss = 0.4929\n",
      "loss 0.4517:  11%|██▍                    | 3199/30000 [11:14<1:20:06,  5.58it/s]11/13/2019 22:54:27 - INFO - __main__ -   ***** Report result *****\n",
      "11/13/2019 22:54:27 - INFO - __main__ -     global_step = 800\n",
      "11/13/2019 22:54:27 - INFO - __main__ -     train loss = 0.4517\n",
      "loss 0.4135:  13%|███                    | 3999/30000 [13:24<1:18:04,  5.55it/s]11/13/2019 22:56:38 - INFO - __main__ -   ***** Report result *****\n",
      "11/13/2019 22:56:38 - INFO - __main__ -     global_step = 1000\n",
      "11/13/2019 22:56:38 - INFO - __main__ -     train loss = 0.4135\n",
      "loss 0.4003:  16%|███▋                   | 4799/30000 [15:35<1:15:13,  5.58it/s]11/13/2019 22:58:49 - INFO - __main__ -   ***** Report result *****\n",
      "11/13/2019 22:58:49 - INFO - __main__ -     global_step = 1200\n",
      "11/13/2019 22:58:49 - INFO - __main__ -     train loss = 0.4003\n",
      "loss 0.4013:  19%|████▎                  | 5599/30000 [17:46<1:12:56,  5.58it/s]11/13/2019 23:00:59 - INFO - __main__ -   ***** Report result *****\n",
      "11/13/2019 23:00:59 - INFO - __main__ -     global_step = 1400\n",
      "11/13/2019 23:00:59 - INFO - __main__ -     train loss = 0.4013\n",
      "loss 0.3807:  21%|████▉                  | 6399/30000 [19:56<1:10:17,  5.60it/s]11/13/2019 23:03:10 - INFO - __main__ -   ***** Report result *****\n",
      "11/13/2019 23:03:10 - INFO - __main__ -     global_step = 1600\n",
      "11/13/2019 23:03:10 - INFO - __main__ -     train loss = 0.3807\n",
      "loss 0.4209:  24%|█████▌                 | 7199/30000 [22:07<1:08:32,  5.54it/s]11/13/2019 23:05:20 - INFO - __main__ -   ***** Report result *****\n",
      "11/13/2019 23:05:20 - INFO - __main__ -     global_step = 1800\n",
      "11/13/2019 23:05:20 - INFO - __main__ -     train loss = 0.4209\n",
      "loss 0.3896:  27%|██████▏                | 7999/30000 [24:17<1:06:15,  5.53it/s]11/13/2019 23:07:31 - INFO - __main__ -   ***** Report result *****\n",
      "11/13/2019 23:07:31 - INFO - __main__ -     global_step = 2000\n",
      "11/13/2019 23:07:31 - INFO - __main__ -     train loss = 0.3896\n",
      "loss 0.3677:  29%|██████▋                | 8799/30000 [26:28<1:03:07,  5.60it/s]11/13/2019 23:09:42 - INFO - __main__ -   ***** Report result *****\n",
      "11/13/2019 23:09:42 - INFO - __main__ -     global_step = 2200\n",
      "11/13/2019 23:09:42 - INFO - __main__ -     train loss = 0.3677\n",
      "loss 0.4022:  32%|███████▎               | 9599/30000 [28:38<1:00:48,  5.59it/s]11/13/2019 23:11:52 - INFO - __main__ -   ***** Report result *****\n",
      "11/13/2019 23:11:52 - INFO - __main__ -     global_step = 2400\n",
      "11/13/2019 23:11:52 - INFO - __main__ -     train loss = 0.4022\n",
      "loss 0.37:  35%|█████████                 | 10399/30000 [30:49<58:50,  5.55it/s]11/13/2019 23:14:03 - INFO - __main__ -   ***** Report result *****\n",
      "11/13/2019 23:14:03 - INFO - __main__ -     global_step = 2600\n",
      "11/13/2019 23:14:03 - INFO - __main__ -     train loss = 0.37\n",
      "loss 0.3941:  37%|████████▉               | 11199/30000 [33:02<55:36,  5.63it/s]11/13/2019 23:16:16 - INFO - __main__ -   ***** Report result *****\n",
      "11/13/2019 23:16:16 - INFO - __main__ -     global_step = 2800\n",
      "11/13/2019 23:16:16 - INFO - __main__ -     train loss = 0.3941\n",
      "11/13/2019 23:16:37 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/13/2019 23:16:37 - INFO - __main__ -     Num examples = 2941\n",
      "11/13/2019 23:16:37 - INFO - __main__ -     Batch size = 48\n",
      "11/13/2019 23:18:43 - INFO - __main__ -     eval_F1 = 0.7885483717111227\n",
      "11/13/2019 23:18:43 - INFO - __main__ -     eval_loss = 0.39210520852958003\n",
      "11/13/2019 23:18:43 - INFO - __main__ -     global_step = 2800\n",
      "11/13/2019 23:18:43 - INFO - __main__ -     loss = 0.3941\n",
      "================================================================================\n",
      "Best F1 0.7885483717111227\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.3593:  40%|█████████▌              | 11999/30000 [37:43<54:03,  5.55it/s]11/13/2019 23:20:57 - INFO - __main__ -   ***** Report result *****\n",
      "11/13/2019 23:20:57 - INFO - __main__ -     global_step = 3000\n",
      "11/13/2019 23:20:57 - INFO - __main__ -     train loss = 0.3593\n",
      "11/13/2019 23:21:18 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/13/2019 23:21:18 - INFO - __main__ -     Num examples = 2941\n",
      "11/13/2019 23:21:18 - INFO - __main__ -     Batch size = 48\n",
      "11/13/2019 23:23:24 - INFO - __main__ -     eval_F1 = 0.8027074305382641\n",
      "11/13/2019 23:23:24 - INFO - __main__ -     eval_loss = 0.3574716603804019\n",
      "11/13/2019 23:23:24 - INFO - __main__ -     global_step = 3000\n",
      "11/13/2019 23:23:24 - INFO - __main__ -     loss = 0.3593\n",
      "================================================================================\n",
      "Best F1 0.8027074305382641\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.2611:  43%|██████████▏             | 12799/30000 [42:27<51:43,  5.54it/s]11/13/2019 23:25:41 - INFO - __main__ -   ***** Report result *****\n",
      "11/13/2019 23:25:41 - INFO - __main__ -     global_step = 3200\n",
      "11/13/2019 23:25:41 - INFO - __main__ -     train loss = 0.2611\n",
      "11/13/2019 23:26:01 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/13/2019 23:26:01 - INFO - __main__ -     Num examples = 2941\n",
      "11/13/2019 23:26:01 - INFO - __main__ -     Batch size = 48\n",
      "11/13/2019 23:28:08 - INFO - __main__ -     eval_F1 = 0.7799095101090087\n",
      "11/13/2019 23:28:08 - INFO - __main__ -     eval_loss = 0.3766688580114034\n",
      "11/13/2019 23:28:08 - INFO - __main__ -     global_step = 3200\n",
      "11/13/2019 23:28:08 - INFO - __main__ -     loss = 0.2611\n",
      "================================================================================\n",
      "loss 0.2412:  45%|██████████▉             | 13599/30000 [47:05<49:17,  5.55it/s]11/13/2019 23:30:19 - INFO - __main__ -   ***** Report result *****\n",
      "11/13/2019 23:30:19 - INFO - __main__ -     global_step = 3400\n",
      "11/13/2019 23:30:19 - INFO - __main__ -     train loss = 0.2412\n",
      "11/13/2019 23:30:40 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/13/2019 23:30:40 - INFO - __main__ -     Num examples = 2941\n",
      "11/13/2019 23:30:40 - INFO - __main__ -     Batch size = 48\n",
      "11/13/2019 23:32:46 - INFO - __main__ -     eval_F1 = 0.7996806905737152\n",
      "11/13/2019 23:32:46 - INFO - __main__ -     eval_loss = 0.38017446018995776\n",
      "11/13/2019 23:32:46 - INFO - __main__ -     global_step = 3400\n",
      "11/13/2019 23:32:46 - INFO - __main__ -     loss = 0.2412\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.2307:  48%|███████████▌            | 14399/30000 [51:43<46:07,  5.64it/s]11/13/2019 23:34:57 - INFO - __main__ -   ***** Report result *****\n",
      "11/13/2019 23:34:57 - INFO - __main__ -     global_step = 3600\n",
      "11/13/2019 23:34:57 - INFO - __main__ -     train loss = 0.2307\n",
      "11/13/2019 23:35:18 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/13/2019 23:35:18 - INFO - __main__ -     Num examples = 2941\n",
      "11/13/2019 23:35:18 - INFO - __main__ -     Batch size = 48\n",
      "11/13/2019 23:37:24 - INFO - __main__ -     eval_F1 = 0.7966077516246325\n",
      "11/13/2019 23:37:24 - INFO - __main__ -     eval_loss = 0.3801679798672276\n",
      "11/13/2019 23:37:24 - INFO - __main__ -     global_step = 3600\n",
      "11/13/2019 23:37:24 - INFO - __main__ -     loss = 0.2307\n",
      "================================================================================\n",
      "loss 0.2301:  51%|████████████▏           | 15199/30000 [56:21<44:15,  5.57it/s]11/13/2019 23:39:34 - INFO - __main__ -   ***** Report result *****\n",
      "11/13/2019 23:39:34 - INFO - __main__ -     global_step = 3800\n",
      "11/13/2019 23:39:34 - INFO - __main__ -     train loss = 0.2301\n",
      "11/13/2019 23:39:55 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/13/2019 23:39:55 - INFO - __main__ -     Num examples = 2941\n",
      "11/13/2019 23:39:55 - INFO - __main__ -     Batch size = 48\n",
      "11/13/2019 23:42:02 - INFO - __main__ -     eval_F1 = 0.7956730713197239\n",
      "11/13/2019 23:42:02 - INFO - __main__ -     eval_loss = 0.382735654831894\n",
      "11/13/2019 23:42:02 - INFO - __main__ -     global_step = 3800\n",
      "11/13/2019 23:42:02 - INFO - __main__ -     loss = 0.2301\n",
      "================================================================================\n",
      "loss 0.201:  53%|████████████▎          | 15999/30000 [1:00:59<41:02,  5.69it/s]11/13/2019 23:44:12 - INFO - __main__ -   ***** Report result *****\n",
      "11/13/2019 23:44:12 - INFO - __main__ -     global_step = 4000\n",
      "11/13/2019 23:44:12 - INFO - __main__ -     train loss = 0.201\n",
      "11/13/2019 23:44:34 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/13/2019 23:44:34 - INFO - __main__ -     Num examples = 2941\n",
      "11/13/2019 23:44:34 - INFO - __main__ -     Batch size = 48\n",
      "11/13/2019 23:46:40 - INFO - __main__ -     eval_F1 = 0.7951514446821877\n",
      "11/13/2019 23:46:40 - INFO - __main__ -     eval_loss = 0.3940320019159586\n",
      "11/13/2019 23:46:40 - INFO - __main__ -     global_step = 4000\n",
      "11/13/2019 23:46:40 - INFO - __main__ -     loss = 0.201\n",
      "================================================================================\n",
      "loss 0.2161:  56%|████████████▎         | 16799/30000 [1:05:37<39:50,  5.52it/s]11/13/2019 23:48:50 - INFO - __main__ -   ***** Report result *****\n",
      "11/13/2019 23:48:50 - INFO - __main__ -     global_step = 4200\n",
      "11/13/2019 23:48:50 - INFO - __main__ -     train loss = 0.2161\n",
      "11/13/2019 23:49:12 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/13/2019 23:49:12 - INFO - __main__ -     Num examples = 2941\n",
      "11/13/2019 23:49:12 - INFO - __main__ -     Batch size = 48\n",
      "11/13/2019 23:51:18 - INFO - __main__ -     eval_F1 = 0.8049486878387553\n",
      "11/13/2019 23:51:18 - INFO - __main__ -     eval_loss = 0.3918310295670263\n",
      "11/13/2019 23:51:18 - INFO - __main__ -     global_step = 4200\n",
      "11/13/2019 23:51:18 - INFO - __main__ -     loss = 0.2161\n",
      "================================================================================\n",
      "Best F1 0.8049486878387553\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.2169:  59%|████████████▉         | 17599/30000 [1:10:20<37:19,  5.54it/s]11/13/2019 23:53:33 - INFO - __main__ -   ***** Report result *****\n",
      "11/13/2019 23:53:33 - INFO - __main__ -     global_step = 4400\n",
      "11/13/2019 23:53:33 - INFO - __main__ -     train loss = 0.2169\n",
      "11/13/2019 23:53:54 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/13/2019 23:53:54 - INFO - __main__ -     Num examples = 2941\n",
      "11/13/2019 23:53:54 - INFO - __main__ -     Batch size = 48\n",
      "11/13/2019 23:56:01 - INFO - __main__ -     eval_F1 = 0.7922432568158259\n",
      "11/13/2019 23:56:01 - INFO - __main__ -     eval_loss = 0.437856447462353\n",
      "11/13/2019 23:56:01 - INFO - __main__ -     global_step = 4400\n",
      "11/13/2019 23:56:01 - INFO - __main__ -     loss = 0.2169\n",
      "================================================================================\n",
      "loss 0.2339:  61%|█████████████▍        | 18399/30000 [1:14:57<35:06,  5.51it/s]11/13/2019 23:58:11 - INFO - __main__ -   ***** Report result *****\n",
      "11/13/2019 23:58:11 - INFO - __main__ -     global_step = 4600\n",
      "11/13/2019 23:58:11 - INFO - __main__ -     train loss = 0.2339\n",
      "11/13/2019 23:58:32 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/13/2019 23:58:32 - INFO - __main__ -     Num examples = 2941\n",
      "11/13/2019 23:58:32 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 00:00:39 - INFO - __main__ -     eval_F1 = 0.7963574185384577\n",
      "11/14/2019 00:00:39 - INFO - __main__ -     eval_loss = 0.4017404618883325\n",
      "11/14/2019 00:00:39 - INFO - __main__ -     global_step = 4600\n",
      "11/14/2019 00:00:39 - INFO - __main__ -     loss = 0.2339\n",
      "================================================================================\n",
      "loss 0.2346:  64%|██████████████        | 19199/30000 [1:19:36<32:18,  5.57it/s]11/14/2019 00:02:50 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 00:02:50 - INFO - __main__ -     global_step = 4800\n",
      "11/14/2019 00:02:50 - INFO - __main__ -     train loss = 0.2346\n",
      "11/14/2019 00:03:11 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 00:03:11 - INFO - __main__ -     Num examples = 2941\n",
      "11/14/2019 00:03:11 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 00:05:17 - INFO - __main__ -     eval_F1 = 0.7927755496981682\n",
      "11/14/2019 00:05:17 - INFO - __main__ -     eval_loss = 0.3793552773854425\n",
      "11/14/2019 00:05:17 - INFO - __main__ -     global_step = 4800\n",
      "11/14/2019 00:05:17 - INFO - __main__ -     loss = 0.2346\n",
      "================================================================================\n",
      "loss 0.1668:  67%|██████████████▋       | 19999/30000 [1:24:14<29:32,  5.64it/s]11/14/2019 00:07:28 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 00:07:28 - INFO - __main__ -     global_step = 5000\n",
      "11/14/2019 00:07:28 - INFO - __main__ -     train loss = 0.1668\n",
      "11/14/2019 00:07:49 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 00:07:49 - INFO - __main__ -     Num examples = 2941\n",
      "11/14/2019 00:07:49 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 00:09:55 - INFO - __main__ -     eval_F1 = 0.8005472212051666\n",
      "11/14/2019 00:09:55 - INFO - __main__ -     eval_loss = 0.39630958906585173\n",
      "11/14/2019 00:09:55 - INFO - __main__ -     global_step = 5000\n",
      "11/14/2019 00:09:55 - INFO - __main__ -     loss = 0.1668\n",
      "================================================================================\n",
      "loss 0.2011:  69%|███████████████▎      | 20799/30000 [1:28:52<27:51,  5.51it/s]11/14/2019 00:12:06 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 00:12:06 - INFO - __main__ -     global_step = 5200\n",
      "11/14/2019 00:12:06 - INFO - __main__ -     train loss = 0.2011\n",
      "11/14/2019 00:12:27 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 00:12:27 - INFO - __main__ -     Num examples = 2941\n",
      "11/14/2019 00:12:27 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 00:14:33 - INFO - __main__ -     eval_F1 = 0.7982991866458488\n",
      "11/14/2019 00:14:33 - INFO - __main__ -     eval_loss = 0.38032074633144564\n",
      "11/14/2019 00:14:33 - INFO - __main__ -     global_step = 5200\n",
      "11/14/2019 00:14:33 - INFO - __main__ -     loss = 0.2011\n",
      "================================================================================\n",
      "loss 0.1976:  72%|███████████████▊      | 21599/30000 [1:33:31<24:54,  5.62it/s]11/14/2019 00:16:45 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 00:16:45 - INFO - __main__ -     global_step = 5400\n",
      "11/14/2019 00:16:45 - INFO - __main__ -     train loss = 0.1976\n",
      "11/14/2019 00:17:06 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 00:17:06 - INFO - __main__ -     Num examples = 2941\n",
      "11/14/2019 00:17:06 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 00:19:12 - INFO - __main__ -     eval_F1 = 0.7964883218278951\n",
      "11/14/2019 00:19:12 - INFO - __main__ -     eval_loss = 0.4213577241546685\n",
      "11/14/2019 00:19:12 - INFO - __main__ -     global_step = 5400\n",
      "11/14/2019 00:19:12 - INFO - __main__ -     loss = 0.1976\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.1764:  75%|████████████████▍     | 22399/30000 [1:38:09<23:04,  5.49it/s]11/14/2019 00:21:23 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 00:21:23 - INFO - __main__ -     global_step = 5600\n",
      "11/14/2019 00:21:23 - INFO - __main__ -     train loss = 0.1764\n",
      "11/14/2019 00:21:44 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 00:21:44 - INFO - __main__ -     Num examples = 2941\n",
      "11/14/2019 00:21:44 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 00:23:50 - INFO - __main__ -     eval_F1 = 0.7993527172791071\n",
      "11/14/2019 00:23:50 - INFO - __main__ -     eval_loss = 0.4057201640980859\n",
      "11/14/2019 00:23:50 - INFO - __main__ -     global_step = 5600\n",
      "11/14/2019 00:23:50 - INFO - __main__ -     loss = 0.1764\n",
      "================================================================================\n",
      "loss 0.2521:  77%|█████████████████     | 23199/30000 [1:42:47<20:13,  5.60it/s]11/14/2019 00:26:01 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 00:26:01 - INFO - __main__ -     global_step = 5800\n",
      "11/14/2019 00:26:01 - INFO - __main__ -     train loss = 0.2521\n",
      "11/14/2019 00:26:21 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 00:26:21 - INFO - __main__ -     Num examples = 2941\n",
      "11/14/2019 00:26:21 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 00:28:27 - INFO - __main__ -     eval_F1 = 0.8009012867023846\n",
      "11/14/2019 00:28:27 - INFO - __main__ -     eval_loss = 0.3852312105797952\n",
      "11/14/2019 00:28:27 - INFO - __main__ -     global_step = 5800\n",
      "11/14/2019 00:28:27 - INFO - __main__ -     loss = 0.2521\n",
      "================================================================================\n",
      "loss 0.1962:  80%|█████████████████▌    | 23999/30000 [1:47:24<17:42,  5.65it/s]11/14/2019 00:30:38 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 00:30:38 - INFO - __main__ -     global_step = 6000\n",
      "11/14/2019 00:30:38 - INFO - __main__ -     train loss = 0.1962\n",
      "11/14/2019 00:30:59 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 00:30:59 - INFO - __main__ -     Num examples = 2941\n",
      "11/14/2019 00:30:59 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 00:33:06 - INFO - __main__ -     eval_F1 = 0.8069507675562738\n",
      "11/14/2019 00:33:06 - INFO - __main__ -     eval_loss = 0.37907317142573094\n",
      "11/14/2019 00:33:06 - INFO - __main__ -     global_step = 6000\n",
      "11/14/2019 00:33:06 - INFO - __main__ -     loss = 0.1962\n",
      "================================================================================\n",
      "Best F1 0.8069507675562738\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.133:  83%|███████████████████    | 24799/30000 [1:52:07<15:29,  5.59it/s]11/14/2019 00:35:21 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 00:35:21 - INFO - __main__ -     global_step = 6200\n",
      "11/14/2019 00:35:21 - INFO - __main__ -     train loss = 0.133\n",
      "11/14/2019 00:35:42 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 00:35:42 - INFO - __main__ -     Num examples = 2941\n",
      "11/14/2019 00:35:42 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 00:37:48 - INFO - __main__ -     eval_F1 = 0.7973725113747484\n",
      "11/14/2019 00:37:48 - INFO - __main__ -     eval_loss = 0.3912419339702014\n",
      "11/14/2019 00:37:48 - INFO - __main__ -     global_step = 6200\n",
      "11/14/2019 00:37:48 - INFO - __main__ -     loss = 0.133\n",
      "================================================================================\n",
      "loss 0.1186:  85%|██████████████████▊   | 25599/30000 [1:56:45<13:02,  5.62it/s]11/14/2019 00:39:58 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 00:39:58 - INFO - __main__ -     global_step = 6400\n",
      "11/14/2019 00:39:58 - INFO - __main__ -     train loss = 0.1186\n",
      "11/14/2019 00:40:20 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 00:40:20 - INFO - __main__ -     Num examples = 2941\n",
      "11/14/2019 00:40:20 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 00:42:26 - INFO - __main__ -     eval_F1 = 0.8119763270716289\n",
      "11/14/2019 00:42:26 - INFO - __main__ -     eval_loss = 0.43216006704155474\n",
      "11/14/2019 00:42:26 - INFO - __main__ -     global_step = 6400\n",
      "11/14/2019 00:42:26 - INFO - __main__ -     loss = 0.1186\n",
      "================================================================================\n",
      "Best F1 0.8119763270716289\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.118:  88%|████████████████████▏  | 26399/30000 [2:01:27<10:55,  5.49it/s]11/14/2019 00:44:40 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 00:44:40 - INFO - __main__ -     global_step = 6600\n",
      "11/14/2019 00:44:40 - INFO - __main__ -     train loss = 0.118\n",
      "11/14/2019 00:45:02 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 00:45:02 - INFO - __main__ -     Num examples = 2941\n",
      "11/14/2019 00:45:02 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 00:47:08 - INFO - __main__ -     eval_F1 = 0.8077427182829732\n",
      "11/14/2019 00:47:08 - INFO - __main__ -     eval_loss = 0.4385046022793939\n",
      "11/14/2019 00:47:08 - INFO - __main__ -     global_step = 6600\n",
      "11/14/2019 00:47:08 - INFO - __main__ -     loss = 0.118\n",
      "================================================================================\n",
      "loss 0.083:  91%|████████████████████▊  | 27199/30000 [2:06:05<08:22,  5.58it/s]11/14/2019 00:49:19 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 00:49:19 - INFO - __main__ -     global_step = 6800\n",
      "11/14/2019 00:49:19 - INFO - __main__ -     train loss = 0.083\n",
      "11/14/2019 00:49:39 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 00:49:39 - INFO - __main__ -     Num examples = 2941\n",
      "11/14/2019 00:49:39 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 00:51:46 - INFO - __main__ -     eval_F1 = 0.8027560079707664\n",
      "11/14/2019 00:51:46 - INFO - __main__ -     eval_loss = 0.4359356947844067\n",
      "11/14/2019 00:51:46 - INFO - __main__ -     global_step = 6800\n",
      "11/14/2019 00:51:46 - INFO - __main__ -     loss = 0.083\n",
      "================================================================================\n",
      "loss 0.098:  93%|█████████████████████▍ | 27999/30000 [2:10:42<06:04,  5.50it/s]11/14/2019 00:53:56 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 00:53:56 - INFO - __main__ -     global_step = 7000\n",
      "11/14/2019 00:53:56 - INFO - __main__ -     train loss = 0.098\n",
      "11/14/2019 00:54:17 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 00:54:17 - INFO - __main__ -     Num examples = 2941\n",
      "11/14/2019 00:54:17 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 00:56:23 - INFO - __main__ -     eval_F1 = 0.8122144428450137\n",
      "11/14/2019 00:56:23 - INFO - __main__ -     eval_loss = 0.43631734014038115\n",
      "11/14/2019 00:56:23 - INFO - __main__ -     global_step = 7000\n",
      "11/14/2019 00:56:23 - INFO - __main__ -     loss = 0.098\n",
      "================================================================================\n",
      "Best F1 0.8122144428450137\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.1041:  96%|█████████████████████ | 28799/30000 [2:15:24<03:33,  5.62it/s]11/14/2019 00:58:38 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 00:58:38 - INFO - __main__ -     global_step = 7200\n",
      "11/14/2019 00:58:38 - INFO - __main__ -     train loss = 0.1041\n",
      "11/14/2019 00:59:00 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 00:59:00 - INFO - __main__ -     Num examples = 2941\n",
      "11/14/2019 00:59:00 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 01:01:06 - INFO - __main__ -     eval_F1 = 0.8079572873151258\n",
      "11/14/2019 01:01:06 - INFO - __main__ -     eval_loss = 0.43901186131481684\n",
      "11/14/2019 01:01:06 - INFO - __main__ -     global_step = 7200\n",
      "11/14/2019 01:01:06 - INFO - __main__ -     loss = 0.1041\n",
      "================================================================================\n",
      "loss 0.1015:  99%|█████████████████████▋| 29599/30000 [2:20:02<01:11,  5.62it/s]11/14/2019 01:03:16 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 01:03:16 - INFO - __main__ -     global_step = 7400\n",
      "11/14/2019 01:03:16 - INFO - __main__ -     train loss = 0.1015\n",
      "11/14/2019 01:03:37 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 01:03:37 - INFO - __main__ -     Num examples = 2941\n",
      "11/14/2019 01:03:37 - INFO - __main__ -     Batch size = 48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/14/2019 01:05:43 - INFO - __main__ -     eval_F1 = 0.8104268008891365\n",
      "11/14/2019 01:05:43 - INFO - __main__ -     eval_loss = 0.4385387531872238\n",
      "11/14/2019 01:05:43 - INFO - __main__ -     global_step = 7400\n",
      "11/14/2019 01:05:43 - INFO - __main__ -     loss = 0.1015\n",
      "================================================================================\n",
      "loss 0.1027: 100%|██████████████████████| 30000/30000 [2:23:34<00:00,  6.25it/s]\n",
      "11/14/2019 01:06:48 - INFO - pytorch_transformers.modeling_utils -   loading weights file ../model/roberta_wwm_large_512_1_last2embedding_cls_replacement/roberta_wwm_large_512_1_last2embedding_cls_replacement_0/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "Traceback (most recent call last):\n",
      "  File \"./run_bert_2562_last2embedding_cls.py\", line 841, in <module>\n",
      "    main()\n",
      "  File \"./run_bert_2562_last2embedding_cls.py\", line 757, in main\n",
      "    logits = model(input_ids=input_ids, token_type_ids=segment_ids, attention_mask=input_mask).detach().cpu().numpy()\n",
      "  File \"/root/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 547, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/root/code/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 1119, in forward\n",
      "    attention_mask=flat_attention_mask, head_mask=head_mask)\n",
      "  File \"/root/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 547, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/root/code/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 727, in forward\n",
      "    head_mask=head_mask)\n",
      "  File \"/root/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 547, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/root/code/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 440, in forward\n",
      "    layer_outputs = layer_module(hidden_states, attention_mask, head_mask[i])\n",
      "  File \"/root/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 547, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/root/code/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 420, in forward\n",
      "    intermediate_output = self.intermediate(attention_output)\n",
      "  File \"/root/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 547, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/root/code/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 392, in forward\n",
      "    hidden_states = self.intermediate_act_fn(hidden_states)\n",
      "  File \"/root/code/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 142, in gelu\n",
      "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 384.00 MiB (GPU 0; 10.73 GiB total capacity; 8.60 GiB already allocated; 339.62 MiB free; 906.30 MiB cached)\n"
     ]
    }
   ],
   "source": [
    "!python ./run_bert_2562_last2embedding_cls.py \\\n",
    "--model_type bert \\\n",
    "--model_name_or_path ../model/chinese_roberta_wwm_large_ext_pytorch  \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_test \\\n",
    "--data_dir ../data/data_StratifiedKFold_42/data_replacement_0 \\\n",
    "--output_dir ../model/roberta_wwm_large_512_1_last2embedding_cls_replacement/roberta_wwm_large_512_1_last2embedding_cls_replacement_0 \\\n",
    "--max_seq_length 512 \\\n",
    "--split_num 1 \\\n",
    "--lstm_hidden_size 512 \\\n",
    "--lstm_layers 1 \\\n",
    "--lstm_dropout 0.1 \\\n",
    "--eval_steps 200 \\\n",
    "--per_gpu_train_batch_size 4 \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--warmup_steps 0 \\\n",
    "--per_gpu_eval_batch_size 48 \\\n",
    "--learning_rate 1e-5 \\\n",
    "--adam_epsilon 1e-6 \\\n",
    "--weight_decay 0 \\\n",
    "--train_steps 30000 \\\n",
    "--freeze 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/14/2019 01:07:20 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "11/14/2019 01:07:20 - INFO - pytorch_transformers.tokenization_utils -   Model name '../model/chinese_roberta_wwm_large_ext_pytorch' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming '../model/chinese_roberta_wwm_large_ext_pytorch' is a path or url to a directory containing tokenizer files.\n",
      "11/14/2019 01:07:20 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../model/chinese_roberta_wwm_large_ext_pytorch/added_tokens.json. We won't load it.\n",
      "11/14/2019 01:07:20 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../model/chinese_roberta_wwm_large_ext_pytorch/special_tokens_map.json. We won't load it.\n",
      "11/14/2019 01:07:20 - INFO - pytorch_transformers.tokenization_utils -   loading file ../model/chinese_roberta_wwm_large_ext_pytorch/vocab.txt\n",
      "11/14/2019 01:07:20 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/14/2019 01:07:20 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/14/2019 01:07:20 - INFO - pytorch_transformers.modeling_utils -   loading configuration file ../model/chinese_roberta_wwm_large_ext_pytorch/config.json\n",
      "11/14/2019 01:07:20 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 3,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "11/14/2019 01:07:20 - INFO - pytorch_transformers.modeling_utils -   loading weights file ../model/chinese_roberta_wwm_large_ext_pytorch/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "11/14/2019 01:07:28 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification_last2embedding_cls not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "11/14/2019 01:07:28 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification_last2embedding_cls: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "11/14/2019 01:07:34 - INFO - __main__ -   ** RAW EXAMPLE **\n",
      "11/14/2019 01:07:34 - INFO - __main__ -   content: ['这', '几', '天', '看', '了', '有', '人', '举', '报', '施', '某', '某', '的', '贴', '子', '，', '经', '与', '举', '报', '人', '联', '系', '证', '实', '，', '是', '宣', '某', '当', '天', '中', '午', '请', '举', '报', '人', '和', '枪', '手', '喝', '酒', '后', '，', '晚', '上', '才', '发', '的', '贴', '子', '！', '本', '人', '不', '去', '讨', '论', '前', '二', '天', '的', '举', '报', '，', '相', '信', '总', '归', '会', '有', '说', '法', '的', '！', '今', '天', '一', '看', '施', '全', '军', '2017', '年', '1', '月', '2', '日', '实', '名', '举', '报', '上', '黄', '镇', '宣', '国', '才', '的', '贴', '子', '（', '仍', '被', '锁', '定', '禁', '止', '评', '论', '）', '已', '经', '正', '好', '一', '整', '年', '了', '750', '001', '##18', '##014', '##29', '##10', '##85', '##79', '##66', '##86', '##17', '##12', '##11', '##23', '750', '##750', '001', '##18', '##014', '##29', '##10', '##85', '##79', '##66', '##86', '##17', '##12', '##11', '##23', '750', '图', '片', '：', '429', '##10', '##85', '##15', '##14', '##98', '##14', '##71', '##47', '##89', '##52', '施', '全', '军', '实', '名', '举', '报', '50', '天', '后', '，', '上', '黄', '镇', '党', '委', '政', '府', '回', '复', '如', '下', '图', '：', '750', '001', '##18', '##014', '##29', '##10', '##85', '##91', '##17', '##21', '##99', '##0', '750', '##750', '001', '##18', '##014', '##29', '##10', '##85', '##91', '##17', '##21', '##99', '##0', '750', '图', '片', '：', '429', '##10', '##85', '##15', '##14', '##98', '##14', '##72', '##63', '##16', '##68', '750', '001', '##18', '##014', '##29', '##10', '##85', '##99', '##39', '##43', '##20', '##75', '750', '##750', '001', '##18', '##014', '##29', '##10', '##85', '##99', '##39', '##43', '##20', '##75', '750', '图', '片', '：', '429', '##10', '##85', '##15', '##14', '##98', '##14', '##72', '##35', '##30', '##75', '一', '年', '的', '贴', '子', '，', '再', '次', '被', '网', '友', '顶', '起', '来', '后', '，', '才', '发', '现', '施', '某', '几', '天', '前', '回', '复', '网', '友', '的', '处', '理', '结', '果', '竟', '如', '下', '图', '：', '750', '001', '##18', '##014', '##29', '##10', '##85', '##93', '##25', '##72', '##76', '##08', '##51', '##31', '750', '##750', '001', '##18', '##014', '##29', '##10', '##85', '##93', '##25', '##72', '##76', '##08', '##51', '##31', '750', '图', '片', '：', '429', '##10', '##85', '##15', '##14', '##98', '##14', '##73', '##54', '##71', '##72', '现', '责', '问', '张', '涛', '书', '记', '：', '1', '、', '宣', '国', '才', '被', '举', '报', '这', '么', '多', '问', '题', '，', '什', '么', '时', '候', '有', '答', '复', '。', '2', '、', '宣', '国', '才', '被', '举', '报', '后', '，', '为', '什', '么', '被', '立', '刻', '免', '了', '村', '书', '记', '职', '务', '？', '为', '什', '么', '又', '被', '安', '排', '到', '城', '管', '队', '[UNK]', '吃', '空', '响', '[UNK]', '，', '自', '己', '却', '天', '天', '在', '我', '们', '水', '泥', '厂', '上', '班', '赚', '黑', '钱', '？', '3', '、', '这', '几', '个', '月', '，', '水', '泥', '每', '吨', '近', '200', '元', '纯', '利', '润', '，', '还', '供', '不', '应', '求', '，', '宣', '国', '才', '还', '清', '上', '黄', '政', '府', '担', '保', '借', '给', '宣', '国', '才', '代', '付', '振', '东', '厂', '工', '资', '社', '保', '的', '钱', '了', '吗', '？', '4', '、', '据', '了', '解', '宣', '国', '才', '占', '他', '人', '企', '业', '经', '营', '，', '又', '欠', '税', '521', '##6', '万', '元', '、', '欠', '社', '保', '327', '##6', '万', '元', '、', '应', '该', '还', '欠', '了', '职', '工', '工', '资', '几', '十', '万', '，', '上', '黄', '政', '府', '打', '算', '替', '宣', '国', '才', '担', '保', '还', '是', '归', '还', '？', '5', '、', '我', '们', '厂', '合', '法', '会', '计', '和', '老', '板', '被', '判', '刑', '四', '到', '六', '年', '，', '现', '在', '服', '刑', '。', '厂', '子', '给', '宣', '国', '才', '强', '占', '，', '宣', '国', '才', '每', '天', '赚', '20', '多', '万', '净', '利', '润', '，', '却', '对', '外', '宣', '称', '天', '天', '亏', '本', '！', '等', '咱', '老', '板', '刑', '满', '回', '厂', '，', '宣', '国', '才', '给', '咱', '厂', '[UNK]', '天', '天', '亏', '[UNK]', '可', '能', '要', '[UNK]', '亏', '[UNK]', '的', '几', '千', '万', '元', '，', '甚', '至', '几', '个', '亿', '，', '张', '涛', '书', '记', '您', '承', '担', '还', '是', '上', '黄', '政', '府', '承', '担', '？', '当', '初', '可', '是', '您', '亲', '自', '把', '厂', '交', '给', '宣', '国', '才', '生', '产', '的', '！', '希', '望', '徐', '市', '长', '看', '到', '本', '贴', '后', '能', '像', '批', '示', '263', '、', '批', '示', '违', '建', '等', '民', '生', '问', '题', '一', '样', '，', '关', '注', '一', '下', '我', '们', '水', '泥', '厂', '的', '将', '来', '！', '也', '请', '徐', '市', '长', '抽', '日', '理', '万', '机', '之', '空', '亲', '自', '约', '谈', '一', '下', '当', '事', '人', '（', '特', '别', '是', '那', '位', '施', '站', '长', '）', '，', '千', '万', '不', '能', '听', '取', '一', '面', '之', '辞', '！']\n",
      "11/14/2019 01:07:34 - INFO - __main__ -   *** Example ***\n",
      "11/14/2019 01:07:34 - INFO - __main__ -   idx: 0\n",
      "11/14/2019 01:07:34 - INFO - __main__ -   guid: 7a3dd79f90ee419da87190cff60f7a86\n",
      "11/14/2019 01:07:34 - INFO - __main__ -   tokens: [CLS] 问 责 领 导 上 黄 镇 党 委 书 记 张 涛 ， 宣 国 才 真 能 一 手 遮 天 吗 ？ [SEP] 后 ， 才 发 现 施 某 几 天 前 回 复 网 友 的 处 理 结 果 竟 如 下 图 ： 750 001 ##18 ##014 ##29 ##10 ##85 ##93 ##25 ##72 ##76 ##08 ##51 ##31 750 ##750 001 ##18 ##014 ##29 ##10 ##85 ##93 ##25 ##72 ##76 ##08 ##51 ##31 750 图 片 ： 429 ##10 ##85 ##15 ##14 ##98 ##14 ##73 ##54 ##71 ##72 现 责 问 张 涛 书 记 ： 1 、 宣 国 才 被 举 报 这 么 多 问 题 ， 什 么 时 候 有 答 复 。 2 、 宣 国 才 被 举 报 后 ， 为 什 么 被 立 刻 免 了 村 书 记 职 务 ？ 为 什 么 又 被 安 排 到 城 管 队 [UNK] 吃 空 响 [UNK] ， 自 己 却 天 天 在 我 们 水 泥 厂 上 班 赚 黑 钱 ？ 3 、 这 几 个 月 ， 水 泥 每 吨 近 200 元 纯 利 润 ， 还 供 不 应 求 ， 宣 国 才 还 清 上 黄 政 府 担 保 借 给 宣 国 才 代 付 振 东 厂 工 资 社 保 的 钱 了 吗 ？ 4 、 据 了 解 宣 国 才 占 他 人 企 业 经 营 ， 又 欠 税 521 ##6 万 元 、 欠 社 保 327 ##6 万 元 、 应 该 还 欠 了 职 工 工 资 几 十 万 ， 上 黄 政 府 打 算 替 宣 国 才 担 保 还 是 归 还 ？ 5 、 我 们 厂 合 法 会 计 和 老 板 被 判 刑 四 到 六 年 ， 现 在 服 刑 。 厂 子 给 宣 国 才 强 占 ， 宣 国 才 每 天 赚 20 多 万 净 利 润 ， 却 对 外 宣 称 天 天 亏 本 ！ 等 咱 老 板 刑 满 回 厂 ， 宣 国 才 给 咱 厂 [UNK] 天 天 亏 [UNK] 可 能 要 [UNK] 亏 [UNK] 的 几 千 万 元 ， 甚 至 几 个 亿 ， 张 涛 书 记 您 承 担 还 是 上 黄 政 府 承 担 ？ 当 初 可 是 您 亲 自 把 厂 交 给 宣 国 才 生 产 的 ！ 希 望 徐 市 长 看 到 本 贴 后 能 像 批 示 263 、 批 示 违 建 等 民 生 问 题 一 样 ， 关 注 一 下 我 们 水 泥 厂 的 将 来 ！ 也 请 徐 市 长 抽 日 理 万 机 之 空 亲 自 约 谈 一 下 当 事 人 （ 特 别 是 那 位 施 站 长 ） ， 千 万 不 能 听 取 一 面 之 辞 [SEP]\n",
      "11/14/2019 01:07:34 - INFO - __main__ -   input_ids: 101 7309 6569 7566 2193 677 7942 7252 1054 1999 741 6381 2476 3875 8024 2146 1744 2798 4696 5543 671 2797 6902 1921 1408 8043 102 1400 8024 2798 1355 4385 3177 3378 1126 1921 1184 1726 1908 5381 1351 4638 1905 4415 5310 3362 4994 1963 678 1745 8038 9180 9263 8662 11365 8887 8311 9169 9676 8743 9492 9624 9153 9216 8805 9180 10969 9263 8662 11365 8887 8311 9169 9676 8743 9492 9624 9153 9216 8805 9180 1745 4275 8038 13249 8311 9169 8493 8717 9215 8717 9148 9488 9097 9492 4385 6569 7309 2476 3875 741 6381 8038 122 510 2146 1744 2798 6158 715 2845 6821 720 1914 7309 7579 8024 784 720 3198 952 3300 5031 1908 511 123 510 2146 1744 2798 6158 715 2845 1400 8024 711 784 720 6158 4989 1174 1048 749 3333 741 6381 5466 1218 8043 711 784 720 1348 6158 2128 2961 1168 1814 5052 7339 100 1391 4958 1510 100 8024 5632 2346 1316 1921 1921 1762 2769 812 3717 3799 1322 677 4408 6611 7946 7178 8043 124 510 6821 1126 702 3299 8024 3717 3799 3680 1417 6818 8185 1039 5283 1164 3883 8024 6820 897 679 2418 3724 8024 2146 1744 2798 6820 3926 677 7942 3124 2424 2857 924 955 5314 2146 1744 2798 807 802 2920 691 1322 2339 6598 4852 924 4638 7178 749 1408 8043 125 510 2945 749 6237 2146 1744 2798 1304 800 782 821 689 5307 5852 8024 1348 3612 4925 11411 8158 674 1039 510 3612 4852 924 12094 8158 674 1039 510 2418 6421 6820 3612 749 5466 2339 2339 6598 1126 1282 674 8024 677 7942 3124 2424 2802 5050 3296 2146 1744 2798 2857 924 6820 3221 2495 6820 8043 126 510 2769 812 1322 1394 3791 833 6369 1469 5439 3352 6158 1161 1152 1724 1168 1063 2399 8024 4385 1762 3302 1152 511 1322 2094 5314 2146 1744 2798 2487 1304 8024 2146 1744 2798 3680 1921 6611 8113 1914 674 1112 1164 3883 8024 1316 2190 1912 2146 4917 1921 1921 755 3315 8013 5023 1493 5439 3352 1152 4007 1726 1322 8024 2146 1744 2798 5314 1493 1322 100 1921 1921 755 100 1377 5543 6206 100 755 100 4638 1126 1283 674 1039 8024 4493 5635 1126 702 783 8024 2476 3875 741 6381 2644 2824 2857 6820 3221 677 7942 3124 2424 2824 2857 8043 2496 1159 1377 3221 2644 779 5632 2828 1322 769 5314 2146 1744 2798 4495 772 4638 8013 2361 3307 2528 2356 7270 4692 1168 3315 6585 1400 5543 1008 2821 4850 10864 510 2821 4850 6824 2456 5023 3696 4495 7309 7579 671 3416 8024 1068 3800 671 678 2769 812 3717 3799 1322 4638 2199 3341 8013 738 6435 2528 2356 7270 2853 3189 4415 674 3322 722 4958 779 5632 5276 6448 671 678 2496 752 782 8020 4294 1166 3221 6929 855 3177 4991 7270 8021 8024 1283 674 679 5543 1420 1357 671 7481 722 6791 102\n",
      "11/14/2019 01:07:34 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/14/2019 01:07:34 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/14/2019 01:07:34 - INFO - __main__ -   label: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/14/2019 01:09:07 - INFO - __main__ -   ***** Running training *****\n",
      "11/14/2019 01:09:07 - INFO - __main__ -     Num examples = 11756\n",
      "11/14/2019 01:09:07 - INFO - __main__ -     Batch size = 4\n",
      "11/14/2019 01:09:07 - INFO - __main__ -     Num steps = 30000\n",
      "  0%|                                                 | 0/30000 [00:00<?, ?it/s]11/14/2019 01:09:29 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 01:09:29 - INFO - __main__ -     Num examples = 2940\n",
      "11/14/2019 01:09:29 - INFO - __main__ -     Batch size = 48\n",
      "/root/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "11/14/2019 01:11:34 - INFO - __main__ -     eval_F1 = 0.21191645759854258\n",
      "11/14/2019 01:11:34 - INFO - __main__ -     eval_loss = 1.3244992023514164\n",
      "11/14/2019 01:11:34 - INFO - __main__ -     global_step = 0\n",
      "================================================================================\n",
      "Best F1 0.21191645759854258\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.6952:   3%|▋                       | 799/30000 [04:37<1:25:08,  5.72it/s]11/14/2019 01:13:44 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 01:13:44 - INFO - __main__ -     global_step = 200\n",
      "11/14/2019 01:13:44 - INFO - __main__ -     train loss = 0.6952\n",
      "loss 0.508:   5%|█▎                      | 1599/30000 [06:47<1:23:44,  5.65it/s]11/14/2019 01:15:54 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 01:15:54 - INFO - __main__ -     global_step = 400\n",
      "11/14/2019 01:15:54 - INFO - __main__ -     train loss = 0.508\n",
      "loss 0.4196:   8%|█▊                     | 2399/30000 [08:57<1:22:43,  5.56it/s]11/14/2019 01:18:04 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 01:18:04 - INFO - __main__ -     global_step = 600\n",
      "11/14/2019 01:18:04 - INFO - __main__ -     train loss = 0.4196\n",
      "loss 0.4685:  11%|██▍                    | 3199/30000 [11:08<1:19:48,  5.60it/s]11/14/2019 01:20:15 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 01:20:15 - INFO - __main__ -     global_step = 800\n",
      "11/14/2019 01:20:15 - INFO - __main__ -     train loss = 0.4685\n",
      "loss 0.4177:  13%|███                    | 3999/30000 [13:18<1:18:07,  5.55it/s]11/14/2019 01:22:26 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 01:22:26 - INFO - __main__ -     global_step = 1000\n",
      "11/14/2019 01:22:26 - INFO - __main__ -     train loss = 0.4177\n",
      "loss 0.4131:  16%|███▋                   | 4799/30000 [15:29<1:15:32,  5.56it/s]11/14/2019 01:24:36 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 01:24:36 - INFO - __main__ -     global_step = 1200\n",
      "11/14/2019 01:24:36 - INFO - __main__ -     train loss = 0.4131\n",
      "loss 0.3817:  19%|████▎                  | 5599/30000 [17:40<1:13:07,  5.56it/s]11/14/2019 01:26:47 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 01:26:47 - INFO - __main__ -     global_step = 1400\n",
      "11/14/2019 01:26:47 - INFO - __main__ -     train loss = 0.3817\n",
      "loss 0.3944:  21%|████▉                  | 6399/30000 [19:50<1:09:43,  5.64it/s]11/14/2019 01:28:57 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 01:28:57 - INFO - __main__ -     global_step = 1600\n",
      "11/14/2019 01:28:57 - INFO - __main__ -     train loss = 0.3944\n",
      "loss 0.3853:  24%|█████▌                 | 7199/30000 [22:00<1:08:30,  5.55it/s]11/14/2019 01:31:07 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 01:31:07 - INFO - __main__ -     global_step = 1800\n",
      "11/14/2019 01:31:07 - INFO - __main__ -     train loss = 0.3853\n",
      "loss 0.4149:  27%|██████▏                | 7999/30000 [24:11<1:05:55,  5.56it/s]11/14/2019 01:33:18 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 01:33:18 - INFO - __main__ -     global_step = 2000\n",
      "11/14/2019 01:33:18 - INFO - __main__ -     train loss = 0.4149\n",
      "loss 0.3723:  29%|██████▋                | 8799/30000 [26:20<1:02:24,  5.66it/s]11/14/2019 01:35:27 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 01:35:27 - INFO - __main__ -     global_step = 2200\n",
      "11/14/2019 01:35:27 - INFO - __main__ -     train loss = 0.3723\n",
      "loss 0.4179:  32%|███████▉                 | 9599/30000 [28:30<59:58,  5.67it/s]11/14/2019 01:37:37 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 01:37:37 - INFO - __main__ -     global_step = 2400\n",
      "11/14/2019 01:37:37 - INFO - __main__ -     train loss = 0.4179\n",
      "loss 0.3701:  35%|████████▎               | 10399/30000 [30:40<58:41,  5.57it/s]11/14/2019 01:39:47 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 01:39:47 - INFO - __main__ -     global_step = 2600\n",
      "11/14/2019 01:39:47 - INFO - __main__ -     train loss = 0.3701\n",
      "loss 0.4137:  37%|████████▉               | 11199/30000 [32:51<56:42,  5.52it/s]11/14/2019 01:41:58 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 01:41:58 - INFO - __main__ -     global_step = 2800\n",
      "11/14/2019 01:41:58 - INFO - __main__ -     train loss = 0.4137\n",
      "11/14/2019 01:42:18 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 01:42:18 - INFO - __main__ -     Num examples = 2940\n",
      "11/14/2019 01:42:18 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 01:44:25 - INFO - __main__ -     eval_F1 = 0.77223820511743\n",
      "11/14/2019 01:44:25 - INFO - __main__ -     eval_loss = 0.3709467410921089\n",
      "11/14/2019 01:44:25 - INFO - __main__ -     global_step = 2800\n",
      "11/14/2019 01:44:25 - INFO - __main__ -     loss = 0.4137\n",
      "================================================================================\n",
      "Best F1 0.77223820511743\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.3609:  40%|█████████▌              | 11999/30000 [37:32<54:07,  5.54it/s]11/14/2019 01:46:39 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 01:46:39 - INFO - __main__ -     global_step = 3000\n",
      "11/14/2019 01:46:39 - INFO - __main__ -     train loss = 0.3609\n",
      "11/14/2019 01:46:59 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 01:46:59 - INFO - __main__ -     Num examples = 2940\n",
      "11/14/2019 01:46:59 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 01:49:06 - INFO - __main__ -     eval_F1 = 0.7866506797739147\n",
      "11/14/2019 01:49:06 - INFO - __main__ -     eval_loss = 0.3516788294598941\n",
      "11/14/2019 01:49:06 - INFO - __main__ -     global_step = 3000\n",
      "11/14/2019 01:49:06 - INFO - __main__ -     loss = 0.3609\n",
      "================================================================================\n",
      "Best F1 0.7866506797739147\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.2994:  43%|██████████▏             | 12799/30000 [42:13<51:51,  5.53it/s]11/14/2019 01:51:20 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 01:51:20 - INFO - __main__ -     global_step = 3200\n",
      "11/14/2019 01:51:20 - INFO - __main__ -     train loss = 0.2994\n",
      "11/14/2019 01:51:41 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 01:51:41 - INFO - __main__ -     Num examples = 2940\n",
      "11/14/2019 01:51:41 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 01:53:47 - INFO - __main__ -     eval_F1 = 0.7997693963186734\n",
      "11/14/2019 01:53:47 - INFO - __main__ -     eval_loss = 0.3788918303626199\n",
      "11/14/2019 01:53:47 - INFO - __main__ -     global_step = 3200\n",
      "11/14/2019 01:53:47 - INFO - __main__ -     loss = 0.2994\n",
      "================================================================================\n",
      "Best F1 0.7997693963186734\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.2174:  45%|██████████▉             | 13599/30000 [46:54<49:35,  5.51it/s]11/14/2019 01:56:02 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 01:56:02 - INFO - __main__ -     global_step = 3400\n",
      "11/14/2019 01:56:02 - INFO - __main__ -     train loss = 0.2174\n",
      "11/14/2019 01:56:22 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 01:56:22 - INFO - __main__ -     Num examples = 2940\n",
      "11/14/2019 01:56:22 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 01:58:28 - INFO - __main__ -     eval_F1 = 0.7996970463324976\n",
      "11/14/2019 01:58:28 - INFO - __main__ -     eval_loss = 0.4226168359239255\n",
      "11/14/2019 01:58:28 - INFO - __main__ -     global_step = 3400\n",
      "11/14/2019 01:58:28 - INFO - __main__ -     loss = 0.2174\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.1941:  48%|███████████▌            | 14399/30000 [51:32<46:19,  5.61it/s]11/14/2019 02:00:39 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 02:00:39 - INFO - __main__ -     global_step = 3600\n",
      "11/14/2019 02:00:39 - INFO - __main__ -     train loss = 0.1941\n",
      "11/14/2019 02:00:59 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 02:00:59 - INFO - __main__ -     Num examples = 2940\n",
      "11/14/2019 02:00:59 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 02:03:05 - INFO - __main__ -     eval_F1 = 0.7992460787755454\n",
      "11/14/2019 02:03:05 - INFO - __main__ -     eval_loss = 0.4323387755411527\n",
      "11/14/2019 02:03:05 - INFO - __main__ -     global_step = 3600\n",
      "11/14/2019 02:03:05 - INFO - __main__ -     loss = 0.1941\n",
      "================================================================================\n",
      "loss 0.2442:  51%|████████████▏           | 15199/30000 [56:10<44:35,  5.53it/s]11/14/2019 02:05:17 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 02:05:17 - INFO - __main__ -     global_step = 3800\n",
      "11/14/2019 02:05:17 - INFO - __main__ -     train loss = 0.2442\n",
      "11/14/2019 02:05:38 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 02:05:38 - INFO - __main__ -     Num examples = 2940\n",
      "11/14/2019 02:05:38 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 02:07:44 - INFO - __main__ -     eval_F1 = 0.7807159778500813\n",
      "11/14/2019 02:07:44 - INFO - __main__ -     eval_loss = 0.4266040417095346\n",
      "11/14/2019 02:07:44 - INFO - __main__ -     global_step = 3800\n",
      "11/14/2019 02:07:44 - INFO - __main__ -     loss = 0.2442\n",
      "================================================================================\n",
      "loss 0.2025:  53%|███████████▋          | 15999/30000 [1:00:48<42:07,  5.54it/s]11/14/2019 02:09:56 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 02:09:56 - INFO - __main__ -     global_step = 4000\n",
      "11/14/2019 02:09:56 - INFO - __main__ -     train loss = 0.2025\n",
      "11/14/2019 02:10:17 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 02:10:17 - INFO - __main__ -     Num examples = 2940\n",
      "11/14/2019 02:10:17 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 02:12:23 - INFO - __main__ -     eval_F1 = 0.7970583809385352\n",
      "11/14/2019 02:12:23 - INFO - __main__ -     eval_loss = 0.42218323113516937\n",
      "11/14/2019 02:12:23 - INFO - __main__ -     global_step = 4000\n",
      "11/14/2019 02:12:23 - INFO - __main__ -     loss = 0.2025\n",
      "================================================================================\n",
      "loss 0.1895:  56%|████████████▎         | 16799/30000 [1:05:27<39:50,  5.52it/s]11/14/2019 02:14:35 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 02:14:35 - INFO - __main__ -     global_step = 4200\n",
      "11/14/2019 02:14:35 - INFO - __main__ -     train loss = 0.1895\n",
      "11/14/2019 02:14:55 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 02:14:55 - INFO - __main__ -     Num examples = 2940\n",
      "11/14/2019 02:14:55 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 02:17:01 - INFO - __main__ -     eval_F1 = 0.7924810517457116\n",
      "11/14/2019 02:17:01 - INFO - __main__ -     eval_loss = 0.43304185620358876\n",
      "11/14/2019 02:17:01 - INFO - __main__ -     global_step = 4200\n",
      "11/14/2019 02:17:01 - INFO - __main__ -     loss = 0.1895\n",
      "================================================================================\n",
      "loss 0.189:  59%|█████████████▍         | 17599/30000 [1:10:05<37:28,  5.51it/s]11/14/2019 02:19:12 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 02:19:12 - INFO - __main__ -     global_step = 4400\n",
      "11/14/2019 02:19:12 - INFO - __main__ -     train loss = 0.189\n",
      "11/14/2019 02:19:32 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 02:19:32 - INFO - __main__ -     Num examples = 2940\n",
      "11/14/2019 02:19:32 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 02:21:39 - INFO - __main__ -     eval_F1 = 0.7939530217494936\n",
      "11/14/2019 02:21:39 - INFO - __main__ -     eval_loss = 0.4299253414445106\n",
      "11/14/2019 02:21:39 - INFO - __main__ -     global_step = 4400\n",
      "11/14/2019 02:21:39 - INFO - __main__ -     loss = 0.189\n",
      "================================================================================\n",
      "loss 0.2289:  61%|█████████████▍        | 18399/30000 [1:14:44<34:32,  5.60it/s]11/14/2019 02:23:51 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 02:23:51 - INFO - __main__ -     global_step = 4600\n",
      "11/14/2019 02:23:51 - INFO - __main__ -     train loss = 0.2289\n",
      "11/14/2019 02:24:11 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 02:24:11 - INFO - __main__ -     Num examples = 2940\n",
      "11/14/2019 02:24:11 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 02:26:17 - INFO - __main__ -     eval_F1 = 0.7813240232642914\n",
      "11/14/2019 02:26:17 - INFO - __main__ -     eval_loss = 0.4332432239616831\n",
      "11/14/2019 02:26:17 - INFO - __main__ -     global_step = 4600\n",
      "11/14/2019 02:26:17 - INFO - __main__ -     loss = 0.2289\n",
      "================================================================================\n",
      "loss 0.2256:  64%|██████████████        | 19199/30000 [1:19:21<31:57,  5.63it/s]11/14/2019 02:28:28 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 02:28:28 - INFO - __main__ -     global_step = 4800\n",
      "11/14/2019 02:28:28 - INFO - __main__ -     train loss = 0.2256\n",
      "11/14/2019 02:28:48 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 02:28:48 - INFO - __main__ -     Num examples = 2940\n",
      "11/14/2019 02:28:48 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 02:30:54 - INFO - __main__ -     eval_F1 = 0.7886661777276686\n",
      "11/14/2019 02:30:54 - INFO - __main__ -     eval_loss = 0.40642151621080214\n",
      "11/14/2019 02:30:54 - INFO - __main__ -     global_step = 4800\n",
      "11/14/2019 02:30:54 - INFO - __main__ -     loss = 0.2256\n",
      "================================================================================\n",
      "loss 0.2137:  67%|██████████████▋       | 19999/30000 [1:23:58<29:46,  5.60it/s]11/14/2019 02:33:05 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 02:33:05 - INFO - __main__ -     global_step = 5000\n",
      "11/14/2019 02:33:05 - INFO - __main__ -     train loss = 0.2137\n",
      "11/14/2019 02:33:25 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 02:33:25 - INFO - __main__ -     Num examples = 2940\n",
      "11/14/2019 02:33:25 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 02:35:31 - INFO - __main__ -     eval_F1 = 0.7920417234505939\n",
      "11/14/2019 02:35:31 - INFO - __main__ -     eval_loss = 0.38411201975278314\n",
      "11/14/2019 02:35:31 - INFO - __main__ -     global_step = 5000\n",
      "11/14/2019 02:35:31 - INFO - __main__ -     loss = 0.2137\n",
      "================================================================================\n",
      "loss 0.2238:  69%|███████████████▎      | 20799/30000 [1:28:34<27:26,  5.59it/s]11/14/2019 02:37:42 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 02:37:42 - INFO - __main__ -     global_step = 5200\n",
      "11/14/2019 02:37:42 - INFO - __main__ -     train loss = 0.2238\n",
      "11/14/2019 02:38:02 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 02:38:02 - INFO - __main__ -     Num examples = 2940\n",
      "11/14/2019 02:38:02 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 02:40:08 - INFO - __main__ -     eval_F1 = 0.7673256363040459\n",
      "11/14/2019 02:40:08 - INFO - __main__ -     eval_loss = 0.4145889141026043\n",
      "11/14/2019 02:40:08 - INFO - __main__ -     global_step = 5200\n",
      "11/14/2019 02:40:08 - INFO - __main__ -     loss = 0.2238\n",
      "================================================================================\n",
      "loss 0.2287:  72%|███████████████▊      | 21599/30000 [1:33:13<25:30,  5.49it/s]11/14/2019 02:42:20 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 02:42:20 - INFO - __main__ -     global_step = 5400\n",
      "11/14/2019 02:42:20 - INFO - __main__ -     train loss = 0.2287\n",
      "11/14/2019 02:42:40 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 02:42:40 - INFO - __main__ -     Num examples = 2940\n",
      "11/14/2019 02:42:40 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 02:44:46 - INFO - __main__ -     eval_F1 = 0.7915772305382919\n",
      "11/14/2019 02:44:46 - INFO - __main__ -     eval_loss = 0.3958915113321235\n",
      "11/14/2019 02:44:46 - INFO - __main__ -     global_step = 5400\n",
      "11/14/2019 02:44:46 - INFO - __main__ -     loss = 0.2287\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.1834:  75%|████████████████▍     | 22399/30000 [1:37:50<22:35,  5.61it/s]11/14/2019 02:46:57 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 02:46:57 - INFO - __main__ -     global_step = 5600\n",
      "11/14/2019 02:46:57 - INFO - __main__ -     train loss = 0.1834\n",
      "11/14/2019 02:47:17 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 02:47:17 - INFO - __main__ -     Num examples = 2940\n",
      "11/14/2019 02:47:17 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 02:49:23 - INFO - __main__ -     eval_F1 = 0.7820113118439798\n",
      "11/14/2019 02:49:23 - INFO - __main__ -     eval_loss = 0.41107940622755595\n",
      "11/14/2019 02:49:23 - INFO - __main__ -     global_step = 5600\n",
      "11/14/2019 02:49:23 - INFO - __main__ -     loss = 0.1834\n",
      "================================================================================\n",
      "loss 0.2297:  77%|█████████████████     | 23199/30000 [1:42:27<20:35,  5.50it/s]11/14/2019 02:51:34 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 02:51:34 - INFO - __main__ -     global_step = 5800\n",
      "11/14/2019 02:51:34 - INFO - __main__ -     train loss = 0.2297\n",
      "11/14/2019 02:51:55 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 02:51:55 - INFO - __main__ -     Num examples = 2940\n",
      "11/14/2019 02:51:55 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 02:54:01 - INFO - __main__ -     eval_F1 = 0.7897623825139076\n",
      "11/14/2019 02:54:01 - INFO - __main__ -     eval_loss = 0.39582034518882153\n",
      "11/14/2019 02:54:01 - INFO - __main__ -     global_step = 5800\n",
      "11/14/2019 02:54:01 - INFO - __main__ -     loss = 0.2297\n",
      "================================================================================\n",
      "loss 0.1926:  80%|█████████████████▌    | 23999/30000 [1:47:05<18:09,  5.51it/s]11/14/2019 02:56:12 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 02:56:12 - INFO - __main__ -     global_step = 6000\n",
      "11/14/2019 02:56:12 - INFO - __main__ -     train loss = 0.1926\n",
      "11/14/2019 02:56:32 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 02:56:32 - INFO - __main__ -     Num examples = 2940\n",
      "11/14/2019 02:56:32 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 02:58:38 - INFO - __main__ -     eval_F1 = 0.7963397572644323\n",
      "11/14/2019 02:58:38 - INFO - __main__ -     eval_loss = 0.38821709546591004\n",
      "11/14/2019 02:58:38 - INFO - __main__ -     global_step = 6000\n",
      "11/14/2019 02:58:38 - INFO - __main__ -     loss = 0.1926\n",
      "================================================================================\n",
      "loss 0.1147:  83%|██████████████████▏   | 24799/30000 [1:51:43<15:30,  5.59it/s]11/14/2019 03:00:50 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 03:00:50 - INFO - __main__ -     global_step = 6200\n",
      "11/14/2019 03:00:50 - INFO - __main__ -     train loss = 0.1147\n",
      "11/14/2019 03:01:10 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 03:01:10 - INFO - __main__ -     Num examples = 2940\n",
      "11/14/2019 03:01:10 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 03:03:16 - INFO - __main__ -     eval_F1 = 0.799141693776507\n",
      "11/14/2019 03:03:16 - INFO - __main__ -     eval_loss = 0.4075992442967911\n",
      "11/14/2019 03:03:16 - INFO - __main__ -     global_step = 6200\n",
      "11/14/2019 03:03:16 - INFO - __main__ -     loss = 0.1147\n",
      "================================================================================\n",
      "loss 0.0964:  85%|██████████████████▊   | 25599/30000 [1:56:19<13:09,  5.57it/s]11/14/2019 03:05:26 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 03:05:26 - INFO - __main__ -     global_step = 6400\n",
      "11/14/2019 03:05:26 - INFO - __main__ -     train loss = 0.0964\n",
      "11/14/2019 03:05:47 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 03:05:47 - INFO - __main__ -     Num examples = 2940\n",
      "11/14/2019 03:05:47 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 03:07:53 - INFO - __main__ -     eval_F1 = 0.8061700725936812\n",
      "11/14/2019 03:07:53 - INFO - __main__ -     eval_loss = 0.43848386970198444\n",
      "11/14/2019 03:07:53 - INFO - __main__ -     global_step = 6400\n",
      "11/14/2019 03:07:53 - INFO - __main__ -     loss = 0.0964\n",
      "================================================================================\n",
      "Best F1 0.8061700725936812\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.0808:  88%|███████████████████▎  | 26399/30000 [2:01:01<10:52,  5.52it/s]11/14/2019 03:10:08 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 03:10:08 - INFO - __main__ -     global_step = 6600\n",
      "11/14/2019 03:10:08 - INFO - __main__ -     train loss = 0.0808\n",
      "11/14/2019 03:10:28 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 03:10:28 - INFO - __main__ -     Num examples = 2940\n",
      "11/14/2019 03:10:28 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 03:12:34 - INFO - __main__ -     eval_F1 = 0.7927084308933519\n",
      "11/14/2019 03:12:34 - INFO - __main__ -     eval_loss = 0.4925727438662321\n",
      "11/14/2019 03:12:34 - INFO - __main__ -     global_step = 6600\n",
      "11/14/2019 03:12:34 - INFO - __main__ -     loss = 0.0808\n",
      "================================================================================\n",
      "loss 0.0986:  91%|███████████████████▉  | 27199/30000 [2:05:39<08:28,  5.51it/s]11/14/2019 03:14:46 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 03:14:46 - INFO - __main__ -     global_step = 6800\n",
      "11/14/2019 03:14:46 - INFO - __main__ -     train loss = 0.0986\n",
      "11/14/2019 03:15:06 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 03:15:06 - INFO - __main__ -     Num examples = 2940\n",
      "11/14/2019 03:15:06 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 03:17:12 - INFO - __main__ -     eval_F1 = 0.7946792831579209\n",
      "11/14/2019 03:17:12 - INFO - __main__ -     eval_loss = 0.4839496332851629\n",
      "11/14/2019 03:17:12 - INFO - __main__ -     global_step = 6800\n",
      "11/14/2019 03:17:12 - INFO - __main__ -     loss = 0.0986\n",
      "================================================================================\n",
      "loss 0.0831:  93%|████████████████████▌ | 27999/30000 [2:10:17<06:03,  5.51it/s]11/14/2019 03:19:24 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 03:19:24 - INFO - __main__ -     global_step = 7000\n",
      "11/14/2019 03:19:24 - INFO - __main__ -     train loss = 0.0831\n",
      "11/14/2019 03:19:44 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 03:19:44 - INFO - __main__ -     Num examples = 2940\n",
      "11/14/2019 03:19:44 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 03:21:50 - INFO - __main__ -     eval_F1 = 0.8059904466202256\n",
      "11/14/2019 03:21:50 - INFO - __main__ -     eval_loss = 0.48057185511495315\n",
      "11/14/2019 03:21:50 - INFO - __main__ -     global_step = 7000\n",
      "11/14/2019 03:21:50 - INFO - __main__ -     loss = 0.0831\n",
      "================================================================================\n",
      "loss 0.0859:  96%|█████████████████████ | 28799/30000 [2:14:53<03:34,  5.61it/s]11/14/2019 03:24:01 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 03:24:01 - INFO - __main__ -     global_step = 7200\n",
      "11/14/2019 03:24:01 - INFO - __main__ -     train loss = 0.0859\n",
      "11/14/2019 03:24:21 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 03:24:21 - INFO - __main__ -     Num examples = 2940\n",
      "11/14/2019 03:24:21 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 03:26:27 - INFO - __main__ -     eval_F1 = 0.8012463746069858\n",
      "11/14/2019 03:26:27 - INFO - __main__ -     eval_loss = 0.4899712288752198\n",
      "11/14/2019 03:26:27 - INFO - __main__ -     global_step = 7200\n",
      "11/14/2019 03:26:27 - INFO - __main__ -     loss = 0.0859\n",
      "================================================================================\n",
      "loss 0.0736:  99%|█████████████████████▋| 29599/30000 [2:19:31<01:13,  5.48it/s]11/14/2019 03:28:38 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 03:28:38 - INFO - __main__ -     global_step = 7400\n",
      "11/14/2019 03:28:38 - INFO - __main__ -     train loss = 0.0736\n",
      "11/14/2019 03:28:58 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 03:28:58 - INFO - __main__ -     Num examples = 2940\n",
      "11/14/2019 03:28:58 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 03:31:04 - INFO - __main__ -     eval_F1 = 0.8026548853146799\n",
      "11/14/2019 03:31:04 - INFO - __main__ -     eval_loss = 0.48496600501840154\n",
      "11/14/2019 03:31:04 - INFO - __main__ -     global_step = 7400\n",
      "11/14/2019 03:31:04 - INFO - __main__ -     loss = 0.0736\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.1065: 100%|██████████████████████| 30000/30000 [2:23:02<00:00,  6.26it/s]\n",
      "11/14/2019 03:32:09 - INFO - pytorch_transformers.modeling_utils -   loading weights file ../model/roberta_wwm_large_512_1_last2embedding_cls_replacement/roberta_wwm_large_512_1_last2embedding_cls_replacement_1/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "Traceback (most recent call last):\n",
      "  File \"./run_bert_2562_last2embedding_cls.py\", line 841, in <module>\n",
      "    main()\n",
      "  File \"./run_bert_2562_last2embedding_cls.py\", line 757, in main\n",
      "    logits = model(input_ids=input_ids, token_type_ids=segment_ids, attention_mask=input_mask).detach().cpu().numpy()\n",
      "  File \"/root/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 547, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/root/code/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 1119, in forward\n",
      "    attention_mask=flat_attention_mask, head_mask=head_mask)\n",
      "  File \"/root/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 547, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/root/code/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 727, in forward\n",
      "    head_mask=head_mask)\n",
      "  File \"/root/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 547, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/root/code/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 440, in forward\n",
      "    layer_outputs = layer_module(hidden_states, attention_mask, head_mask[i])\n",
      "  File \"/root/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 547, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/root/code/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 420, in forward\n",
      "    intermediate_output = self.intermediate(attention_output)\n",
      "  File \"/root/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 547, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/root/code/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 392, in forward\n",
      "    hidden_states = self.intermediate_act_fn(hidden_states)\n",
      "  File \"/root/code/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 142, in gelu\n",
      "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 384.00 MiB (GPU 0; 10.73 GiB total capacity; 8.60 GiB already allocated; 339.62 MiB free; 906.30 MiB cached)\n"
     ]
    }
   ],
   "source": [
    "!python ./run_bert_2562_last2embedding_cls.py \\\n",
    "--model_type bert \\\n",
    "--model_name_or_path ../model/chinese_roberta_wwm_large_ext_pytorch  \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_test \\\n",
    "--data_dir ../data/data_StratifiedKFold_42/data_replacement_1 \\\n",
    "--output_dir ../model/roberta_wwm_large_512_1_last2embedding_cls_replacement/roberta_wwm_large_512_1_last2embedding_cls_replacement_1 \\\n",
    "--max_seq_length 512 \\\n",
    "--split_num 1 \\\n",
    "--lstm_hidden_size 512 \\\n",
    "--lstm_layers 1 \\\n",
    "--lstm_dropout 0.1 \\\n",
    "--eval_steps 200 \\\n",
    "--per_gpu_train_batch_size 4 \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--warmup_steps 0 \\\n",
    "--per_gpu_eval_batch_size 48 \\\n",
    "--learning_rate 1e-5 \\\n",
    "--adam_epsilon 1e-6 \\\n",
    "--weight_decay 0 \\\n",
    "--train_steps 30000 \\\n",
    "--freeze 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/14/2019 03:32:40 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "11/14/2019 03:32:40 - INFO - pytorch_transformers.tokenization_utils -   Model name '../model/chinese_roberta_wwm_large_ext_pytorch' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming '../model/chinese_roberta_wwm_large_ext_pytorch' is a path or url to a directory containing tokenizer files.\n",
      "11/14/2019 03:32:40 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../model/chinese_roberta_wwm_large_ext_pytorch/added_tokens.json. We won't load it.\n",
      "11/14/2019 03:32:40 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../model/chinese_roberta_wwm_large_ext_pytorch/special_tokens_map.json. We won't load it.\n",
      "11/14/2019 03:32:40 - INFO - pytorch_transformers.tokenization_utils -   loading file ../model/chinese_roberta_wwm_large_ext_pytorch/vocab.txt\n",
      "11/14/2019 03:32:40 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/14/2019 03:32:40 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/14/2019 03:32:40 - INFO - pytorch_transformers.modeling_utils -   loading configuration file ../model/chinese_roberta_wwm_large_ext_pytorch/config.json\n",
      "11/14/2019 03:32:40 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 3,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "11/14/2019 03:32:40 - INFO - pytorch_transformers.modeling_utils -   loading weights file ../model/chinese_roberta_wwm_large_ext_pytorch/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "11/14/2019 03:32:50 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification_last2embedding_cls not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "11/14/2019 03:32:50 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification_last2embedding_cls: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "11/14/2019 03:32:57 - INFO - __main__ -   ** RAW EXAMPLE **\n",
      "11/14/2019 03:32:57 - INFO - __main__ -   content: ['这', '几', '天', '看', '了', '有', '人', '举', '报', '施', '某', '某', '的', '贴', '子', '，', '经', '与', '举', '报', '人', '联', '系', '证', '实', '，', '是', '宣', '某', '当', '天', '中', '午', '请', '举', '报', '人', '和', '枪', '手', '喝', '酒', '后', '，', '晚', '上', '才', '发', '的', '贴', '子', '！', '本', '人', '不', '去', '讨', '论', '前', '二', '天', '的', '举', '报', '，', '相', '信', '总', '归', '会', '有', '说', '法', '的', '！', '今', '天', '一', '看', '施', '全', '军', '2017', '年', '1', '月', '2', '日', '实', '名', '举', '报', '上', '黄', '镇', '宣', '国', '才', '的', '贴', '子', '（', '仍', '被', '锁', '定', '禁', '止', '评', '论', '）', '已', '经', '正', '好', '一', '整', '年', '了', '750', '001', '##18', '##014', '##29', '##10', '##85', '##79', '##66', '##86', '##17', '##12', '##11', '##23', '750', '##750', '001', '##18', '##014', '##29', '##10', '##85', '##79', '##66', '##86', '##17', '##12', '##11', '##23', '750', '图', '片', '：', '429', '##10', '##85', '##15', '##14', '##98', '##14', '##71', '##47', '##89', '##52', '施', '全', '军', '实', '名', '举', '报', '50', '天', '后', '，', '上', '黄', '镇', '党', '委', '政', '府', '回', '复', '如', '下', '图', '：', '750', '001', '##18', '##014', '##29', '##10', '##85', '##91', '##17', '##21', '##99', '##0', '750', '##750', '001', '##18', '##014', '##29', '##10', '##85', '##91', '##17', '##21', '##99', '##0', '750', '图', '片', '：', '429', '##10', '##85', '##15', '##14', '##98', '##14', '##72', '##63', '##16', '##68', '750', '001', '##18', '##014', '##29', '##10', '##85', '##99', '##39', '##43', '##20', '##75', '750', '##750', '001', '##18', '##014', '##29', '##10', '##85', '##99', '##39', '##43', '##20', '##75', '750', '图', '片', '：', '429', '##10', '##85', '##15', '##14', '##98', '##14', '##72', '##35', '##30', '##75', '一', '年', '的', '贴', '子', '，', '再', '次', '被', '网', '友', '顶', '起', '来', '后', '，', '才', '发', '现', '施', '某', '几', '天', '前', '回', '复', '网', '友', '的', '处', '理', '结', '果', '竟', '如', '下', '图', '：', '750', '001', '##18', '##014', '##29', '##10', '##85', '##93', '##25', '##72', '##76', '##08', '##51', '##31', '750', '##750', '001', '##18', '##014', '##29', '##10', '##85', '##93', '##25', '##72', '##76', '##08', '##51', '##31', '750', '图', '片', '：', '429', '##10', '##85', '##15', '##14', '##98', '##14', '##73', '##54', '##71', '##72', '现', '责', '问', '张', '涛', '书', '记', '：', '1', '、', '宣', '国', '才', '被', '举', '报', '这', '么', '多', '问', '题', '，', '什', '么', '时', '候', '有', '答', '复', '。', '2', '、', '宣', '国', '才', '被', '举', '报', '后', '，', '为', '什', '么', '被', '立', '刻', '免', '了', '村', '书', '记', '职', '务', '？', '为', '什', '么', '又', '被', '安', '排', '到', '城', '管', '队', '[UNK]', '吃', '空', '响', '[UNK]', '，', '自', '己', '却', '天', '天', '在', '我', '们', '水', '泥', '厂', '上', '班', '赚', '黑', '钱', '？', '3', '、', '这', '几', '个', '月', '，', '水', '泥', '每', '吨', '近', '200', '元', '纯', '利', '润', '，', '还', '供', '不', '应', '求', '，', '宣', '国', '才', '还', '清', '上', '黄', '政', '府', '担', '保', '借', '给', '宣', '国', '才', '代', '付', '振', '东', '厂', '工', '资', '社', '保', '的', '钱', '了', '吗', '？', '4', '、', '据', '了', '解', '宣', '国', '才', '占', '他', '人', '企', '业', '经', '营', '，', '又', '欠', '税', '521', '##6', '万', '元', '、', '欠', '社', '保', '327', '##6', '万', '元', '、', '应', '该', '还', '欠', '了', '职', '工', '工', '资', '几', '十', '万', '，', '上', '黄', '政', '府', '打', '算', '替', '宣', '国', '才', '担', '保', '还', '是', '归', '还', '？', '5', '、', '我', '们', '厂', '合', '法', '会', '计', '和', '老', '板', '被', '判', '刑', '四', '到', '六', '年', '，', '现', '在', '服', '刑', '。', '厂', '子', '给', '宣', '国', '才', '强', '占', '，', '宣', '国', '才', '每', '天', '赚', '20', '多', '万', '净', '利', '润', '，', '却', '对', '外', '宣', '称', '天', '天', '亏', '本', '！', '等', '咱', '老', '板', '刑', '满', '回', '厂', '，', '宣', '国', '才', '给', '咱', '厂', '[UNK]', '天', '天', '亏', '[UNK]', '可', '能', '要', '[UNK]', '亏', '[UNK]', '的', '几', '千', '万', '元', '，', '甚', '至', '几', '个', '亿', '，', '张', '涛', '书', '记', '您', '承', '担', '还', '是', '上', '黄', '政', '府', '承', '担', '？', '当', '初', '可', '是', '您', '亲', '自', '把', '厂', '交', '给', '宣', '国', '才', '生', '产', '的', '！', '希', '望', '徐', '市', '长', '看', '到', '本', '贴', '后', '能', '像', '批', '示', '263', '、', '批', '示', '违', '建', '等', '民', '生', '问', '题', '一', '样', '，', '关', '注', '一', '下', '我', '们', '水', '泥', '厂', '的', '将', '来', '！', '也', '请', '徐', '市', '长', '抽', '日', '理', '万', '机', '之', '空', '亲', '自', '约', '谈', '一', '下', '当', '事', '人', '（', '特', '别', '是', '那', '位', '施', '站', '长', '）', '，', '千', '万', '不', '能', '听', '取', '一', '面', '之', '辞', '！']\n",
      "11/14/2019 03:32:57 - INFO - __main__ -   *** Example ***\n",
      "11/14/2019 03:32:57 - INFO - __main__ -   idx: 0\n",
      "11/14/2019 03:32:57 - INFO - __main__ -   guid: 7a3dd79f90ee419da87190cff60f7a86\n",
      "11/14/2019 03:32:57 - INFO - __main__ -   tokens: [CLS] 问 责 领 导 上 黄 镇 党 委 书 记 张 涛 ， 宣 国 才 真 能 一 手 遮 天 吗 ？ [SEP] 后 ， 才 发 现 施 某 几 天 前 回 复 网 友 的 处 理 结 果 竟 如 下 图 ： 750 001 ##18 ##014 ##29 ##10 ##85 ##93 ##25 ##72 ##76 ##08 ##51 ##31 750 ##750 001 ##18 ##014 ##29 ##10 ##85 ##93 ##25 ##72 ##76 ##08 ##51 ##31 750 图 片 ： 429 ##10 ##85 ##15 ##14 ##98 ##14 ##73 ##54 ##71 ##72 现 责 问 张 涛 书 记 ： 1 、 宣 国 才 被 举 报 这 么 多 问 题 ， 什 么 时 候 有 答 复 。 2 、 宣 国 才 被 举 报 后 ， 为 什 么 被 立 刻 免 了 村 书 记 职 务 ？ 为 什 么 又 被 安 排 到 城 管 队 [UNK] 吃 空 响 [UNK] ， 自 己 却 天 天 在 我 们 水 泥 厂 上 班 赚 黑 钱 ？ 3 、 这 几 个 月 ， 水 泥 每 吨 近 200 元 纯 利 润 ， 还 供 不 应 求 ， 宣 国 才 还 清 上 黄 政 府 担 保 借 给 宣 国 才 代 付 振 东 厂 工 资 社 保 的 钱 了 吗 ？ 4 、 据 了 解 宣 国 才 占 他 人 企 业 经 营 ， 又 欠 税 521 ##6 万 元 、 欠 社 保 327 ##6 万 元 、 应 该 还 欠 了 职 工 工 资 几 十 万 ， 上 黄 政 府 打 算 替 宣 国 才 担 保 还 是 归 还 ？ 5 、 我 们 厂 合 法 会 计 和 老 板 被 判 刑 四 到 六 年 ， 现 在 服 刑 。 厂 子 给 宣 国 才 强 占 ， 宣 国 才 每 天 赚 20 多 万 净 利 润 ， 却 对 外 宣 称 天 天 亏 本 ！ 等 咱 老 板 刑 满 回 厂 ， 宣 国 才 给 咱 厂 [UNK] 天 天 亏 [UNK] 可 能 要 [UNK] 亏 [UNK] 的 几 千 万 元 ， 甚 至 几 个 亿 ， 张 涛 书 记 您 承 担 还 是 上 黄 政 府 承 担 ？ 当 初 可 是 您 亲 自 把 厂 交 给 宣 国 才 生 产 的 ！ 希 望 徐 市 长 看 到 本 贴 后 能 像 批 示 263 、 批 示 违 建 等 民 生 问 题 一 样 ， 关 注 一 下 我 们 水 泥 厂 的 将 来 ！ 也 请 徐 市 长 抽 日 理 万 机 之 空 亲 自 约 谈 一 下 当 事 人 （ 特 别 是 那 位 施 站 长 ） ， 千 万 不 能 听 取 一 面 之 辞 [SEP]\n",
      "11/14/2019 03:32:57 - INFO - __main__ -   input_ids: 101 7309 6569 7566 2193 677 7942 7252 1054 1999 741 6381 2476 3875 8024 2146 1744 2798 4696 5543 671 2797 6902 1921 1408 8043 102 1400 8024 2798 1355 4385 3177 3378 1126 1921 1184 1726 1908 5381 1351 4638 1905 4415 5310 3362 4994 1963 678 1745 8038 9180 9263 8662 11365 8887 8311 9169 9676 8743 9492 9624 9153 9216 8805 9180 10969 9263 8662 11365 8887 8311 9169 9676 8743 9492 9624 9153 9216 8805 9180 1745 4275 8038 13249 8311 9169 8493 8717 9215 8717 9148 9488 9097 9492 4385 6569 7309 2476 3875 741 6381 8038 122 510 2146 1744 2798 6158 715 2845 6821 720 1914 7309 7579 8024 784 720 3198 952 3300 5031 1908 511 123 510 2146 1744 2798 6158 715 2845 1400 8024 711 784 720 6158 4989 1174 1048 749 3333 741 6381 5466 1218 8043 711 784 720 1348 6158 2128 2961 1168 1814 5052 7339 100 1391 4958 1510 100 8024 5632 2346 1316 1921 1921 1762 2769 812 3717 3799 1322 677 4408 6611 7946 7178 8043 124 510 6821 1126 702 3299 8024 3717 3799 3680 1417 6818 8185 1039 5283 1164 3883 8024 6820 897 679 2418 3724 8024 2146 1744 2798 6820 3926 677 7942 3124 2424 2857 924 955 5314 2146 1744 2798 807 802 2920 691 1322 2339 6598 4852 924 4638 7178 749 1408 8043 125 510 2945 749 6237 2146 1744 2798 1304 800 782 821 689 5307 5852 8024 1348 3612 4925 11411 8158 674 1039 510 3612 4852 924 12094 8158 674 1039 510 2418 6421 6820 3612 749 5466 2339 2339 6598 1126 1282 674 8024 677 7942 3124 2424 2802 5050 3296 2146 1744 2798 2857 924 6820 3221 2495 6820 8043 126 510 2769 812 1322 1394 3791 833 6369 1469 5439 3352 6158 1161 1152 1724 1168 1063 2399 8024 4385 1762 3302 1152 511 1322 2094 5314 2146 1744 2798 2487 1304 8024 2146 1744 2798 3680 1921 6611 8113 1914 674 1112 1164 3883 8024 1316 2190 1912 2146 4917 1921 1921 755 3315 8013 5023 1493 5439 3352 1152 4007 1726 1322 8024 2146 1744 2798 5314 1493 1322 100 1921 1921 755 100 1377 5543 6206 100 755 100 4638 1126 1283 674 1039 8024 4493 5635 1126 702 783 8024 2476 3875 741 6381 2644 2824 2857 6820 3221 677 7942 3124 2424 2824 2857 8043 2496 1159 1377 3221 2644 779 5632 2828 1322 769 5314 2146 1744 2798 4495 772 4638 8013 2361 3307 2528 2356 7270 4692 1168 3315 6585 1400 5543 1008 2821 4850 10864 510 2821 4850 6824 2456 5023 3696 4495 7309 7579 671 3416 8024 1068 3800 671 678 2769 812 3717 3799 1322 4638 2199 3341 8013 738 6435 2528 2356 7270 2853 3189 4415 674 3322 722 4958 779 5632 5276 6448 671 678 2496 752 782 8020 4294 1166 3221 6929 855 3177 4991 7270 8021 8024 1283 674 679 5543 1420 1357 671 7481 722 6791 102\n",
      "11/14/2019 03:32:57 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/14/2019 03:32:57 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/14/2019 03:32:57 - INFO - __main__ -   label: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/14/2019 03:34:26 - INFO - __main__ -   ***** Running training *****\n",
      "11/14/2019 03:34:26 - INFO - __main__ -     Num examples = 11757\n",
      "11/14/2019 03:34:26 - INFO - __main__ -     Batch size = 4\n",
      "11/14/2019 03:34:26 - INFO - __main__ -     Num steps = 30000\n",
      "  0%|                                                 | 0/30000 [00:00<?, ?it/s]11/14/2019 03:34:51 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 03:34:51 - INFO - __main__ -     Num examples = 2939\n",
      "11/14/2019 03:34:51 - INFO - __main__ -     Batch size = 48\n",
      "/root/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "11/14/2019 03:36:56 - INFO - __main__ -     eval_F1 = 0.20621648435738948\n",
      "11/14/2019 03:36:56 - INFO - __main__ -     eval_loss = 1.324413255337746\n",
      "11/14/2019 03:36:56 - INFO - __main__ -     global_step = 0\n",
      "================================================================================\n",
      "Best F1 0.20621648435738948\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.6872:   3%|▋                       | 799/30000 [04:41<1:27:29,  5.56it/s]11/14/2019 03:39:07 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 03:39:07 - INFO - __main__ -     global_step = 200\n",
      "11/14/2019 03:39:07 - INFO - __main__ -     train loss = 0.6872\n",
      "loss 0.5131:   5%|█▏                     | 1599/30000 [06:52<1:25:22,  5.54it/s]11/14/2019 03:41:19 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 03:41:19 - INFO - __main__ -     global_step = 400\n",
      "11/14/2019 03:41:19 - INFO - __main__ -     train loss = 0.5131\n",
      "loss 0.4515:   8%|█▊                     | 2399/30000 [09:03<1:22:12,  5.60it/s]11/14/2019 03:43:30 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 03:43:30 - INFO - __main__ -     global_step = 600\n",
      "11/14/2019 03:43:30 - INFO - __main__ -     train loss = 0.4515\n",
      "loss 0.4325:  11%|██▍                    | 3199/30000 [11:14<1:20:54,  5.52it/s]11/14/2019 03:45:40 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 03:45:40 - INFO - __main__ -     global_step = 800\n",
      "11/14/2019 03:45:40 - INFO - __main__ -     train loss = 0.4325\n",
      "loss 0.4487:  13%|███                    | 3999/30000 [13:24<1:17:03,  5.62it/s]11/14/2019 03:47:51 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 03:47:51 - INFO - __main__ -     global_step = 1000\n",
      "11/14/2019 03:47:51 - INFO - __main__ -     train loss = 0.4487\n",
      "loss 0.6041:  16%|███▋                   | 4799/30000 [15:36<1:14:58,  5.60it/s]11/14/2019 03:50:02 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 03:50:02 - INFO - __main__ -     global_step = 1200\n",
      "11/14/2019 03:50:02 - INFO - __main__ -     train loss = 0.6041\n",
      "loss 0.4703:  19%|████▎                  | 5599/30000 [17:45<1:12:27,  5.61it/s]11/14/2019 03:52:12 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 03:52:12 - INFO - __main__ -     global_step = 1400\n",
      "11/14/2019 03:52:12 - INFO - __main__ -     train loss = 0.4703\n",
      "loss 0.377:  21%|█████                   | 6399/30000 [19:55<1:09:55,  5.63it/s]11/14/2019 03:54:21 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 03:54:21 - INFO - __main__ -     global_step = 1600\n",
      "11/14/2019 03:54:21 - INFO - __main__ -     train loss = 0.377\n",
      "loss 0.354:  24%|█████▊                  | 7199/30000 [22:05<1:08:58,  5.51it/s]11/14/2019 03:56:31 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 03:56:31 - INFO - __main__ -     global_step = 1800\n",
      "11/14/2019 03:56:31 - INFO - __main__ -     train loss = 0.354\n",
      "loss 0.3435:  27%|██████▏                | 7999/30000 [24:16<1:05:58,  5.56it/s]11/14/2019 03:58:42 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 03:58:42 - INFO - __main__ -     global_step = 2000\n",
      "11/14/2019 03:58:42 - INFO - __main__ -     train loss = 0.3435\n",
      "loss 0.3769:  29%|██████▋                | 8799/30000 [26:27<1:03:42,  5.55it/s]11/14/2019 04:00:54 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 04:00:54 - INFO - __main__ -     global_step = 2200\n",
      "11/14/2019 04:00:54 - INFO - __main__ -     train loss = 0.3769\n",
      "loss 0.3887:  32%|███████▎               | 9599/30000 [28:38<1:01:08,  5.56it/s]11/14/2019 04:03:05 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 04:03:05 - INFO - __main__ -     global_step = 2400\n",
      "11/14/2019 04:03:05 - INFO - __main__ -     train loss = 0.3887\n",
      "loss 0.3858:  35%|████████▎               | 10399/30000 [30:49<58:36,  5.57it/s]11/14/2019 04:05:16 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 04:05:16 - INFO - __main__ -     global_step = 2600\n",
      "11/14/2019 04:05:16 - INFO - __main__ -     train loss = 0.3858\n",
      "loss 0.4005:  37%|████████▉               | 11199/30000 [32:59<55:28,  5.65it/s]11/14/2019 04:07:26 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 04:07:26 - INFO - __main__ -     global_step = 2800\n",
      "11/14/2019 04:07:26 - INFO - __main__ -     train loss = 0.4005\n",
      "11/14/2019 04:07:48 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 04:07:48 - INFO - __main__ -     Num examples = 2939\n",
      "11/14/2019 04:07:48 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 04:09:55 - INFO - __main__ -     eval_F1 = 0.7796991395716897\n",
      "11/14/2019 04:09:55 - INFO - __main__ -     eval_loss = 0.37853801995515823\n",
      "11/14/2019 04:09:55 - INFO - __main__ -     global_step = 2800\n",
      "11/14/2019 04:09:55 - INFO - __main__ -     loss = 0.4005\n",
      "================================================================================\n",
      "Best F1 0.7796991395716897\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.3436:  40%|█████████▌              | 11999/30000 [37:43<53:46,  5.58it/s]11/14/2019 04:12:09 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 04:12:09 - INFO - __main__ -     global_step = 3000\n",
      "11/14/2019 04:12:09 - INFO - __main__ -     train loss = 0.3436\n",
      "11/14/2019 04:12:31 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 04:12:31 - INFO - __main__ -     Num examples = 2939\n",
      "11/14/2019 04:12:31 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 04:14:38 - INFO - __main__ -     eval_F1 = 0.7638071875253546\n",
      "11/14/2019 04:14:38 - INFO - __main__ -     eval_loss = 0.36618270196260944\n",
      "11/14/2019 04:14:38 - INFO - __main__ -     global_step = 3000\n",
      "11/14/2019 04:14:38 - INFO - __main__ -     loss = 0.3436\n",
      "================================================================================\n",
      "loss 0.279:  43%|██████████▋              | 12799/30000 [42:22<51:53,  5.52it/s]11/14/2019 04:16:49 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 04:16:49 - INFO - __main__ -     global_step = 3200\n",
      "11/14/2019 04:16:49 - INFO - __main__ -     train loss = 0.279\n",
      "11/14/2019 04:17:10 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 04:17:10 - INFO - __main__ -     Num examples = 2939\n",
      "11/14/2019 04:17:10 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 04:19:16 - INFO - __main__ -     eval_F1 = 0.7759914536476051\n",
      "11/14/2019 04:19:16 - INFO - __main__ -     eval_loss = 0.43338144319732824\n",
      "11/14/2019 04:19:16 - INFO - __main__ -     global_step = 3200\n",
      "11/14/2019 04:19:16 - INFO - __main__ -     loss = 0.279\n",
      "================================================================================\n",
      "loss 0.2355:  45%|██████████▉             | 13599/30000 [47:01<49:38,  5.51it/s]11/14/2019 04:21:28 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 04:21:28 - INFO - __main__ -     global_step = 3400\n",
      "11/14/2019 04:21:28 - INFO - __main__ -     train loss = 0.2355\n",
      "11/14/2019 04:21:49 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 04:21:49 - INFO - __main__ -     Num examples = 2939\n",
      "11/14/2019 04:21:49 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 04:23:56 - INFO - __main__ -     eval_F1 = 0.779060265415494\n",
      "11/14/2019 04:23:56 - INFO - __main__ -     eval_loss = 0.3895908145294074\n",
      "11/14/2019 04:23:56 - INFO - __main__ -     global_step = 3400\n",
      "11/14/2019 04:23:56 - INFO - __main__ -     loss = 0.2355\n",
      "================================================================================\n",
      "loss 0.2324:  48%|███████████▌            | 14399/30000 [51:40<46:24,  5.60it/s]11/14/2019 04:26:06 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 04:26:06 - INFO - __main__ -     global_step = 3600\n",
      "11/14/2019 04:26:06 - INFO - __main__ -     train loss = 0.2324\n",
      "11/14/2019 04:26:27 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 04:26:27 - INFO - __main__ -     Num examples = 2939\n",
      "11/14/2019 04:26:27 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 04:28:34 - INFO - __main__ -     eval_F1 = 0.7949944563005031\n",
      "11/14/2019 04:28:34 - INFO - __main__ -     eval_loss = 0.3932495667569099\n",
      "11/14/2019 04:28:34 - INFO - __main__ -     global_step = 3600\n",
      "11/14/2019 04:28:34 - INFO - __main__ -     loss = 0.2324\n",
      "================================================================================\n",
      "Best F1 0.7949944563005031\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.1996:  51%|████████████▏           | 15199/30000 [56:21<44:12,  5.58it/s]11/14/2019 04:30:47 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 04:30:47 - INFO - __main__ -     global_step = 3800\n",
      "11/14/2019 04:30:47 - INFO - __main__ -     train loss = 0.1996\n",
      "11/14/2019 04:31:08 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 04:31:08 - INFO - __main__ -     Num examples = 2939\n",
      "11/14/2019 04:31:08 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 04:33:15 - INFO - __main__ -     eval_F1 = 0.7886291205430286\n",
      "11/14/2019 04:33:15 - INFO - __main__ -     eval_loss = 0.4601638790430321\n",
      "11/14/2019 04:33:15 - INFO - __main__ -     global_step = 3800\n",
      "11/14/2019 04:33:15 - INFO - __main__ -     loss = 0.1996\n",
      "================================================================================\n",
      "loss 0.2701:  53%|███████████▋          | 15999/30000 [1:00:59<42:19,  5.51it/s]11/14/2019 04:35:26 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 04:35:26 - INFO - __main__ -     global_step = 4000\n",
      "11/14/2019 04:35:26 - INFO - __main__ -     train loss = 0.2701\n",
      "11/14/2019 04:35:47 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 04:35:47 - INFO - __main__ -     Num examples = 2939\n",
      "11/14/2019 04:35:47 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 04:37:53 - INFO - __main__ -     eval_F1 = 0.7745391177353063\n",
      "11/14/2019 04:37:53 - INFO - __main__ -     eval_loss = 0.42392047894217316\n",
      "11/14/2019 04:37:53 - INFO - __main__ -     global_step = 4000\n",
      "11/14/2019 04:37:53 - INFO - __main__ -     loss = 0.2701\n",
      "================================================================================\n",
      "loss 0.3181:  56%|████████████▎         | 16799/30000 [1:05:39<39:56,  5.51it/s]11/14/2019 04:40:05 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 04:40:05 - INFO - __main__ -     global_step = 4200\n",
      "11/14/2019 04:40:05 - INFO - __main__ -     train loss = 0.3181\n",
      "11/14/2019 04:40:27 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 04:40:27 - INFO - __main__ -     Num examples = 2939\n",
      "11/14/2019 04:40:27 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 04:42:34 - INFO - __main__ -     eval_F1 = 0.7877711870783317\n",
      "11/14/2019 04:42:34 - INFO - __main__ -     eval_loss = 0.3660362954702108\n",
      "11/14/2019 04:42:34 - INFO - __main__ -     global_step = 4200\n",
      "11/14/2019 04:42:34 - INFO - __main__ -     loss = 0.3181\n",
      "================================================================================\n",
      "loss 0.2589:  59%|████████████▉         | 17599/30000 [1:10:17<36:42,  5.63it/s]11/14/2019 04:44:44 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 04:44:44 - INFO - __main__ -     global_step = 4400\n",
      "11/14/2019 04:44:44 - INFO - __main__ -     train loss = 0.2589\n",
      "11/14/2019 04:45:05 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 04:45:05 - INFO - __main__ -     Num examples = 2939\n",
      "11/14/2019 04:45:05 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 04:47:11 - INFO - __main__ -     eval_F1 = 0.7932773679728422\n",
      "11/14/2019 04:47:11 - INFO - __main__ -     eval_loss = 0.39294020413991904\n",
      "11/14/2019 04:47:11 - INFO - __main__ -     global_step = 4400\n",
      "11/14/2019 04:47:11 - INFO - __main__ -     loss = 0.2589\n",
      "================================================================================\n",
      "loss 0.2308:  61%|█████████████▍        | 18399/30000 [1:14:55<34:38,  5.58it/s]11/14/2019 04:49:21 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 04:49:21 - INFO - __main__ -     global_step = 4600\n",
      "11/14/2019 04:49:21 - INFO - __main__ -     train loss = 0.2308\n",
      "11/14/2019 04:49:47 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 04:49:47 - INFO - __main__ -     Num examples = 2939\n",
      "11/14/2019 04:49:47 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 04:51:53 - INFO - __main__ -     eval_F1 = 0.7919986309773477\n",
      "11/14/2019 04:51:53 - INFO - __main__ -     eval_loss = 0.3841439536021602\n",
      "11/14/2019 04:51:53 - INFO - __main__ -     global_step = 4600\n",
      "11/14/2019 04:51:53 - INFO - __main__ -     loss = 0.2308\n",
      "================================================================================\n",
      "loss 0.2083:  64%|██████████████        | 19199/30000 [1:19:38<32:39,  5.51it/s]11/14/2019 04:54:04 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 04:54:04 - INFO - __main__ -     global_step = 4800\n",
      "11/14/2019 04:54:04 - INFO - __main__ -     train loss = 0.2083\n",
      "11/14/2019 04:54:26 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 04:54:26 - INFO - __main__ -     Num examples = 2939\n",
      "11/14/2019 04:54:26 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 04:56:32 - INFO - __main__ -     eval_F1 = 0.7939975388420187\n",
      "11/14/2019 04:56:32 - INFO - __main__ -     eval_loss = 0.41207000315790215\n",
      "11/14/2019 04:56:32 - INFO - __main__ -     global_step = 4800\n",
      "11/14/2019 04:56:32 - INFO - __main__ -     loss = 0.2083\n",
      "================================================================================\n",
      "loss 0.1931:  67%|██████████████▋       | 19999/30000 [1:24:16<29:41,  5.61it/s]11/14/2019 04:58:42 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 04:58:42 - INFO - __main__ -     global_step = 5000\n",
      "11/14/2019 04:58:42 - INFO - __main__ -     train loss = 0.1931\n",
      "11/14/2019 04:59:03 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 04:59:03 - INFO - __main__ -     Num examples = 2939\n",
      "11/14/2019 04:59:03 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 05:01:09 - INFO - __main__ -     eval_F1 = 0.7986719181429547\n",
      "11/14/2019 05:01:09 - INFO - __main__ -     eval_loss = 0.4077195098563548\n",
      "11/14/2019 05:01:09 - INFO - __main__ -     global_step = 5000\n",
      "11/14/2019 05:01:09 - INFO - __main__ -     loss = 0.1931\n",
      "================================================================================\n",
      "Best F1 0.7986719181429547\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.2221:  69%|███████████████▎      | 20799/30000 [1:28:57<27:28,  5.58it/s]11/14/2019 05:03:23 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 05:03:23 - INFO - __main__ -     global_step = 5200\n",
      "11/14/2019 05:03:23 - INFO - __main__ -     train loss = 0.2221\n",
      "11/14/2019 05:03:45 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 05:03:45 - INFO - __main__ -     Num examples = 2939\n",
      "11/14/2019 05:03:45 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 05:05:51 - INFO - __main__ -     eval_F1 = 0.7882033464816395\n",
      "11/14/2019 05:05:51 - INFO - __main__ -     eval_loss = 0.3878000057152202\n",
      "11/14/2019 05:05:51 - INFO - __main__ -     global_step = 5200\n",
      "11/14/2019 05:05:51 - INFO - __main__ -     loss = 0.2221\n",
      "================================================================================\n",
      "loss 0.24:  72%|█████████████████▎      | 21599/30000 [1:33:35<25:03,  5.59it/s]11/14/2019 05:08:02 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 05:08:02 - INFO - __main__ -     global_step = 5400\n",
      "11/14/2019 05:08:02 - INFO - __main__ -     train loss = 0.24\n",
      "11/14/2019 05:08:25 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 05:08:25 - INFO - __main__ -     Num examples = 2939\n",
      "11/14/2019 05:08:25 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 05:10:31 - INFO - __main__ -     eval_F1 = 0.7885067262714148\n",
      "11/14/2019 05:10:31 - INFO - __main__ -     eval_loss = 0.3705126769360035\n",
      "11/14/2019 05:10:31 - INFO - __main__ -     global_step = 5400\n",
      "11/14/2019 05:10:31 - INFO - __main__ -     loss = 0.24\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.2236:  75%|████████████████▍     | 22399/30000 [1:38:17<22:50,  5.54it/s]11/14/2019 05:12:43 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 05:12:43 - INFO - __main__ -     global_step = 5600\n",
      "11/14/2019 05:12:43 - INFO - __main__ -     train loss = 0.2236\n",
      "11/14/2019 05:13:05 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 05:13:05 - INFO - __main__ -     Num examples = 2939\n",
      "11/14/2019 05:13:05 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 05:15:12 - INFO - __main__ -     eval_F1 = 0.7921735722726276\n",
      "11/14/2019 05:15:12 - INFO - __main__ -     eval_loss = 0.3706089349824094\n",
      "11/14/2019 05:15:12 - INFO - __main__ -     global_step = 5600\n",
      "11/14/2019 05:15:12 - INFO - __main__ -     loss = 0.2236\n",
      "================================================================================\n",
      "loss 0.2501:  77%|█████████████████     | 23199/30000 [1:42:56<20:36,  5.50it/s]11/14/2019 05:17:23 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 05:17:23 - INFO - __main__ -     global_step = 5800\n",
      "11/14/2019 05:17:23 - INFO - __main__ -     train loss = 0.2501\n",
      "11/14/2019 05:17:45 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 05:17:45 - INFO - __main__ -     Num examples = 2939\n",
      "11/14/2019 05:17:45 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 05:19:51 - INFO - __main__ -     eval_F1 = 0.7931827896696501\n",
      "11/14/2019 05:19:51 - INFO - __main__ -     eval_loss = 0.37806114338098035\n",
      "11/14/2019 05:19:51 - INFO - __main__ -     global_step = 5800\n",
      "11/14/2019 05:19:51 - INFO - __main__ -     loss = 0.2501\n",
      "================================================================================\n",
      "loss 0.1846:  80%|█████████████████▌    | 23999/30000 [1:47:36<17:57,  5.57it/s]11/14/2019 05:22:03 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 05:22:03 - INFO - __main__ -     global_step = 6000\n",
      "11/14/2019 05:22:03 - INFO - __main__ -     train loss = 0.1846\n",
      "11/14/2019 05:22:25 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 05:22:25 - INFO - __main__ -     Num examples = 2939\n",
      "11/14/2019 05:22:25 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 05:24:31 - INFO - __main__ -     eval_F1 = 0.7929718323539579\n",
      "11/14/2019 05:24:31 - INFO - __main__ -     eval_loss = 0.40215543531362086\n",
      "11/14/2019 05:24:31 - INFO - __main__ -     global_step = 6000\n",
      "11/14/2019 05:24:31 - INFO - __main__ -     loss = 0.1846\n",
      "================================================================================\n",
      "loss 0.1537:  83%|██████████████████▏   | 24799/30000 [1:52:15<15:33,  5.57it/s]11/14/2019 05:26:42 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 05:26:42 - INFO - __main__ -     global_step = 6200\n",
      "11/14/2019 05:26:42 - INFO - __main__ -     train loss = 0.1537\n",
      "11/14/2019 05:27:07 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 05:27:07 - INFO - __main__ -     Num examples = 2939\n",
      "11/14/2019 05:27:07 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 05:29:13 - INFO - __main__ -     eval_F1 = 0.7874480819121071\n",
      "11/14/2019 05:29:13 - INFO - __main__ -     eval_loss = 0.42396285578668597\n",
      "11/14/2019 05:29:13 - INFO - __main__ -     global_step = 6200\n",
      "11/14/2019 05:29:13 - INFO - __main__ -     loss = 0.1537\n",
      "================================================================================\n",
      "loss 0.1247:  85%|██████████████████▊   | 25599/30000 [1:56:57<13:06,  5.60it/s]11/14/2019 05:31:24 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 05:31:24 - INFO - __main__ -     global_step = 6400\n",
      "11/14/2019 05:31:24 - INFO - __main__ -     train loss = 0.1247\n",
      "11/14/2019 05:31:45 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 05:31:45 - INFO - __main__ -     Num examples = 2939\n",
      "11/14/2019 05:31:45 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 05:33:51 - INFO - __main__ -     eval_F1 = 0.7981968884157769\n",
      "11/14/2019 05:33:51 - INFO - __main__ -     eval_loss = 0.4111948861259847\n",
      "11/14/2019 05:33:51 - INFO - __main__ -     global_step = 6400\n",
      "11/14/2019 05:33:51 - INFO - __main__ -     loss = 0.1247\n",
      "================================================================================\n",
      "loss 0.1043:  88%|███████████████████▎  | 26399/30000 [2:01:35<10:44,  5.59it/s]11/14/2019 05:36:01 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 05:36:01 - INFO - __main__ -     global_step = 6600\n",
      "11/14/2019 05:36:01 - INFO - __main__ -     train loss = 0.1043\n",
      "11/14/2019 05:36:26 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 05:36:26 - INFO - __main__ -     Num examples = 2939\n",
      "11/14/2019 05:36:26 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 05:38:32 - INFO - __main__ -     eval_F1 = 0.7975041607334078\n",
      "11/14/2019 05:38:32 - INFO - __main__ -     eval_loss = 0.4443053624427487\n",
      "11/14/2019 05:38:32 - INFO - __main__ -     global_step = 6600\n",
      "11/14/2019 05:38:32 - INFO - __main__ -     loss = 0.1043\n",
      "================================================================================\n",
      "loss 0.1294:  91%|███████████████████▉  | 27199/30000 [2:06:17<08:30,  5.48it/s]11/14/2019 05:40:44 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 05:40:44 - INFO - __main__ -     global_step = 6800\n",
      "11/14/2019 05:40:44 - INFO - __main__ -     train loss = 0.1294\n",
      "11/14/2019 05:41:05 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 05:41:05 - INFO - __main__ -     Num examples = 2939\n",
      "11/14/2019 05:41:05 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 05:43:11 - INFO - __main__ -     eval_F1 = 0.7978772041305312\n",
      "11/14/2019 05:43:11 - INFO - __main__ -     eval_loss = 0.4331368156032817\n",
      "11/14/2019 05:43:11 - INFO - __main__ -     global_step = 6800\n",
      "11/14/2019 05:43:11 - INFO - __main__ -     loss = 0.1294\n",
      "================================================================================\n",
      "loss 0.1315:  93%|████████████████████▌ | 27999/30000 [2:10:56<06:02,  5.52it/s]11/14/2019 05:45:23 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 05:45:23 - INFO - __main__ -     global_step = 7000\n",
      "11/14/2019 05:45:23 - INFO - __main__ -     train loss = 0.1315\n",
      "11/14/2019 05:45:45 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 05:45:45 - INFO - __main__ -     Num examples = 2939\n",
      "11/14/2019 05:45:45 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 05:47:51 - INFO - __main__ -     eval_F1 = 0.7955640281082506\n",
      "11/14/2019 05:47:51 - INFO - __main__ -     eval_loss = 0.4389137772903327\n",
      "11/14/2019 05:47:51 - INFO - __main__ -     global_step = 7000\n",
      "11/14/2019 05:47:51 - INFO - __main__ -     loss = 0.1315\n",
      "================================================================================\n",
      "loss 0.1243:  96%|█████████████████████ | 28799/30000 [2:15:37<03:38,  5.50it/s]11/14/2019 05:50:04 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 05:50:04 - INFO - __main__ -     global_step = 7200\n",
      "11/14/2019 05:50:04 - INFO - __main__ -     train loss = 0.1243\n",
      "11/14/2019 05:50:26 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 05:50:26 - INFO - __main__ -     Num examples = 2939\n",
      "11/14/2019 05:50:26 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 05:52:32 - INFO - __main__ -     eval_F1 = 0.7990296042189696\n",
      "11/14/2019 05:52:32 - INFO - __main__ -     eval_loss = 0.4330124922186857\n",
      "11/14/2019 05:52:32 - INFO - __main__ -     global_step = 7200\n",
      "11/14/2019 05:52:32 - INFO - __main__ -     loss = 0.1243\n",
      "================================================================================\n",
      "Best F1 0.7990296042189696\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.1312:  99%|█████████████████████▋| 29599/30000 [2:20:22<01:12,  5.51it/s]11/14/2019 05:54:48 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 05:54:48 - INFO - __main__ -     global_step = 7400\n",
      "11/14/2019 05:54:48 - INFO - __main__ -     train loss = 0.1312\n",
      "11/14/2019 05:55:10 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 05:55:10 - INFO - __main__ -     Num examples = 2939\n",
      "11/14/2019 05:55:10 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 05:57:16 - INFO - __main__ -     eval_F1 = 0.7988950250055925\n",
      "11/14/2019 05:57:16 - INFO - __main__ -     eval_loss = 0.43584929128748273\n",
      "11/14/2019 05:57:16 - INFO - __main__ -     global_step = 7400\n",
      "11/14/2019 05:57:16 - INFO - __main__ -     loss = 0.1312\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.1206: 100%|██████████████████████| 30000/30000 [2:23:55<00:00,  6.21it/s]\n",
      "11/14/2019 05:58:22 - INFO - pytorch_transformers.modeling_utils -   loading weights file ../model/roberta_wwm_large_512_1_last2embedding_cls_replacement/roberta_wwm_large_512_1_last2embedding_cls_replacement_2/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "Traceback (most recent call last):\n",
      "  File \"./run_bert_2562_last2embedding_cls.py\", line 841, in <module>\n",
      "    main()\n",
      "  File \"./run_bert_2562_last2embedding_cls.py\", line 757, in main\n",
      "    logits = model(input_ids=input_ids, token_type_ids=segment_ids, attention_mask=input_mask).detach().cpu().numpy()\n",
      "  File \"/root/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 547, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/root/code/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 1119, in forward\n",
      "    attention_mask=flat_attention_mask, head_mask=head_mask)\n",
      "  File \"/root/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 547, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/root/code/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 727, in forward\n",
      "    head_mask=head_mask)\n",
      "  File \"/root/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 547, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/root/code/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 440, in forward\n",
      "    layer_outputs = layer_module(hidden_states, attention_mask, head_mask[i])\n",
      "  File \"/root/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 547, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/root/code/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 420, in forward\n",
      "    intermediate_output = self.intermediate(attention_output)\n",
      "  File \"/root/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 547, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/root/code/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 392, in forward\n",
      "    hidden_states = self.intermediate_act_fn(hidden_states)\n",
      "  File \"/root/code/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 142, in gelu\n",
      "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 384.00 MiB (GPU 0; 10.73 GiB total capacity; 8.60 GiB already allocated; 339.62 MiB free; 906.30 MiB cached)\n"
     ]
    }
   ],
   "source": [
    "!python ./run_bert_2562_last2embedding_cls.py \\\n",
    "--model_type bert \\\n",
    "--model_name_or_path ../model/chinese_roberta_wwm_large_ext_pytorch  \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_test \\\n",
    "--data_dir ../data/data_StratifiedKFold_42/data_replacement_2 \\\n",
    "--output_dir ../model/roberta_wwm_large_512_1_last2embedding_cls_replacement/roberta_wwm_large_512_1_last2embedding_cls_replacement_2 \\\n",
    "--max_seq_length 512 \\\n",
    "--split_num 1 \\\n",
    "--lstm_hidden_size 512 \\\n",
    "--lstm_layers 1 \\\n",
    "--lstm_dropout 0.1 \\\n",
    "--eval_steps 200 \\\n",
    "--per_gpu_train_batch_size 4 \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--warmup_steps 0 \\\n",
    "--per_gpu_eval_batch_size 48 \\\n",
    "--learning_rate 1e-5 \\\n",
    "--adam_epsilon 1e-6 \\\n",
    "--weight_decay 0 \\\n",
    "--train_steps 30000 \\\n",
    "--freeze 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/14/2019 05:58:55 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "11/14/2019 05:58:55 - INFO - pytorch_transformers.tokenization_utils -   Model name '../model/chinese_roberta_wwm_large_ext_pytorch' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming '../model/chinese_roberta_wwm_large_ext_pytorch' is a path or url to a directory containing tokenizer files.\n",
      "11/14/2019 05:58:55 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../model/chinese_roberta_wwm_large_ext_pytorch/added_tokens.json. We won't load it.\n",
      "11/14/2019 05:58:55 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../model/chinese_roberta_wwm_large_ext_pytorch/special_tokens_map.json. We won't load it.\n",
      "11/14/2019 05:58:55 - INFO - pytorch_transformers.tokenization_utils -   loading file ../model/chinese_roberta_wwm_large_ext_pytorch/vocab.txt\n",
      "11/14/2019 05:58:55 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/14/2019 05:58:55 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/14/2019 05:58:55 - INFO - pytorch_transformers.modeling_utils -   loading configuration file ../model/chinese_roberta_wwm_large_ext_pytorch/config.json\n",
      "11/14/2019 05:58:55 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 3,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "11/14/2019 05:58:55 - INFO - pytorch_transformers.modeling_utils -   loading weights file ../model/chinese_roberta_wwm_large_ext_pytorch/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "11/14/2019 05:59:04 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification_last2embedding_cls not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "11/14/2019 05:59:04 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification_last2embedding_cls: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "11/14/2019 05:59:11 - INFO - __main__ -   ** RAW EXAMPLE **\n",
      "11/14/2019 05:59:11 - INFO - __main__ -   content: ['过', '去', '一', '年', '的', '江', '歌', '悲', '剧', '，', '这', '几', '日', '再', '次', '刷', '屏', '：', '住', '在', '东', '京', '都', '中', '野', '区', '的', '中', '国', '女', '留', '学', '生', '江', '歌', '，', '收', '留', '了', '被', '前', '男', '友', '陈', '世', '锋', '恶', '意', '纠', '缠', '的', '闺', '蜜', '刘', '鑫', '，', '两', '人', '在', '回', '到', '江', '歌', '公', '寓', '楼', '时', '，', '陈', '世', '锋', '已', '经', '等', '在', '楼', '下', '，', '叫', '嚣', '着', '要', '刘', '鑫', '给', '自', '己', '一', '个', '说', '法', '（', '男', '友', '此', '时', '的', '情', '绪', '处', '于', '濒', '临', '崩', '溃', '的', '状', '态', '）', '。', '江', '歌', '为', '了', '保', '护', '刘', '鑫', '，', '就', '让', '她', '先', '进', '了', '房', '间', '，', '自', '己', '拦', '在', '外', '面', '要', '求', '陈', '世', '锋', '离', '开', '。', '结', '果', '江', '歌', '被', '陈', '世', '锋', '用', '刀', '多', '处', '刺', '伤', '脖', '子', '和', '胸', '部', '，', '刀', '刀', '毙', '命', '，', '残', '忍', '至', '极', '，', '最', '终', '因', '失', '血', '过', '多', '丧', '生', '。', '十', '几', '刀', '，', '刘', '鑫', '躲', '在', '屋', '里', '，', '躲', '在', '门', '后', '，', '亲', '耳', '听', '着', '闺', '蜜', '江', '歌', '的', '声', '声', '求', '助', '及', '惨', '叫', '，', '却', '始', '终', '没', '有', '打', '开', '门', '。', '连', '邻', '居', '都', '听', '到', '呼', '救', '纷', '纷', '开', '门', '查', '看', '究', '竟', '，', '那', '扇', '可', '以', '救', '命', '的', '们', '始', '终', '没', '有', '打', '开', '。', '江', '歌', '死', '后', '，', '刘', '鑫', '面', '对', '警', '方', '的', '询', '问', '，', '称', '自', '己', '一', '无', '所', '知', '，', '什', '么', '都', '没', '有', '听', '见', '，', '不', '肯', '出', '来', '指', '证', '凶', '手', '、', '为', '江', '歌', '伸', '冤', '，', '只', '想', '着', '撇', '清', '关', '系', '、', '澄', '清', '自', '己', '，', '任', '江', '歌', '遭', '外', '界', '议', '论', '指', '摘', '，', '甚', '至', '拒', '绝', '同', '江', '歌', '妈', '妈', '联', '系', '，', '自', '己', '做', '了', '新', '头', '发', '买', '了', '新', '包', '包', '快', '快', '乐', '乐', '地', '过', '着', '自', '己', '的', '生', '活', '，', '暗', '自', '庆', '幸', '自', '己', '终', '于', '摆', '脱', '了', '那', '个', '纠', '缠', '自', '己', '的', '前', '男', '友', '。', '迫', '于', '舆', '论', '的', '压', '力', '，', '刘', '鑫', '终', '于', '出', '现', '了', '。', '江', '歌', '为', '刘', '鑫', '失', '去', '了', '生', '命', '，', '可', '在', '刘', '鑫', '眼', '里', '，', '江', '歌', '的', '一', '条', '命', '抵', '不', '过', '自', '己', '的', '名', '声', '，', '她', '只', '在', '意', '自', '己', '及', '家', '人', '的', '生', '活', '受', '到', '了', '严', '重', '的', '影', '响', '，', '认', '为', '自', '己', '知', '道', '错', '了', '江', '歌', '妈', '妈', '就', '该', '原', '谅', '她', '了', '，', '不', '应', '该', '揪', '着', '她', '不', '放', '。', '甚', '至', '在', '事', '搁', '一', '年', '后', '首', '次', '与', '她', '死', '者', '母', '亲', '见', '面', '，', '还', '身', '穿', '艳', '色', '裤', '子', '，', '不', '摘', '帽', '子', '丝', '毫', '没', '有', '尊', '重', '。', '江', '歌', '遇', '害', '至', '今', '，', '她', '每', '一', '次', '出', '现', '在', '网', '络', '那', '头', '，', '都', '是', '在', '为', '自', '己', '争', '辩', '，', '为', '自', '己', '的', '利', '益', '作', '斗', '争', '。', '如', '果', '说', '，', '江', '歌', '的', '正', '直', '与', '善', '良', '，', '是', '来', '自', '江', '歌', '妈', '妈', '的', '言', '传', '身', '教', '。', '那', '刘', '鑫', '的', '自', '私', '与', '冷', '漠', '呢', '？', '在', '刘', '鑫', '妈', '妈', '与', '江', '歌', '妈', '妈', '的', '争', '执', '中', '可', '见', '一', '斑', '，', '刘', '鑫', '妈', '妈', '说', '：', '[UNK]', '她', '命', '短', '了', '，', '她', '不', '是', '为', '了', '俺', '闺', '女', '！', '[UNK]', '江', '歌', '妈', '妈', '后', '悔', '吗', '？', '一', '定', '后', '悔', '，', '后', '悔', '自', '己', '教', '女', '儿', '这', '样', '善', '良', '，', '让', '她', '付', '出', '了', '生', '命', '的', '代', '价', '去', '为', '别', '人', '的', '过', '错', '买', '单', '。', '曾', '经', '还', '有', '一', '个', '事', '件', '，', '一', '位', '爸', '爸', '带', '着', '自', '己', '的', '孩', '子', '去', '河', '边', '钓', '鱼', '，', '遇', '到', '一', '位', '带', '着', '孩', '子', '玩', '的', '妈', '妈', '。', '虽', '然', '并', '不', '相', '识', '，', '但', '是', '两', '个', '孩', '子', '很', '快', '打', '成', '了', '一', '片', '，', '玩', '得', '不', '亦', '乐', '乎', '。', '谁', '知', '突', '然', '两', '个', '孩', '子', '掉', '进', '了', '河', '里', '。', '这', '位', '爸', '爸', '二', '话', '没', '说', '，', '立', '刻', '下', '河', '去', '救', '孩', '子', '。', '这', '个', '时', '候', '他', '的', '孩', '子', '离', '岸', '边', '近', '，', '另', '一', '个', '孩', '子', '已', '经', '飘', '得', '比', '较', '远', '了', '。', '孩', '子', '妈', '妈', '因', '为', '不', '会', '游', '泳', '，', '一', '直', '在', '岸', '边', '喊', '话', '，', '救', '救', '她', '的', '孩', '子', '。', '这', '位', '爸', '爸', '稍', '一', '犹', '豫', '，', '还', '是', '奋', '力', '向', '着', '自', '己', '的', '孩', '子', '游', '过', '去', '，', '将', '自', '己', '的', '孩', '子', '拖', '到', '岸', '边', '，', '这', '才', '向', '着', '另', '外', '一', '个', '孩', '子', '游', '去', '。', '可', '是', '遗', '憾', '的', '是', '，', '另', '外', '一', '个', '孩', '子', '被', '救', '上', '岸', '，', '却', '已', '停', '止', '了', '呼', '吸', '。', '这', '位', '妈', '妈', '悲', '痛', '欲', '绝', '，', '不', '停', '地', '埋', '怨', '这', '位', '父', '亲', '：', '[UNK]', '我', '的', '孩', '子', '就', '在', '距', '离', '岸', '边', '不', '远', '处', '，', '你', '一', '伸', '手', '就', '可', '以', '抓', '得', '到', '，', '为', '什', '么', '你', '要', '游', '得', '那', '么', '远', '先', '救', '自', '己', '的', '孩', '子', '？', '你', '完', '全', '可', '以', '救', '起', '我', '的', '孩', '子', '再', '救', '你', '的', '孩', '子', '，', '你', '为', '什', '么', '这', '么', '没', '人', '性', '，', '为', '什', '么', '为', '什', '么', '[UNK]', '[UNK]', '[UNK]', '这', '是', '人', '的', '本', '能', '，', '看', '到', '自', '己', '的', '孩', '子', '落', '水', '，', '作', '为', '父', '母', '肯', '定', '会', '不', '顾', '一', '切', '地', '去', '救', '他', '，', '更', '何', '况', '是', '他', '自', '己', '的', '孩', '子', '离', '得', '近', '，', '他', '先', '救', '自', '己', '的', '孩', '子', '于', '情', '于', '理', '都', '说', '得', '过', '去', '。', '不', '说', '别', '的', '，', '世', '界', '上', '的', '人', '都', '是', '自', '私', '的', '，', '对', '于', '自', '己', '的', '孩', '子', '，', '更', '是', '如', '此', '，', '如', '果', '这', '位', '父', '亲', '不', '去', '救', '自', '己', '儿', '子', '，', '而', '是', '救', '了', '互', '不', '相', '识', '的', '小', '孩', '，', '万', '一', '孩', '子', '也', '遭', '遇', '了', '不', '幸', '，', '那', '么', '，', '他', '的', '家', '庭', '呢', '？', '他', '后', '半', '辈', '子', '呢', '？', '谁', '来', '为', '他', '破', '碎', '的', '家', '庭', '负', '责', '？', '孩', '子', '，', '舍', '己', '为', '人', '是', '很', '伟', '大', '，', '可', '是', '你', '的', '生', '命', '同', '样', '宝', '贵', '，', '同', '样', '值', '得', '珍', '惜', '。', '我', '们', '不', '能', '容', '许', '你', '为', '了', '别', '的', '任', '何', '人', '，', '去', '做', '以', '身', '犯', '险', '的', '事', '情', '，', '付', '出', '生', '命', '的', '代', '价', '去', '承', '担', '别', '人', '的', '过', '错', '或', '失', '误', '。', '告', '诉', '孩', '子', '，', '善', '良', '的', '前', '提', '是', '懂', '得', '保', '护', '自', '己', '！', '在', '这', '四', '种', '情', '况', '下', '，', '收', '起', '你', '的', '善', '良', '1', '当', '你', '弱', '势', '而', '强', '势', '时', '有', '一', '天', '，', '有', '位', '妈', '妈', '去', '学', '校', '接', '孩', '子', '迟', '到', '了', '，', '一', '个', '人', '贩', '子', '抓', '住', '机', '会', '，', '趁', '机', '哄', '骗', '她', '的', '孩', '子', '。', '骗', '子', '说', '：', '[UNK]', '哎', '呀', '，', '小', '朋', '友', '，', '我', '东', '西', '丢', '在', '厕', '所', '了', '，', '现', '在', '你', '能', '不', '能', '帮', '我', '去', '找', '一', '下', '。', '[UNK]', '她', '是', '想', '把', '孩', '子', '引', '到', '厕', '所', '里', '，', '好', '让', '同', '伴', '下', '手', '。', '结', '果', '她', '连', '续', '问', '了', '三', '遍', '，', '这', '个', '小', '孩', '子', '一', '直', '摇', '头', '，', '她', '恼', '了', '，', '你', '这', '孩', '子', '，', '没', '学', '过', '什', '么', '叫', '助', '人', '为', '乐', '吗', '？', '小', '孩', '瞪', '了', '他', '一', '眼', '，', '跑', '到', '老', '师', '身', '边', '去', '了', '，', '人', '贩', '子', '看', '这', '个', '孩', '子', '机', '警', '，', '马', '上', '转', '换', '目', '标', '。', '没', '到', '十', '分', '钟', '，', '他', '们', '骗', '到', '另', '一', '个', '小', '孩', '儿', '，', '正', '打', '算', '开', '车', '走', '人', '的', '时', '候', '，', '被', '警', '察', '抓', '了', '个', '正', '着', '。', '原', '来', '这', '个', '小', '孩', '子', '跑', '到', '老', '师', '身', '边', '，', '很', '笃', '定', '地', '跟', '老', '师', '说', '，', '这', '几', '个', '人', '是', '坏', '人', '。', '老', '师', '半', '信', '半', '疑', '，', '但', '是', '为', '了', '安', '全', '着', '想', '，', '还', '是', '报', '了', '警', '。', '结', '果', '没', '想', '到', '，', '他', '们', '真', '的', '是', '坏', '人', '。', '老', '师', '就', '问', '这', '个', '小', '孩', '，', '你', '怎', '么', '知', '道', '他', '们', '是', '坏', '人', '的', '。', '小', '孩', '子', '说', '：', '[UNK]', '妈', '妈', '说', '过', '，', '如', '果', '有', '大', '人', '找', '你', '帮', '忙', '，', '千', '万', '不', '要', '理', '他', '，', '因', '为', '如', '果', '大', '人', '遇', '到', '解', '决', '不', '了', '的', '困', '难', '，', '一', '定', '会', '寻', '求', '成', '人', '帮', '忙', '，', '而', '不', '是', '比', '他', '还', '弱', '小', '的', '孩', '子', '。', '[UNK]', '成', '年', '男', '子', '一', '般', '不', '会', '找', '孕', '妇', '帮', '忙', '，', '健', '壮', '的', '成', '年', '人', '一', '般', '不', '会', '让', '老', '人', '帮', '忙', '，', '大', '人', '一', '般', '不', '会', '找', '孩', '子', '帮', '忙', '，', '寻', '求', '帮', '助', '一', '定', '是', '因', '为', '你', '比', '他', '强', '，', '所', '以', '他', '才', '找', '你', '帮', '忙', '。', '如', '果', '你', '本', '就', '是', '弱', '势', '，', '而', '强', '势', '的', '一', '方', '反', '而', '要', '你', '帮', '忙', '，', '那', '么', '说', '明', '他', '一', '定', '另', '有', '所', '图', '！', '这', '时', '候', '，', '请', '务', '必', '收', '起', '你', '的', '善', '良', '！', '2', '避', '免', '进', '入', '封', '闭', '环', '境', '四', '年', '前', '，', '佳', '木', '斯', '市', '桦', '南', '县', '，', '曾', '发', '生', '一', '起', '惨', '案', '。', '一', '名', '刚', '刚', '17', '岁', '的', '年', '轻', '女', '护', '士', '，', '在', '路', '上', '看', '到', '一', '名', '孕', '妇', '跌', '倒', '，', '急', '忙', '上', '前', '搀', '扶', '，', '并', '将', '孕', '妇', '送', '回', '家', '中', '。', '当', '天', '下', '午', '3', '点', '15', '分', '，', '女', '护', '士', '给', '朋', '友', '发', '来', '微', '信', '：', '送', '一', '名', '孕', '妇', '阿', '姨', '，', '到', '她', '家', '了', '[UNK]', '[UNK]', '让', '所', '有', '人', '都', '没', '想', '到', '的', '是', '，', '这', '条', '充', '满', '爱', '心', '的', '留', '言', '，', '成', '为', '善', '良', '女', '孩', '留', '在', '人', '世', '间', '最', '后', '的', '信', '息', '。', '善', '良', '的', '女', '孩', '失', '踪', '，', '警', '方', '四', '处', '侦', '查', '，', '不', '久', '捕', '获', '邪', '恶', '的', '孕', '妇', '谭', '某', '[UNK]', '[UNK]', '网', '络', '上', '的', '新', '闻', '称', '，', '此', '女', '为', '讨', '好', '丈', '夫', '，', '假', '装', '在', '街', '头', '跌', '倒', '，', '引', '诱', '善', '良', '女', '孩', '，', '拐', '到', '家', '里', '让', '丈', '夫', '伤', '害', '。', '毫', '无', '防', '范', '的', '女', '孩', '，', '送', '其', '至', '出', '租', '屋', '后', '，', '被', '孕', '妇', '谭', '某', '以', '一', '瓶', '掺', '了', '安', '眠', '药', '的', '酸', '奶', '迷', '昏', '，', '而', '后', '将', '善', '良', '的', '姑', '娘', '杀', '害', '。', '有', '天', '下', '雨', '，', '有', '个', '孕', '妇', '见', '门', '口', '有', '个', '乞', '丐', '在', '淋', '雨', '，', '于', '心', '不', '忍', '，', '于', '是', '请', '他', '进', '来', '歇', '歇', '脚', '。', '乞', '丐', '感', '恩', '戴', '德', '地', '进', '了', '她', '的', '家', '，', '在', '换', '了', '干', '净', '的', '衣', '服', '，', '吃', '饱', '饭', '之', '后', '，', '他', '发', '现', '只', '有', '孕', '妇', '一', '个', '人', '在', '家', '，', '于', '是', '威', '胁', '孕', '妇', '给', '他', '一', '笔', '生', '活', '费', '。', '在', '这', '样', '封', '闭', '的', '环', '境', '下', '，', '只', '有', '他', '们', '两', '人', '在', '，', '她', '失', '去', '了', '讨', '价', '还', '价', '的', '余', '地', '，', '为', '了', '避', '免', '遭', '受', '更', '大', '的', '伤', '害', '，', '破', '财', '消', '灾', '，', '付', '了', '一', '万', '块', '钱', '给', '乞', '丐', '。', '当', '一', '个', '场', '合', '，', '从', '开', '放', '变', '成', '封', '闭', '，', '缺', '少', '了', '大', '家', '的', '监', '督', '，', '很', '难', '想', '象', '，', '人', '到', '底', '会', '做', '出', '什', '么', '事', '情', '。', '在', '人', '前', '可', '能', '是', '谦', '谦', '君', '子', '，', '但', '是', '到', '了', '人', '后', '，', '明', '显', '你', '弱', '人', '强', '的', '封', '闭', '环', '境', '中', '，', '你', '只', '能', '予', '取', '予', '求', '。', '所', '以', '，', '助', '人', '为', '乐', '可', '以', '，', '但', '是', '尽', '量', '避', '免', '在', '封', '闭', '的', '空', '间', '里', '。', '这', '时', '候', '，', '请', '务', '必', '收', '起', '你', '的', '善', '良', '！', '3', '有', '时', '候', '善', '良', '也', '会', '坏', '事', '有', '个', '广', '为', '人', '知', '的', '故', '事', '。', '游', '客', '在', '禁', '猎', '区', '保', '护', '组', '织', '的', '护', '送', '下', '，', '来', '到', '了', '神', '秘', '的', '可', '可', '西', '里', '。', '一', '只', '可', '爱', '的', '小', '藏', '羚', '羊', '跑', '了', '来', '，', '好', '奇', '的', '探', '头', '探', '脑', '。', '可', '爱', '的', '小', '生', '灵', '，', '游', '客', '们', '顿', '时', '骚', '动', '起', '来', '，', '纷', '纷', '上', '前', '拍', '照', '，', '拿', '出', '食', '物', '和', '饮', '水', '，', '要', '喂', '这', '只', '可', '爱', '的', '小', '动', '物', '。', '突', '然', '之', '间', '，', '一', '声', '怒', '吼', '响', '起', '：', '[UNK]', '滚', '开', '！', '[UNK]', '禁', '猎', '区', '的', '保', '护', '队', '长', '冲', '过', '来', '，', '打', '跑', '可', '爱', '的', '小', '藏', '羚', '羊', '，', '不', '许', '游', '客', '们', '喂', '食', '。', '游', '客', '怒', '了', '：', '[UNK]', '你', '这', '是', '干', '什', '么', '？', '怎', '么', '可', '以', '如', '此', '粗', '暴', '的', '对', '待', '小', '动', '物', '。', '我', '们', '爱', '护', '这', '些', '小', '动', '物', '，', '又', '有', '什', '么', '不', '对', '？', '[UNK]', '[UNK]', '你', '们', '这', '是', '在', '造', '孽', '！', '[UNK]', '保', '护', '队', '长', '喝', '斥', '道', '：', '[UNK]', '如', '果', '你', '们', '对', '待', '野', '生', '动', '物', '太', '友', '善', '，', '它', '们', '就', '会', '以', '为', '人', '类', '都', '是', '善', '良', '的', '，', '一', '旦', '遭', '遇', '盗', '猎', '者', '，', '就', '有', '可', '能', '惨', '遭', '猎', '杀', '。', '[UNK]', '游', '客', '们', '惊', '呆', '了', '。', '[UNK]', '[UNK]', '你', '的', '善', '良', '，', '必', '须', '要', '经', '得', '起', '人', '心', '的', '复', '杂', '！', '看', '起', '来', '是', '善', '良', '的', '事', '情', '，', '却', '可', '能', '因', '为', '人', '的', '复', '杂', '贪', '婪', '而', '变', '成', '坏', '事', '，', '行', '善', '务', '必', '要', '考', '察', '清', '楚', '，', '你', '的', '善', '良', '到', '底', '会', '造', '成', '什', '么', '样', '的', '后', '果', '。', '这', '时', '候', '，', '请', '务', '必', '收', '起', '你', '的', '善', '良', '！', '4', '升', '米', '恩', '，', '斗', '米', '仇', '从', '前', '，', '有', '两', '户', '人', '家', '是', '邻', '居', '，', '平', '时', '关', '系', '还', '不', '错', '。', '其', '中', '一', '家', '人', '因', '为', '能', '干', '些', '，', '家', '中', '要', '富', '裕', '的', '多', '。', '这', '两', '家', '本', '来', '没', '有', '什', '么', '恩', '怨', '的', '，', '可', '是', '，', '这', '一', '年', '，', '老', '天', '爷', '发', '怒', '，', '降', '下', '了', '灾', '祸', '，', '田', '中', '颗', '粒', '无', '收', '。', '这', '穷', '的', '一', '家', '没', '有', '了', '收', '成', '，', '只', '好', '躺', '着', '等', '死', '。', '这', '个', '时', '候', '，', '富', '的', '一', '家', '买', '到', '了', '很', '多', '粮', '食', '，', '想', '着', '大', '家', '邻', '居', '的', '，', '就', '给', '穷', '的', '一', '家', '送', '去', '了', '一', '升', '米', '，', '救', '了', '急', '。', '这', '穷', '的', '一', '家', '非', '常', '感', '激', '富', '人', '，', '认', '为', '这', '真', '是', '救', '命', '的', '恩', '人', '呀', '！', '熬', '过', '最', '艰', '苦', '的', '时', '刻', '后', '，', '穷', '人', '就', '前', '往', '感', '谢', '富', '人', '。', '说', '话', '间', '，', '谈', '起', '明', '年', '的', '种', '子', '还', '没', '有', '着', '落', '，', '富', '的', '一', '家', '慷', '慨', '地', '说', '：', '这', '样', '吧', '，', '我', '这', '里', '的', '粮', '食', '还', '有', '很', '多', '，', '你', '就', '再', '拿', '去', '一', '斗', '吧', '。', '这', '穷', '的', '千', '恩', '万', '谢', '地', '拿', '着', '一', '斗', '米', '回', '家', '了', '。', '回', '家', '后', '，', '他', '的', '兄', '弟', '说', '了', '，', '这', '斗', '米', '能', '做', '什', '么', '？', '除', '了', '吃', '以', '外', '，', '根', '本', '就', '不', '够', '我', '们', '明', '年', '地', '里', '的', '种', '子', '，', '这', '个', '富', '人', '太', '过', '分', '了', '，', '既', '然', '你', '这', '么', '有', '钱', '，', '就', '应', '该', '多', '送', '我', '们', '一', '些', '粮', '食', '和', '钱', '，', '才', '给', '这', '么', '一', '点', '，', '真', '是', '坏', '的', '很', '。', '这', '话', '传', '到', '了', '富', '人', '耳', '朵', '里', '，', '他', '很', '生', '气', '，', '心', '想', '，', '我', '白', '白', '送', '你', '这', '么', '多', '的', '粮', '食', '，', '你', '不', '仅', '不', '感', '谢', '我', '，', '还', '把', '我', '当', '仇', '人', '一', '样', '忌', '恨', '，', '真', '不', '是', '人', '。', '于', '是', '，', '本', '来', '关', '系', '不', '错', '的', '两', '家', '人', '。', '从', '此', '就', '成', '了', '仇', '人', '，', '老', '死', '不', '相', '往', '来', '。', '一', '个', '人', '饥', '寒', '交', '迫', '的', '时', '候', '，', '你', '给', '他', '一', '碗', '米', '，', '就', '是', '解', '决', '了', '他', '的', '大', '问', '题', '，', '他', '会', '感', '恩', '不', '尽', '。', '但', '是', '，', '你', '如', '果', '继', '续', '给', '他', '米', '，', '他', '就', '会', '觉', '得', '理', '所', '当', '然', '了', '。', '一', '碗', '米', '不', '够', '，', '两', '碗', '米', '不', '够', '，', '三', '碗', '四', '碗', '还', '是', '觉', '得', '你', '只', '给', '了', '沧', '海', '一', '粟', '。', '生', '活', '里', '常', '有', '这', '样', '的', '事', '，', '第', '一', '次', '为', '一', '个', '人', '提', '供', '帮', '助', '时', '，', '他', '会', '对', '你', '心', '存', '感', '激', '，', '第', '二', '次', '他', '的', '感', '恩', '心', '理', '就', '会', '淡', '化', '，', '到', '了', '次', '以', '后', '，', '他', '简', '直', '就', '理', '直', '气', '壮', '地', '认', '为', '这', '都', '是', '你', '应', '该', '为', '他', '做', '的', '，', '甚', '至', '当', '没', '有', '了', '这', '种', '帮', '助', '时', '，', '他', '会', '对', '施', '助', '者', '心', '存', '怨', '恨', '。', '所', '以', '，', '人', '的', '善', '良', '一', '定', '要', '有', '度', '！', '当', '一', '个', '人', '不', '思', '进', '取', '，', '一', '味', '索', '取', '帮', '助', '时', '，', '请', '及', '时', '收', '起', '你', '的', '善', '良', '！', '这', '时', '候', '，', '请', '务', '必', '收', '起', '你', '的', '善', '良', '！', '我', '们', '不', '愿', '意', '世', '间', '满', '是', '冷', '漠', '之', '人', '，', '我', '们', '也', '不', '愿', '意', '教', '育', '孩', '子', '成', '为', '利', '己', '主', '义', '者', '，', '但', '是', '世', '间', '险', '恶', '，', '有', '些', '时', '候', '选', '择', '自', '保', '无', '可', '厚', '非', '。', '对', '于', '孩', '子', '来', '说', '，', '他', '们', '还', '不', '能', '很', '好', '地', '判', '断', '事', '情', '的', '真', '假', '，', '作', '为', '父', '母', '，', '我', '们', '要', '教', '会', '孩', '子', '：', '善', '良', '没', '有', '错', '，', '但', '是', '要', '有', '底', '线', '，', '人', '性', '复', '杂', '，', '教', '会', '孩', '子', '懂', '得', '先', '保', '护', '自', '己', '，', '才', '能', '考', '虑', '善', '良', '以', '及', '其', '他', '。', '成', '功', '的', '父', '母', '每', '时', '每', '刻', '都', '会', '注', '重', '优', '质', '信', '息', '的', '分', '享', '，', '把', '好', '的', '亲', '子', '教', '育', '文', '章', '分', '享', '给', '身', '边', '的', '朋', '友', '，', '也', '许', '就', '是', '这', '样', '一', '个', '简', '单', '的', '动', '作', '，', '数', '亿', '的', '孩', '子', '会', '更', '加', '健', '康', '、', '快', '乐', '的', '成', '长', '！', '下', '面', '的', '内', '容', '你', '可', '能', '会', '喜', '欢', '↓↓↓', '（', '公', '众', '号', '回', '复', '[UNK]', '爱', '娃', '[UNK]', '即', '可', '查', '看', '文', '章', '）', '育', '儿', '福', '利', '（', '公', '益', '父', '母', '课', '堂', '！', '育', '儿', '问', '题', '解', '答', '！', '儿', '童', '能', '力', '测', '评', '）', '育', '儿', '干', '货', '精', '选', '（', '如', '：', '如', '何', '夸', '孩', '子', '？', '如', '何', '培', '养', '阅', '读', '？', '如', '何', '过', '渡', '幼', '儿', '园', '？', '）', '父', '母', '成', '长', '记', '（', '如', '：', '犹', '太', '人', '的', '家', '教', '！', '经', '验', '说', '科', '学', '说', '！', '家', '庭', '教', '育', '如', '何', '培', '养', '出', '成', '功', '的', '孩', '子', '？', '）', '万', '千', '妈', '妈', '推', '荐', '（', '如', '何', '进', '行', '优', '质', '早', '教', '？', '如', '何', '选', '择', '合', '适', '孩', '子', '的', '早', '教', '课', '程', '？', '早', '教', '精', '品', '课', '程', '推', '荐', '！', '）', '最', '牛', '早', '教', '↓', '卓', '越', '七', '田', '中', '国', '首', '家', '效', '果', '可', '衡', '量', '的', '早', '教', '机', '构', '，', '深', '圳', '东', '莞', '十', '多', '家', '分', '校', '，', '为', '近', '百', '家', '早', '教', '机', '构', '提', '供', '师', '资', '培', '训', '！', '专', '注', '09', '岁', '孩', '子', '学', '习', '潜', '能', '、', '性', '格', '潜', '能', '的', '科', '学', '育', '儿', '方', '案', '。', '您', '身', '边', '的', '早', '教', '专', '家', '邀', '您', '一', '起', '科', '学', '育', '儿', '育', '儿', '亲', '子', '教', '育', '↓', '深', '圳', '早', '教', '专', '家', '觉', '得', '文', '章', '还', '不', '错', '，', '分', '享', '给', '需', '要', '的', '朋', '友', '吧', '！', '点', '击', '阅', '读', '原', '文', '↓↓↓', '，', '免', '费', '获', '取', '价', '值', '480', '元', '儿', '童', '生', '理', '发', '育', '测', '评']\n",
      "11/14/2019 05:59:11 - INFO - __main__ -   *** Example ***\n",
      "11/14/2019 05:59:11 - INFO - __main__ -   idx: 0\n",
      "11/14/2019 05:59:11 - INFO - __main__ -   guid: 7640a5589bc7486ca199eeeb38af79dd\n",
      "11/14/2019 05:59:11 - INFO - __main__ -   tokens: [CLS] 江 歌 事 件 ： 教 会 孩 子 ， 善 良 的 同 时 更 要 懂 得 保 护 自 己 ！ [SEP] 教 育 孩 子 成 为 利 己 主 义 者 ， 但 是 世 间 险 恶 ， 有 些 时 候 选 择 自 保 无 可 厚 非 。 对 于 孩 子 来 说 ， 他 们 还 不 能 很 好 地 判 断 事 情 的 真 假 ， 作 为 父 母 ， 我 们 要 教 会 孩 子 ： 善 良 没 有 错 ， 但 是 要 有 底 线 ， 人 性 复 杂 ， 教 会 孩 子 懂 得 先 保 护 自 己 ， 才 能 考 虑 善 良 以 及 其 他 。 成 功 的 父 母 每 时 每 刻 都 会 注 重 优 质 信 息 的 分 享 ， 把 好 的 亲 子 教 育 文 章 分 享 给 身 边 的 朋 友 ， 也 许 就 是 这 样 一 个 简 单 的 动 作 ， 数 亿 的 孩 子 会 更 加 健 康 、 快 乐 的 成 长 ！ 下 面 的 内 容 你 可 能 会 喜 欢 ↓↓↓ （ 公 众 号 回 复 [UNK] 爱 娃 [UNK] 即 可 查 看 文 章 ） 育 儿 福 利 （ 公 益 父 母 课 堂 ！ 育 儿 问 题 解 答 ！ 儿 童 能 力 测 评 ） 育 儿 干 货 精 选 （ 如 ： 如 何 夸 孩 子 ？ 如 何 培 养 阅 读 ？ 如 何 过 渡 幼 儿 园 ？ ） 父 母 成 长 记 （ 如 ： 犹 太 人 的 家 教 ！ 经 验 说 科 学 说 ！ 家 庭 教 育 如 何 培 养 出 成 功 的 孩 子 ？ ） 万 千 妈 妈 推 荐 （ 如 何 进 行 优 质 早 教 ？ 如 何 选 择 合 适 孩 子 的 早 教 课 程 ？ 早 教 精 品 课 程 推 荐 ！ ） 最 牛 早 教 ↓ 卓 越 七 田 中 国 首 家 效 果 可 衡 量 的 早 教 机 构 ， 深 圳 东 莞 十 多 家 分 校 ， 为 近 百 家 早 教 机 构 提 供 师 资 培 训 ！ 专 注 09 岁 孩 子 学 习 潜 能 、 性 格 潜 能 的 科 学 育 儿 方 案 。 您 身 边 的 早 教 专 家 邀 您 一 起 科 学 育 儿 育 儿 亲 子 教 育 ↓ 深 圳 早 教 专 家 觉 得 文 章 还 不 错 ， 分 享 给 需 要 的 朋 友 吧 ！ 点 击 阅 读 原 文 ↓↓↓ ， 免 费 获 取 价 值 480 元 儿 童 生 理 发 育 测 [SEP]\n",
      "11/14/2019 05:59:11 - INFO - __main__ -   input_ids: 101 3736 3625 752 816 8038 3136 833 2111 2094 8024 1587 5679 4638 1398 3198 3291 6206 2743 2533 924 2844 5632 2346 8013 102 3136 5509 2111 2094 2768 711 1164 2346 712 721 5442 8024 852 3221 686 7313 7372 2626 8024 3300 763 3198 952 6848 2885 5632 924 3187 1377 1331 7478 511 2190 754 2111 2094 3341 6432 8024 800 812 6820 679 5543 2523 1962 1765 1161 3171 752 2658 4638 4696 969 8024 868 711 4266 3678 8024 2769 812 6206 3136 833 2111 2094 8038 1587 5679 3766 3300 7231 8024 852 3221 6206 3300 2419 5296 8024 782 2595 1908 3325 8024 3136 833 2111 2094 2743 2533 1044 924 2844 5632 2346 8024 2798 5543 5440 5991 1587 5679 809 1350 1071 800 511 2768 1216 4638 4266 3678 3680 3198 3680 1174 6963 833 3800 7028 831 6574 928 2622 4638 1146 775 8024 2828 1962 4638 779 2094 3136 5509 3152 4995 1146 775 5314 6716 6804 4638 3301 1351 8024 738 6387 2218 3221 6821 3416 671 702 5042 1296 4638 1220 868 8024 3144 783 4638 2111 2094 833 3291 1217 978 2434 510 2571 727 4638 2768 7270 8013 678 7481 4638 1079 2159 872 1377 5543 833 1599 3614 9010 8020 1062 830 1384 1726 1908 100 4263 2015 100 1315 1377 3389 4692 3152 4995 8021 5509 1036 4886 1164 8020 1062 4660 4266 3678 6440 1828 8013 5509 1036 7309 7579 6237 5031 8013 1036 4997 5543 1213 3844 6397 8021 5509 1036 2397 6573 5125 6848 8020 1963 8038 1963 862 1930 2111 2094 8043 1963 862 1824 1075 7325 6438 8043 1963 862 6814 3941 2405 1036 1736 8043 8021 4266 3678 2768 7270 6381 8020 1963 8038 4310 1922 782 4638 2157 3136 8013 5307 7741 6432 4906 2110 6432 8013 2157 2431 3136 5509 1963 862 1824 1075 1139 2768 1216 4638 2111 2094 8043 8021 674 1283 1968 1968 2972 5773 8020 1963 862 6822 6121 831 6574 3193 3136 8043 1963 862 6848 2885 1394 6844 2111 2094 4638 3193 3136 6440 4923 8043 3193 3136 5125 1501 6440 4923 2972 5773 8013 8021 3297 4281 3193 3136 371 1294 6632 673 4506 704 1744 7674 2157 3126 3362 1377 6130 7030 4638 3193 3136 3322 3354 8024 3918 1766 691 5806 1282 1914 2157 1146 3413 8024 711 6818 4636 2157 3193 3136 3322 3354 2990 897 2360 6598 1824 6378 8013 683 3800 8141 2259 2111 2094 2110 739 4052 5543 510 2595 3419 4052 5543 4638 4906 2110 5509 1036 3175 3428 511 2644 6716 6804 4638 3193 3136 683 2157 6913 2644 671 6629 4906 2110 5509 1036 5509 1036 779 2094 3136 5509 371 3918 1766 3193 3136 683 2157 6230 2533 3152 4995 6820 679 7231 8024 1146 775 5314 7444 6206 4638 3301 1351 1416 8013 4157 1140 7325 6438 1333 3152 9010 8024 1048 6589 5815 1357 817 966 9482 1039 1036 4997 4495 4415 1355 5509 3844 102\n",
      "11/14/2019 05:59:11 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/14/2019 05:59:11 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/14/2019 05:59:11 - INFO - __main__ -   label: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/14/2019 06:00:38 - INFO - __main__ -   ***** Running training *****\n",
      "11/14/2019 06:00:38 - INFO - __main__ -     Num examples = 11758\n",
      "11/14/2019 06:00:38 - INFO - __main__ -     Batch size = 4\n",
      "11/14/2019 06:00:38 - INFO - __main__ -     Num steps = 30000\n",
      "  0%|                                                 | 0/30000 [00:00<?, ?it/s]11/14/2019 06:00:59 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 06:00:59 - INFO - __main__ -     Num examples = 2938\n",
      "11/14/2019 06:00:59 - INFO - __main__ -     Batch size = 48\n",
      "/root/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "11/14/2019 06:03:04 - INFO - __main__ -     eval_F1 = 0.21425730394669398\n",
      "11/14/2019 06:03:04 - INFO - __main__ -     eval_loss = 1.3095991967185852\n",
      "11/14/2019 06:03:04 - INFO - __main__ -     global_step = 0\n",
      "================================================================================\n",
      "Best F1 0.21425730394669398\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.6888:   3%|▋                       | 799/30000 [04:37<1:27:05,  5.59it/s]11/14/2019 06:05:16 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 06:05:16 - INFO - __main__ -     global_step = 200\n",
      "11/14/2019 06:05:16 - INFO - __main__ -     train loss = 0.6888\n",
      "loss 0.4888:   5%|█▏                     | 1599/30000 [06:48<1:25:16,  5.55it/s]11/14/2019 06:07:27 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 06:07:27 - INFO - __main__ -     global_step = 400\n",
      "11/14/2019 06:07:27 - INFO - __main__ -     train loss = 0.4888\n",
      "loss 0.4521:   8%|█▊                     | 2399/30000 [08:59<1:22:55,  5.55it/s]11/14/2019 06:09:37 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 06:09:37 - INFO - __main__ -     global_step = 600\n",
      "11/14/2019 06:09:37 - INFO - __main__ -     train loss = 0.4521\n",
      "loss 0.446:  11%|██▌                     | 3199/30000 [11:10<1:20:22,  5.56it/s]11/14/2019 06:11:49 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 06:11:49 - INFO - __main__ -     global_step = 800\n",
      "11/14/2019 06:11:49 - INFO - __main__ -     train loss = 0.446\n",
      "loss 0.4297:  13%|███                    | 3999/30000 [13:22<1:17:56,  5.56it/s]11/14/2019 06:14:00 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 06:14:00 - INFO - __main__ -     global_step = 1000\n",
      "11/14/2019 06:14:00 - INFO - __main__ -     train loss = 0.4297\n",
      "loss 0.4057:  16%|███▋                   | 4799/30000 [15:33<1:15:30,  5.56it/s]11/14/2019 06:16:11 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 06:16:11 - INFO - __main__ -     global_step = 1200\n",
      "11/14/2019 06:16:11 - INFO - __main__ -     train loss = 0.4057\n",
      "loss 0.4234:  19%|████▎                  | 5599/30000 [17:44<1:13:01,  5.57it/s]11/14/2019 06:18:22 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 06:18:22 - INFO - __main__ -     global_step = 1400\n",
      "11/14/2019 06:18:22 - INFO - __main__ -     train loss = 0.4234\n",
      "loss 0.3481:  21%|████▉                  | 6399/30000 [19:55<1:09:29,  5.66it/s]11/14/2019 06:20:33 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 06:20:33 - INFO - __main__ -     global_step = 1600\n",
      "11/14/2019 06:20:33 - INFO - __main__ -     train loss = 0.3481\n",
      "loss 0.4493:  24%|█████▌                 | 7199/30000 [22:05<1:08:24,  5.56it/s]11/14/2019 06:22:43 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 06:22:43 - INFO - __main__ -     global_step = 1800\n",
      "11/14/2019 06:22:43 - INFO - __main__ -     train loss = 0.4493\n",
      "loss 0.4429:  27%|██████▏                | 7999/30000 [24:16<1:05:49,  5.57it/s]11/14/2019 06:24:54 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 06:24:54 - INFO - __main__ -     global_step = 2000\n",
      "11/14/2019 06:24:54 - INFO - __main__ -     train loss = 0.4429\n",
      "loss 0.3352:  29%|██████▋                | 8799/30000 [26:27<1:03:02,  5.61it/s]11/14/2019 06:27:05 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 06:27:05 - INFO - __main__ -     global_step = 2200\n",
      "11/14/2019 06:27:05 - INFO - __main__ -     train loss = 0.3352\n",
      "loss 0.4184:  32%|███████▎               | 9599/30000 [28:38<1:01:06,  5.56it/s]11/14/2019 06:29:16 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 06:29:16 - INFO - __main__ -     global_step = 2400\n",
      "11/14/2019 06:29:16 - INFO - __main__ -     train loss = 0.4184\n",
      "loss 0.4199:  35%|████████▎               | 10399/30000 [30:48<57:41,  5.66it/s]11/14/2019 06:31:26 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 06:31:26 - INFO - __main__ -     global_step = 2600\n",
      "11/14/2019 06:31:26 - INFO - __main__ -     train loss = 0.4199\n",
      "loss 0.341:  37%|█████████▎               | 11199/30000 [32:58<56:11,  5.58it/s]11/14/2019 06:33:36 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 06:33:36 - INFO - __main__ -     global_step = 2800\n",
      "11/14/2019 06:33:36 - INFO - __main__ -     train loss = 0.341\n",
      "11/14/2019 06:34:00 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 06:34:00 - INFO - __main__ -     Num examples = 2938\n",
      "11/14/2019 06:34:00 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 06:36:07 - INFO - __main__ -     eval_F1 = 0.7813555385359856\n",
      "11/14/2019 06:36:07 - INFO - __main__ -     eval_loss = 0.33879588922906306\n",
      "11/14/2019 06:36:07 - INFO - __main__ -     global_step = 2800\n",
      "11/14/2019 06:36:07 - INFO - __main__ -     loss = 0.341\n",
      "================================================================================\n",
      "Best F1 0.7813555385359856\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.4061:  40%|█████████▌              | 11999/30000 [37:44<54:50,  5.47it/s]11/14/2019 06:38:22 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 06:38:22 - INFO - __main__ -     global_step = 3000\n",
      "11/14/2019 06:38:22 - INFO - __main__ -     train loss = 0.4061\n",
      "11/14/2019 06:38:45 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 06:38:45 - INFO - __main__ -     Num examples = 2938\n",
      "11/14/2019 06:38:45 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 06:40:51 - INFO - __main__ -     eval_F1 = 0.8041249751740706\n",
      "11/14/2019 06:40:51 - INFO - __main__ -     eval_loss = 0.33720035093926615\n",
      "11/14/2019 06:40:51 - INFO - __main__ -     global_step = 3000\n",
      "11/14/2019 06:40:51 - INFO - __main__ -     loss = 0.4061\n",
      "================================================================================\n",
      "Best F1 0.8041249751740706\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.2735:  43%|██████████▏             | 12799/30000 [42:27<51:50,  5.53it/s]11/14/2019 06:43:05 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 06:43:05 - INFO - __main__ -     global_step = 3200\n",
      "11/14/2019 06:43:05 - INFO - __main__ -     train loss = 0.2735\n",
      "11/14/2019 06:43:28 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 06:43:28 - INFO - __main__ -     Num examples = 2938\n",
      "11/14/2019 06:43:28 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 06:45:34 - INFO - __main__ -     eval_F1 = 0.7970343914232583\n",
      "11/14/2019 06:45:34 - INFO - __main__ -     eval_loss = 0.3535304889683762\n",
      "11/14/2019 06:45:34 - INFO - __main__ -     global_step = 3200\n",
      "11/14/2019 06:45:34 - INFO - __main__ -     loss = 0.2735\n",
      "================================================================================\n",
      "loss 0.2505:  45%|██████████▉             | 13599/30000 [47:08<49:50,  5.48it/s]11/14/2019 06:47:46 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 06:47:46 - INFO - __main__ -     global_step = 3400\n",
      "11/14/2019 06:47:46 - INFO - __main__ -     train loss = 0.2505\n",
      "11/14/2019 06:48:10 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 06:48:10 - INFO - __main__ -     Num examples = 2938\n",
      "11/14/2019 06:48:10 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 06:50:17 - INFO - __main__ -     eval_F1 = 0.7459094612933344\n",
      "11/14/2019 06:50:17 - INFO - __main__ -     eval_loss = 0.35967567118425525\n",
      "11/14/2019 06:50:17 - INFO - __main__ -     global_step = 3400\n",
      "11/14/2019 06:50:17 - INFO - __main__ -     loss = 0.2505\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.2154:  48%|███████████▌            | 14399/30000 [51:50<46:40,  5.57it/s]11/14/2019 06:52:29 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 06:52:29 - INFO - __main__ -     global_step = 3600\n",
      "11/14/2019 06:52:29 - INFO - __main__ -     train loss = 0.2154\n",
      "11/14/2019 06:52:51 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 06:52:51 - INFO - __main__ -     Num examples = 2938\n",
      "11/14/2019 06:52:51 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 06:54:57 - INFO - __main__ -     eval_F1 = 0.7823307042404691\n",
      "11/14/2019 06:54:57 - INFO - __main__ -     eval_loss = 0.41411360257094904\n",
      "11/14/2019 06:54:57 - INFO - __main__ -     global_step = 3600\n",
      "11/14/2019 06:54:57 - INFO - __main__ -     loss = 0.2154\n",
      "================================================================================\n",
      "loss 0.2545:  51%|████████████▏           | 15199/30000 [56:30<44:15,  5.57it/s]11/14/2019 06:57:08 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 06:57:08 - INFO - __main__ -     global_step = 3800\n",
      "11/14/2019 06:57:08 - INFO - __main__ -     train loss = 0.2545\n",
      "11/14/2019 06:57:29 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 06:57:29 - INFO - __main__ -     Num examples = 2938\n",
      "11/14/2019 06:57:29 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 06:59:36 - INFO - __main__ -     eval_F1 = 0.7968111869593901\n",
      "11/14/2019 06:59:36 - INFO - __main__ -     eval_loss = 0.39038485571021037\n",
      "11/14/2019 06:59:36 - INFO - __main__ -     global_step = 3800\n",
      "11/14/2019 06:59:36 - INFO - __main__ -     loss = 0.2545\n",
      "================================================================================\n",
      "loss 0.2105:  53%|███████████▋          | 15999/30000 [1:01:09<41:49,  5.58it/s]11/14/2019 07:01:47 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 07:01:47 - INFO - __main__ -     global_step = 4000\n",
      "11/14/2019 07:01:47 - INFO - __main__ -     train loss = 0.2105\n",
      "11/14/2019 07:02:09 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 07:02:09 - INFO - __main__ -     Num examples = 2938\n",
      "11/14/2019 07:02:09 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 07:04:15 - INFO - __main__ -     eval_F1 = 0.803689408957417\n",
      "11/14/2019 07:04:15 - INFO - __main__ -     eval_loss = 0.3657319954326076\n",
      "11/14/2019 07:04:15 - INFO - __main__ -     global_step = 4000\n",
      "11/14/2019 07:04:15 - INFO - __main__ -     loss = 0.2105\n",
      "================================================================================\n",
      "loss 0.2378:  56%|████████████▎         | 16799/30000 [1:05:47<39:15,  5.60it/s]11/14/2019 07:06:26 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 07:06:26 - INFO - __main__ -     global_step = 4200\n",
      "11/14/2019 07:06:26 - INFO - __main__ -     train loss = 0.2378\n",
      "11/14/2019 07:06:49 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 07:06:49 - INFO - __main__ -     Num examples = 2938\n",
      "11/14/2019 07:06:49 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 07:08:55 - INFO - __main__ -     eval_F1 = 0.8043271122113157\n",
      "11/14/2019 07:08:55 - INFO - __main__ -     eval_loss = 0.3450401276650448\n",
      "11/14/2019 07:08:55 - INFO - __main__ -     global_step = 4200\n",
      "11/14/2019 07:08:55 - INFO - __main__ -     loss = 0.2378\n",
      "================================================================================\n",
      "Best F1 0.8043271122113157\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.2017:  59%|████████████▉         | 17599/30000 [1:10:32<36:58,  5.59it/s]11/14/2019 07:11:10 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 07:11:10 - INFO - __main__ -     global_step = 4400\n",
      "11/14/2019 07:11:10 - INFO - __main__ -     train loss = 0.2017\n",
      "11/14/2019 07:11:31 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 07:11:31 - INFO - __main__ -     Num examples = 2938\n",
      "11/14/2019 07:11:31 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 07:13:37 - INFO - __main__ -     eval_F1 = 0.8067398373205449\n",
      "11/14/2019 07:13:37 - INFO - __main__ -     eval_loss = 0.35187027612400634\n",
      "11/14/2019 07:13:37 - INFO - __main__ -     global_step = 4400\n",
      "11/14/2019 07:13:37 - INFO - __main__ -     loss = 0.2017\n",
      "================================================================================\n",
      "Best F1 0.8067398373205449\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.1811:  61%|█████████████▍        | 18399/30000 [1:15:13<34:44,  5.57it/s]11/14/2019 07:15:51 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 07:15:51 - INFO - __main__ -     global_step = 4600\n",
      "11/14/2019 07:15:51 - INFO - __main__ -     train loss = 0.1811\n",
      "11/14/2019 07:16:13 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 07:16:13 - INFO - __main__ -     Num examples = 2938\n",
      "11/14/2019 07:16:13 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 07:18:19 - INFO - __main__ -     eval_F1 = 0.79534582015464\n",
      "11/14/2019 07:18:19 - INFO - __main__ -     eval_loss = 0.41084549308664375\n",
      "11/14/2019 07:18:19 - INFO - __main__ -     global_step = 4600\n",
      "11/14/2019 07:18:19 - INFO - __main__ -     loss = 0.1811\n",
      "================================================================================\n",
      "loss 0.2396:  64%|██████████████        | 19199/30000 [1:19:53<32:13,  5.59it/s]11/14/2019 07:20:31 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 07:20:31 - INFO - __main__ -     global_step = 4800\n",
      "11/14/2019 07:20:31 - INFO - __main__ -     train loss = 0.2396\n",
      "11/14/2019 07:20:52 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 07:20:52 - INFO - __main__ -     Num examples = 2938\n",
      "11/14/2019 07:20:52 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 07:22:59 - INFO - __main__ -     eval_F1 = 0.8034279271143271\n",
      "11/14/2019 07:22:59 - INFO - __main__ -     eval_loss = 0.34975462959658715\n",
      "11/14/2019 07:22:59 - INFO - __main__ -     global_step = 4800\n",
      "11/14/2019 07:22:59 - INFO - __main__ -     loss = 0.2396\n",
      "================================================================================\n",
      "loss 0.2302:  67%|██████████████▋       | 19999/30000 [1:24:31<29:46,  5.60it/s]11/14/2019 07:25:09 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 07:25:09 - INFO - __main__ -     global_step = 5000\n",
      "11/14/2019 07:25:09 - INFO - __main__ -     train loss = 0.2302\n",
      "11/14/2019 07:25:30 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 07:25:30 - INFO - __main__ -     Num examples = 2938\n",
      "11/14/2019 07:25:30 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 07:27:36 - INFO - __main__ -     eval_F1 = 0.8040633571308478\n",
      "11/14/2019 07:27:36 - INFO - __main__ -     eval_loss = 0.3633372048817335\n",
      "11/14/2019 07:27:36 - INFO - __main__ -     global_step = 5000\n",
      "11/14/2019 07:27:36 - INFO - __main__ -     loss = 0.2302\n",
      "================================================================================\n",
      "loss 0.1847:  69%|███████████████▎      | 20799/30000 [1:29:08<27:19,  5.61it/s]11/14/2019 07:29:46 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 07:29:46 - INFO - __main__ -     global_step = 5200\n",
      "11/14/2019 07:29:46 - INFO - __main__ -     train loss = 0.1847\n",
      "11/14/2019 07:30:07 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 07:30:07 - INFO - __main__ -     Num examples = 2938\n",
      "11/14/2019 07:30:07 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 07:32:14 - INFO - __main__ -     eval_F1 = 0.782537533828693\n",
      "11/14/2019 07:32:14 - INFO - __main__ -     eval_loss = 0.4123027389981754\n",
      "11/14/2019 07:32:14 - INFO - __main__ -     global_step = 5200\n",
      "11/14/2019 07:32:14 - INFO - __main__ -     loss = 0.1847\n",
      "================================================================================\n",
      "loss 0.2235:  72%|███████████████▊      | 21599/30000 [1:33:47<25:09,  5.57it/s]11/14/2019 07:34:25 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 07:34:25 - INFO - __main__ -     global_step = 5400\n",
      "11/14/2019 07:34:25 - INFO - __main__ -     train loss = 0.2235\n",
      "11/14/2019 07:34:46 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 07:34:46 - INFO - __main__ -     Num examples = 2938\n",
      "11/14/2019 07:34:46 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 07:36:53 - INFO - __main__ -     eval_F1 = 0.8012806069513193\n",
      "11/14/2019 07:36:53 - INFO - __main__ -     eval_loss = 0.3697014678449881\n",
      "11/14/2019 07:36:53 - INFO - __main__ -     global_step = 5400\n",
      "11/14/2019 07:36:53 - INFO - __main__ -     loss = 0.2235\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.2234:  75%|████████████████▍     | 22399/30000 [1:38:26<23:02,  5.50it/s]11/14/2019 07:39:05 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 07:39:05 - INFO - __main__ -     global_step = 5600\n",
      "11/14/2019 07:39:05 - INFO - __main__ -     train loss = 0.2234\n",
      "11/14/2019 07:39:28 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 07:39:28 - INFO - __main__ -     Num examples = 2938\n",
      "11/14/2019 07:39:28 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 07:41:34 - INFO - __main__ -     eval_F1 = 0.8027052265078947\n",
      "11/14/2019 07:41:34 - INFO - __main__ -     eval_loss = 0.36424731018562473\n",
      "11/14/2019 07:41:34 - INFO - __main__ -     global_step = 5600\n",
      "11/14/2019 07:41:34 - INFO - __main__ -     loss = 0.2234\n",
      "================================================================================\n",
      "loss 0.2187:  77%|█████████████████     | 23199/30000 [1:43:08<20:31,  5.52it/s]11/14/2019 07:43:46 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 07:43:46 - INFO - __main__ -     global_step = 5800\n",
      "11/14/2019 07:43:46 - INFO - __main__ -     train loss = 0.2187\n",
      "11/14/2019 07:44:08 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 07:44:08 - INFO - __main__ -     Num examples = 2938\n",
      "11/14/2019 07:44:08 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 07:46:14 - INFO - __main__ -     eval_F1 = 0.8006620590302655\n",
      "11/14/2019 07:46:14 - INFO - __main__ -     eval_loss = 0.37107574675352345\n",
      "11/14/2019 07:46:14 - INFO - __main__ -     global_step = 5800\n",
      "11/14/2019 07:46:14 - INFO - __main__ -     loss = 0.2187\n",
      "================================================================================\n",
      "loss 0.1833:  80%|█████████████████▌    | 23999/30000 [1:47:46<17:45,  5.63it/s]11/14/2019 07:48:24 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 07:48:24 - INFO - __main__ -     global_step = 6000\n",
      "11/14/2019 07:48:24 - INFO - __main__ -     train loss = 0.1833\n",
      "11/14/2019 07:48:47 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 07:48:47 - INFO - __main__ -     Num examples = 2938\n",
      "11/14/2019 07:48:47 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 07:50:53 - INFO - __main__ -     eval_F1 = 0.8111131421260049\n",
      "11/14/2019 07:50:53 - INFO - __main__ -     eval_loss = 0.3531813925252326\n",
      "11/14/2019 07:50:53 - INFO - __main__ -     global_step = 6000\n",
      "11/14/2019 07:50:53 - INFO - __main__ -     loss = 0.1833\n",
      "================================================================================\n",
      "Best F1 0.8111131421260049\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.1551:  83%|██████████████████▏   | 24799/30000 [1:52:29<16:01,  5.41it/s]11/14/2019 07:53:08 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 07:53:08 - INFO - __main__ -     global_step = 6200\n",
      "11/14/2019 07:53:08 - INFO - __main__ -     train loss = 0.1551\n",
      "11/14/2019 07:53:29 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 07:53:29 - INFO - __main__ -     Num examples = 2938\n",
      "11/14/2019 07:53:29 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 07:55:36 - INFO - __main__ -     eval_F1 = 0.806232424385092\n",
      "11/14/2019 07:55:36 - INFO - __main__ -     eval_loss = 0.34956549764460615\n",
      "11/14/2019 07:55:36 - INFO - __main__ -     global_step = 6200\n",
      "11/14/2019 07:55:36 - INFO - __main__ -     loss = 0.1551\n",
      "================================================================================\n",
      "loss 0.1298:  85%|██████████████████▊   | 25599/30000 [1:57:07<13:01,  5.63it/s]11/14/2019 07:57:46 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 07:57:46 - INFO - __main__ -     global_step = 6400\n",
      "11/14/2019 07:57:46 - INFO - __main__ -     train loss = 0.1298\n",
      "11/14/2019 07:58:07 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 07:58:07 - INFO - __main__ -     Num examples = 2938\n",
      "11/14/2019 07:58:07 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 08:00:14 - INFO - __main__ -     eval_F1 = 0.807267801745545\n",
      "11/14/2019 08:00:14 - INFO - __main__ -     eval_loss = 0.37179943178630165\n",
      "11/14/2019 08:00:14 - INFO - __main__ -     global_step = 6400\n",
      "11/14/2019 08:00:14 - INFO - __main__ -     loss = 0.1298\n",
      "================================================================================\n",
      "loss 0.0988:  88%|███████████████████▎  | 26399/30000 [2:01:45<10:38,  5.64it/s]11/14/2019 08:02:23 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 08:02:23 - INFO - __main__ -     global_step = 6600\n",
      "11/14/2019 08:02:23 - INFO - __main__ -     train loss = 0.0988\n",
      "11/14/2019 08:02:44 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 08:02:44 - INFO - __main__ -     Num examples = 2938\n",
      "11/14/2019 08:02:44 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 08:04:51 - INFO - __main__ -     eval_F1 = 0.8094690218194233\n",
      "11/14/2019 08:04:51 - INFO - __main__ -     eval_loss = 0.38506053370093146\n",
      "11/14/2019 08:04:51 - INFO - __main__ -     global_step = 6600\n",
      "11/14/2019 08:04:51 - INFO - __main__ -     loss = 0.0988\n",
      "================================================================================\n",
      "loss 0.0913:  91%|███████████████████▉  | 27199/30000 [2:06:22<08:20,  5.60it/s]11/14/2019 08:07:00 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 08:07:00 - INFO - __main__ -     global_step = 6800\n",
      "11/14/2019 08:07:00 - INFO - __main__ -     train loss = 0.0913\n",
      "11/14/2019 08:07:22 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 08:07:22 - INFO - __main__ -     Num examples = 2938\n",
      "11/14/2019 08:07:22 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 08:09:28 - INFO - __main__ -     eval_F1 = 0.8087331050527822\n",
      "11/14/2019 08:09:28 - INFO - __main__ -     eval_loss = 0.39719581238234475\n",
      "11/14/2019 08:09:28 - INFO - __main__ -     global_step = 6800\n",
      "11/14/2019 08:09:28 - INFO - __main__ -     loss = 0.0913\n",
      "================================================================================\n",
      "loss 0.0999:  93%|████████████████████▌ | 27999/30000 [2:10:59<05:54,  5.64it/s]11/14/2019 08:11:38 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 08:11:38 - INFO - __main__ -     global_step = 7000\n",
      "11/14/2019 08:11:38 - INFO - __main__ -     train loss = 0.0999\n",
      "11/14/2019 08:11:59 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 08:11:59 - INFO - __main__ -     Num examples = 2938\n",
      "11/14/2019 08:11:59 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 08:14:05 - INFO - __main__ -     eval_F1 = 0.8130676085518607\n",
      "11/14/2019 08:14:05 - INFO - __main__ -     eval_loss = 0.41275931630403767\n",
      "11/14/2019 08:14:05 - INFO - __main__ -     global_step = 7000\n",
      "11/14/2019 08:14:05 - INFO - __main__ -     loss = 0.0999\n",
      "================================================================================\n",
      "Best F1 0.8130676085518607\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.1084:  96%|█████████████████████ | 28799/30000 [2:15:42<03:36,  5.56it/s]11/14/2019 08:16:20 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 08:16:20 - INFO - __main__ -     global_step = 7200\n",
      "11/14/2019 08:16:20 - INFO - __main__ -     train loss = 0.1084\n",
      "11/14/2019 08:16:42 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 08:16:42 - INFO - __main__ -     Num examples = 2938\n",
      "11/14/2019 08:16:42 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 08:18:48 - INFO - __main__ -     eval_F1 = 0.810362247649869\n",
      "11/14/2019 08:18:48 - INFO - __main__ -     eval_loss = 0.41085375109387023\n",
      "11/14/2019 08:18:48 - INFO - __main__ -     global_step = 7200\n",
      "11/14/2019 08:18:48 - INFO - __main__ -     loss = 0.1084\n",
      "================================================================================\n",
      "loss 0.0943:  99%|█████████████████████▋| 29599/30000 [2:20:20<01:11,  5.61it/s]11/14/2019 08:20:58 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 08:20:58 - INFO - __main__ -     global_step = 7400\n",
      "11/14/2019 08:20:58 - INFO - __main__ -     train loss = 0.0943\n",
      "11/14/2019 08:21:20 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 08:21:20 - INFO - __main__ -     Num examples = 2938\n",
      "11/14/2019 08:21:20 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 08:23:26 - INFO - __main__ -     eval_F1 = 0.8124185663772298\n",
      "11/14/2019 08:23:26 - INFO - __main__ -     eval_loss = 0.4127315283122082\n",
      "11/14/2019 08:23:26 - INFO - __main__ -     global_step = 7400\n",
      "11/14/2019 08:23:26 - INFO - __main__ -     loss = 0.0943\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.073: 100%|███████████████████████| 30000/30000 [2:23:53<00:00,  6.24it/s]\n",
      "11/14/2019 08:24:31 - INFO - pytorch_transformers.modeling_utils -   loading weights file ../model/roberta_wwm_large_512_1_last2embedding_cls_replacement/roberta_wwm_large_512_1_last2embedding_cls_replacement_3/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "Traceback (most recent call last):\n",
      "  File \"./run_bert_2562_last2embedding_cls.py\", line 841, in <module>\n",
      "    main()\n",
      "  File \"./run_bert_2562_last2embedding_cls.py\", line 757, in main\n",
      "    logits = model(input_ids=input_ids, token_type_ids=segment_ids, attention_mask=input_mask).detach().cpu().numpy()\n",
      "  File \"/root/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 547, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/root/code/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 1119, in forward\n",
      "    attention_mask=flat_attention_mask, head_mask=head_mask)\n",
      "  File \"/root/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 547, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/root/code/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 727, in forward\n",
      "    head_mask=head_mask)\n",
      "  File \"/root/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 547, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/root/code/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 440, in forward\n",
      "    layer_outputs = layer_module(hidden_states, attention_mask, head_mask[i])\n",
      "  File \"/root/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 547, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/root/code/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 420, in forward\n",
      "    intermediate_output = self.intermediate(attention_output)\n",
      "  File \"/root/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 547, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/root/code/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 392, in forward\n",
      "    hidden_states = self.intermediate_act_fn(hidden_states)\n",
      "  File \"/root/code/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 142, in gelu\n",
      "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 384.00 MiB (GPU 0; 10.73 GiB total capacity; 8.60 GiB already allocated; 339.62 MiB free; 906.30 MiB cached)\n"
     ]
    }
   ],
   "source": [
    "!python ./run_bert_2562_last2embedding_cls.py \\\n",
    "--model_type bert \\\n",
    "--model_name_or_path ../model/chinese_roberta_wwm_large_ext_pytorch  \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_test \\\n",
    "--data_dir ../data/data_StratifiedKFold_42/data_replacement_3 \\\n",
    "--output_dir ../model/roberta_wwm_large_512_1_last2embedding_cls_replacement/roberta_wwm_large_512_1_last2embedding_cls_replacement_3 \\\n",
    "--max_seq_length 512 \\\n",
    "--split_num 1 \\\n",
    "--lstm_hidden_size 512 \\\n",
    "--lstm_layers 1 \\\n",
    "--lstm_dropout 0.1 \\\n",
    "--eval_steps 200 \\\n",
    "--per_gpu_train_batch_size 4 \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--warmup_steps 0 \\\n",
    "--per_gpu_eval_batch_size 48 \\\n",
    "--learning_rate 1e-5 \\\n",
    "--adam_epsilon 1e-6 \\\n",
    "--weight_decay 0 \\\n",
    "--train_steps 30000 \\\n",
    "--freeze 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/14/2019 08:25:04 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "11/14/2019 08:25:04 - INFO - pytorch_transformers.tokenization_utils -   Model name '../model/chinese_roberta_wwm_large_ext_pytorch' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming '../model/chinese_roberta_wwm_large_ext_pytorch' is a path or url to a directory containing tokenizer files.\n",
      "11/14/2019 08:25:04 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../model/chinese_roberta_wwm_large_ext_pytorch/added_tokens.json. We won't load it.\n",
      "11/14/2019 08:25:04 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../model/chinese_roberta_wwm_large_ext_pytorch/special_tokens_map.json. We won't load it.\n",
      "11/14/2019 08:25:04 - INFO - pytorch_transformers.tokenization_utils -   loading file ../model/chinese_roberta_wwm_large_ext_pytorch/vocab.txt\n",
      "11/14/2019 08:25:04 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/14/2019 08:25:04 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/14/2019 08:25:04 - INFO - pytorch_transformers.modeling_utils -   loading configuration file ../model/chinese_roberta_wwm_large_ext_pytorch/config.json\n",
      "11/14/2019 08:25:04 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 3,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "11/14/2019 08:25:04 - INFO - pytorch_transformers.modeling_utils -   loading weights file ../model/chinese_roberta_wwm_large_ext_pytorch/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "11/14/2019 08:25:13 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification_last2embedding_cls not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "11/14/2019 08:25:13 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification_last2embedding_cls: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "11/14/2019 08:25:20 - INFO - __main__ -   ** RAW EXAMPLE **\n",
      "11/14/2019 08:25:20 - INFO - __main__ -   content: ['这', '几', '天', '看', '了', '有', '人', '举', '报', '施', '某', '某', '的', '贴', '子', '，', '经', '与', '举', '报', '人', '联', '系', '证', '实', '，', '是', '宣', '某', '当', '天', '中', '午', '请', '举', '报', '人', '和', '枪', '手', '喝', '酒', '后', '，', '晚', '上', '才', '发', '的', '贴', '子', '！', '本', '人', '不', '去', '讨', '论', '前', '二', '天', '的', '举', '报', '，', '相', '信', '总', '归', '会', '有', '说', '法', '的', '！', '今', '天', '一', '看', '施', '全', '军', '2017', '年', '1', '月', '2', '日', '实', '名', '举', '报', '上', '黄', '镇', '宣', '国', '才', '的', '贴', '子', '（', '仍', '被', '锁', '定', '禁', '止', '评', '论', '）', '已', '经', '正', '好', '一', '整', '年', '了', '750', '001', '##18', '##014', '##29', '##10', '##85', '##79', '##66', '##86', '##17', '##12', '##11', '##23', '750', '##750', '001', '##18', '##014', '##29', '##10', '##85', '##79', '##66', '##86', '##17', '##12', '##11', '##23', '750', '图', '片', '：', '429', '##10', '##85', '##15', '##14', '##98', '##14', '##71', '##47', '##89', '##52', '施', '全', '军', '实', '名', '举', '报', '50', '天', '后', '，', '上', '黄', '镇', '党', '委', '政', '府', '回', '复', '如', '下', '图', '：', '750', '001', '##18', '##014', '##29', '##10', '##85', '##91', '##17', '##21', '##99', '##0', '750', '##750', '001', '##18', '##014', '##29', '##10', '##85', '##91', '##17', '##21', '##99', '##0', '750', '图', '片', '：', '429', '##10', '##85', '##15', '##14', '##98', '##14', '##72', '##63', '##16', '##68', '750', '001', '##18', '##014', '##29', '##10', '##85', '##99', '##39', '##43', '##20', '##75', '750', '##750', '001', '##18', '##014', '##29', '##10', '##85', '##99', '##39', '##43', '##20', '##75', '750', '图', '片', '：', '429', '##10', '##85', '##15', '##14', '##98', '##14', '##72', '##35', '##30', '##75', '一', '年', '的', '贴', '子', '，', '再', '次', '被', '网', '友', '顶', '起', '来', '后', '，', '才', '发', '现', '施', '某', '几', '天', '前', '回', '复', '网', '友', '的', '处', '理', '结', '果', '竟', '如', '下', '图', '：', '750', '001', '##18', '##014', '##29', '##10', '##85', '##93', '##25', '##72', '##76', '##08', '##51', '##31', '750', '##750', '001', '##18', '##014', '##29', '##10', '##85', '##93', '##25', '##72', '##76', '##08', '##51', '##31', '750', '图', '片', '：', '429', '##10', '##85', '##15', '##14', '##98', '##14', '##73', '##54', '##71', '##72', '现', '责', '问', '张', '涛', '书', '记', '：', '1', '、', '宣', '国', '才', '被', '举', '报', '这', '么', '多', '问', '题', '，', '什', '么', '时', '候', '有', '答', '复', '。', '2', '、', '宣', '国', '才', '被', '举', '报', '后', '，', '为', '什', '么', '被', '立', '刻', '免', '了', '村', '书', '记', '职', '务', '？', '为', '什', '么', '又', '被', '安', '排', '到', '城', '管', '队', '[UNK]', '吃', '空', '响', '[UNK]', '，', '自', '己', '却', '天', '天', '在', '我', '们', '水', '泥', '厂', '上', '班', '赚', '黑', '钱', '？', '3', '、', '这', '几', '个', '月', '，', '水', '泥', '每', '吨', '近', '200', '元', '纯', '利', '润', '，', '还', '供', '不', '应', '求', '，', '宣', '国', '才', '还', '清', '上', '黄', '政', '府', '担', '保', '借', '给', '宣', '国', '才', '代', '付', '振', '东', '厂', '工', '资', '社', '保', '的', '钱', '了', '吗', '？', '4', '、', '据', '了', '解', '宣', '国', '才', '占', '他', '人', '企', '业', '经', '营', '，', '又', '欠', '税', '521', '##6', '万', '元', '、', '欠', '社', '保', '327', '##6', '万', '元', '、', '应', '该', '还', '欠', '了', '职', '工', '工', '资', '几', '十', '万', '，', '上', '黄', '政', '府', '打', '算', '替', '宣', '国', '才', '担', '保', '还', '是', '归', '还', '？', '5', '、', '我', '们', '厂', '合', '法', '会', '计', '和', '老', '板', '被', '判', '刑', '四', '到', '六', '年', '，', '现', '在', '服', '刑', '。', '厂', '子', '给', '宣', '国', '才', '强', '占', '，', '宣', '国', '才', '每', '天', '赚', '20', '多', '万', '净', '利', '润', '，', '却', '对', '外', '宣', '称', '天', '天', '亏', '本', '！', '等', '咱', '老', '板', '刑', '满', '回', '厂', '，', '宣', '国', '才', '给', '咱', '厂', '[UNK]', '天', '天', '亏', '[UNK]', '可', '能', '要', '[UNK]', '亏', '[UNK]', '的', '几', '千', '万', '元', '，', '甚', '至', '几', '个', '亿', '，', '张', '涛', '书', '记', '您', '承', '担', '还', '是', '上', '黄', '政', '府', '承', '担', '？', '当', '初', '可', '是', '您', '亲', '自', '把', '厂', '交', '给', '宣', '国', '才', '生', '产', '的', '！', '希', '望', '徐', '市', '长', '看', '到', '本', '贴', '后', '能', '像', '批', '示', '263', '、', '批', '示', '违', '建', '等', '民', '生', '问', '题', '一', '样', '，', '关', '注', '一', '下', '我', '们', '水', '泥', '厂', '的', '将', '来', '！', '也', '请', '徐', '市', '长', '抽', '日', '理', '万', '机', '之', '空', '亲', '自', '约', '谈', '一', '下', '当', '事', '人', '（', '特', '别', '是', '那', '位', '施', '站', '长', '）', '，', '千', '万', '不', '能', '听', '取', '一', '面', '之', '辞', '！']\n",
      "11/14/2019 08:25:20 - INFO - __main__ -   *** Example ***\n",
      "11/14/2019 08:25:20 - INFO - __main__ -   idx: 0\n",
      "11/14/2019 08:25:20 - INFO - __main__ -   guid: 7a3dd79f90ee419da87190cff60f7a86\n",
      "11/14/2019 08:25:20 - INFO - __main__ -   tokens: [CLS] 问 责 领 导 上 黄 镇 党 委 书 记 张 涛 ， 宣 国 才 真 能 一 手 遮 天 吗 ？ [SEP] 后 ， 才 发 现 施 某 几 天 前 回 复 网 友 的 处 理 结 果 竟 如 下 图 ： 750 001 ##18 ##014 ##29 ##10 ##85 ##93 ##25 ##72 ##76 ##08 ##51 ##31 750 ##750 001 ##18 ##014 ##29 ##10 ##85 ##93 ##25 ##72 ##76 ##08 ##51 ##31 750 图 片 ： 429 ##10 ##85 ##15 ##14 ##98 ##14 ##73 ##54 ##71 ##72 现 责 问 张 涛 书 记 ： 1 、 宣 国 才 被 举 报 这 么 多 问 题 ， 什 么 时 候 有 答 复 。 2 、 宣 国 才 被 举 报 后 ， 为 什 么 被 立 刻 免 了 村 书 记 职 务 ？ 为 什 么 又 被 安 排 到 城 管 队 [UNK] 吃 空 响 [UNK] ， 自 己 却 天 天 在 我 们 水 泥 厂 上 班 赚 黑 钱 ？ 3 、 这 几 个 月 ， 水 泥 每 吨 近 200 元 纯 利 润 ， 还 供 不 应 求 ， 宣 国 才 还 清 上 黄 政 府 担 保 借 给 宣 国 才 代 付 振 东 厂 工 资 社 保 的 钱 了 吗 ？ 4 、 据 了 解 宣 国 才 占 他 人 企 业 经 营 ， 又 欠 税 521 ##6 万 元 、 欠 社 保 327 ##6 万 元 、 应 该 还 欠 了 职 工 工 资 几 十 万 ， 上 黄 政 府 打 算 替 宣 国 才 担 保 还 是 归 还 ？ 5 、 我 们 厂 合 法 会 计 和 老 板 被 判 刑 四 到 六 年 ， 现 在 服 刑 。 厂 子 给 宣 国 才 强 占 ， 宣 国 才 每 天 赚 20 多 万 净 利 润 ， 却 对 外 宣 称 天 天 亏 本 ！ 等 咱 老 板 刑 满 回 厂 ， 宣 国 才 给 咱 厂 [UNK] 天 天 亏 [UNK] 可 能 要 [UNK] 亏 [UNK] 的 几 千 万 元 ， 甚 至 几 个 亿 ， 张 涛 书 记 您 承 担 还 是 上 黄 政 府 承 担 ？ 当 初 可 是 您 亲 自 把 厂 交 给 宣 国 才 生 产 的 ！ 希 望 徐 市 长 看 到 本 贴 后 能 像 批 示 263 、 批 示 违 建 等 民 生 问 题 一 样 ， 关 注 一 下 我 们 水 泥 厂 的 将 来 ！ 也 请 徐 市 长 抽 日 理 万 机 之 空 亲 自 约 谈 一 下 当 事 人 （ 特 别 是 那 位 施 站 长 ） ， 千 万 不 能 听 取 一 面 之 辞 [SEP]\n",
      "11/14/2019 08:25:20 - INFO - __main__ -   input_ids: 101 7309 6569 7566 2193 677 7942 7252 1054 1999 741 6381 2476 3875 8024 2146 1744 2798 4696 5543 671 2797 6902 1921 1408 8043 102 1400 8024 2798 1355 4385 3177 3378 1126 1921 1184 1726 1908 5381 1351 4638 1905 4415 5310 3362 4994 1963 678 1745 8038 9180 9263 8662 11365 8887 8311 9169 9676 8743 9492 9624 9153 9216 8805 9180 10969 9263 8662 11365 8887 8311 9169 9676 8743 9492 9624 9153 9216 8805 9180 1745 4275 8038 13249 8311 9169 8493 8717 9215 8717 9148 9488 9097 9492 4385 6569 7309 2476 3875 741 6381 8038 122 510 2146 1744 2798 6158 715 2845 6821 720 1914 7309 7579 8024 784 720 3198 952 3300 5031 1908 511 123 510 2146 1744 2798 6158 715 2845 1400 8024 711 784 720 6158 4989 1174 1048 749 3333 741 6381 5466 1218 8043 711 784 720 1348 6158 2128 2961 1168 1814 5052 7339 100 1391 4958 1510 100 8024 5632 2346 1316 1921 1921 1762 2769 812 3717 3799 1322 677 4408 6611 7946 7178 8043 124 510 6821 1126 702 3299 8024 3717 3799 3680 1417 6818 8185 1039 5283 1164 3883 8024 6820 897 679 2418 3724 8024 2146 1744 2798 6820 3926 677 7942 3124 2424 2857 924 955 5314 2146 1744 2798 807 802 2920 691 1322 2339 6598 4852 924 4638 7178 749 1408 8043 125 510 2945 749 6237 2146 1744 2798 1304 800 782 821 689 5307 5852 8024 1348 3612 4925 11411 8158 674 1039 510 3612 4852 924 12094 8158 674 1039 510 2418 6421 6820 3612 749 5466 2339 2339 6598 1126 1282 674 8024 677 7942 3124 2424 2802 5050 3296 2146 1744 2798 2857 924 6820 3221 2495 6820 8043 126 510 2769 812 1322 1394 3791 833 6369 1469 5439 3352 6158 1161 1152 1724 1168 1063 2399 8024 4385 1762 3302 1152 511 1322 2094 5314 2146 1744 2798 2487 1304 8024 2146 1744 2798 3680 1921 6611 8113 1914 674 1112 1164 3883 8024 1316 2190 1912 2146 4917 1921 1921 755 3315 8013 5023 1493 5439 3352 1152 4007 1726 1322 8024 2146 1744 2798 5314 1493 1322 100 1921 1921 755 100 1377 5543 6206 100 755 100 4638 1126 1283 674 1039 8024 4493 5635 1126 702 783 8024 2476 3875 741 6381 2644 2824 2857 6820 3221 677 7942 3124 2424 2824 2857 8043 2496 1159 1377 3221 2644 779 5632 2828 1322 769 5314 2146 1744 2798 4495 772 4638 8013 2361 3307 2528 2356 7270 4692 1168 3315 6585 1400 5543 1008 2821 4850 10864 510 2821 4850 6824 2456 5023 3696 4495 7309 7579 671 3416 8024 1068 3800 671 678 2769 812 3717 3799 1322 4638 2199 3341 8013 738 6435 2528 2356 7270 2853 3189 4415 674 3322 722 4958 779 5632 5276 6448 671 678 2496 752 782 8020 4294 1166 3221 6929 855 3177 4991 7270 8021 8024 1283 674 679 5543 1420 1357 671 7481 722 6791 102\n",
      "11/14/2019 08:25:20 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/14/2019 08:25:20 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "11/14/2019 08:25:20 - INFO - __main__ -   label: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/14/2019 08:26:44 - INFO - __main__ -   ***** Running training *****\n",
      "11/14/2019 08:26:44 - INFO - __main__ -     Num examples = 11758\n",
      "11/14/2019 08:26:44 - INFO - __main__ -     Batch size = 4\n",
      "11/14/2019 08:26:44 - INFO - __main__ -     Num steps = 30000\n",
      "  0%|                                                 | 0/30000 [00:00<?, ?it/s]11/14/2019 08:27:06 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 08:27:06 - INFO - __main__ -     Num examples = 2938\n",
      "11/14/2019 08:27:06 - INFO - __main__ -     Batch size = 48\n",
      "/root/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "11/14/2019 08:29:11 - INFO - __main__ -     eval_F1 = 0.20777464756786634\n",
      "11/14/2019 08:29:11 - INFO - __main__ -     eval_loss = 1.3233700135061819\n",
      "11/14/2019 08:29:11 - INFO - __main__ -     global_step = 0\n",
      "================================================================================\n",
      "Best F1 0.20777464756786634\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.6938:   3%|▋                       | 799/30000 [04:38<1:26:49,  5.61it/s]11/14/2019 08:31:22 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 08:31:22 - INFO - __main__ -     global_step = 200\n",
      "11/14/2019 08:31:22 - INFO - __main__ -     train loss = 0.6938\n",
      "loss 0.5716:   5%|█▏                     | 1599/30000 [06:48<1:24:18,  5.61it/s]11/14/2019 08:33:32 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 08:33:32 - INFO - __main__ -     global_step = 400\n",
      "11/14/2019 08:33:32 - INFO - __main__ -     train loss = 0.5716\n",
      "loss 0.4833:   8%|█▊                     | 2399/30000 [08:58<1:21:58,  5.61it/s]11/14/2019 08:35:42 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 08:35:42 - INFO - __main__ -     global_step = 600\n",
      "11/14/2019 08:35:42 - INFO - __main__ -     train loss = 0.4833\n",
      "loss 0.4402:  11%|██▍                    | 3199/30000 [11:08<1:19:24,  5.63it/s]11/14/2019 08:37:52 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 08:37:52 - INFO - __main__ -     global_step = 800\n",
      "11/14/2019 08:37:52 - INFO - __main__ -     train loss = 0.4402\n",
      "loss 0.3877:  13%|███                    | 3999/30000 [13:19<1:17:58,  5.56it/s]11/14/2019 08:40:03 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 08:40:03 - INFO - __main__ -     global_step = 1000\n",
      "11/14/2019 08:40:03 - INFO - __main__ -     train loss = 0.3877\n",
      "loss 0.3681:  16%|███▋                   | 4799/30000 [15:29<1:14:46,  5.62it/s]11/14/2019 08:42:13 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 08:42:13 - INFO - __main__ -     global_step = 1200\n",
      "11/14/2019 08:42:13 - INFO - __main__ -     train loss = 0.3681\n",
      "loss 0.4185:  19%|████▎                  | 5599/30000 [17:39<1:12:19,  5.62it/s]11/14/2019 08:44:23 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 08:44:23 - INFO - __main__ -     global_step = 1400\n",
      "11/14/2019 08:44:23 - INFO - __main__ -     train loss = 0.4185\n",
      "loss 0.4166:  21%|████▉                  | 6399/30000 [19:48<1:09:33,  5.66it/s]11/14/2019 08:46:33 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 08:46:33 - INFO - __main__ -     global_step = 1600\n",
      "11/14/2019 08:46:33 - INFO - __main__ -     train loss = 0.4166\n",
      "loss 0.377:  24%|█████▊                  | 7199/30000 [21:59<1:07:52,  5.60it/s]11/14/2019 08:48:43 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 08:48:43 - INFO - __main__ -     global_step = 1800\n",
      "11/14/2019 08:48:43 - INFO - __main__ -     train loss = 0.377\n",
      "loss 0.4026:  27%|██████▏                | 7999/30000 [24:09<1:05:19,  5.61it/s]11/14/2019 08:50:53 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 08:50:53 - INFO - __main__ -     global_step = 2000\n",
      "11/14/2019 08:50:53 - INFO - __main__ -     train loss = 0.4026\n",
      "loss 0.3878:  29%|██████▋                | 8799/30000 [26:20<1:03:40,  5.55it/s]11/14/2019 08:53:04 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 08:53:04 - INFO - __main__ -     global_step = 2200\n",
      "11/14/2019 08:53:04 - INFO - __main__ -     train loss = 0.3878\n",
      "loss 0.3684:  32%|███████▎               | 9599/30000 [28:31<1:01:08,  5.56it/s]11/14/2019 08:55:15 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 08:55:15 - INFO - __main__ -     global_step = 2400\n",
      "11/14/2019 08:55:15 - INFO - __main__ -     train loss = 0.3684\n",
      "loss 0.3944:  35%|████████▎               | 10399/30000 [30:42<58:23,  5.59it/s]11/14/2019 08:57:26 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 08:57:26 - INFO - __main__ -     global_step = 2600\n",
      "11/14/2019 08:57:26 - INFO - __main__ -     train loss = 0.3944\n",
      "loss 0.3477:  37%|████████▉               | 11199/30000 [32:52<56:12,  5.57it/s]11/14/2019 08:59:37 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 08:59:37 - INFO - __main__ -     global_step = 2800\n",
      "11/14/2019 08:59:37 - INFO - __main__ -     train loss = 0.3477\n",
      "11/14/2019 08:59:59 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 08:59:59 - INFO - __main__ -     Num examples = 2938\n",
      "11/14/2019 08:59:59 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 09:02:06 - INFO - __main__ -     eval_F1 = 0.7670843040944479\n",
      "11/14/2019 09:02:06 - INFO - __main__ -     eval_loss = 0.398044265446163\n",
      "11/14/2019 09:02:06 - INFO - __main__ -     global_step = 2800\n",
      "11/14/2019 09:02:06 - INFO - __main__ -     loss = 0.3477\n",
      "================================================================================\n",
      "Best F1 0.7670843040944479\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.3642:  40%|█████████▌              | 11999/30000 [37:36<54:51,  5.47it/s]11/14/2019 09:04:20 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 09:04:20 - INFO - __main__ -     global_step = 3000\n",
      "11/14/2019 09:04:20 - INFO - __main__ -     train loss = 0.3642\n",
      "11/14/2019 09:04:43 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 09:04:43 - INFO - __main__ -     Num examples = 2938\n",
      "11/14/2019 09:04:43 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 09:06:49 - INFO - __main__ -     eval_F1 = 0.776192757219615\n",
      "11/14/2019 09:06:49 - INFO - __main__ -     eval_loss = 0.36740334740569514\n",
      "11/14/2019 09:06:49 - INFO - __main__ -     global_step = 3000\n",
      "11/14/2019 09:06:49 - INFO - __main__ -     loss = 0.3642\n",
      "================================================================================\n",
      "Best F1 0.776192757219615\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.2968:  43%|██████████▏             | 12799/30000 [42:19<51:30,  5.57it/s]11/14/2019 09:09:03 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 09:09:03 - INFO - __main__ -     global_step = 3200\n",
      "11/14/2019 09:09:03 - INFO - __main__ -     train loss = 0.2968\n",
      "11/14/2019 09:09:26 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 09:09:26 - INFO - __main__ -     Num examples = 2938\n",
      "11/14/2019 09:09:26 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 09:11:32 - INFO - __main__ -     eval_F1 = 0.7936086866507579\n",
      "11/14/2019 09:11:32 - INFO - __main__ -     eval_loss = 0.4095012836398617\n",
      "11/14/2019 09:11:32 - INFO - __main__ -     global_step = 3200\n",
      "11/14/2019 09:11:32 - INFO - __main__ -     loss = 0.2968\n",
      "================================================================================\n",
      "Best F1 0.7936086866507579\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.2996:  45%|██████████▉             | 13599/30000 [47:03<49:31,  5.52it/s]11/14/2019 09:13:47 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 09:13:47 - INFO - __main__ -     global_step = 3400\n",
      "11/14/2019 09:13:47 - INFO - __main__ -     train loss = 0.2996\n",
      "11/14/2019 09:14:10 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 09:14:10 - INFO - __main__ -     Num examples = 2938\n",
      "11/14/2019 09:14:10 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 09:16:16 - INFO - __main__ -     eval_F1 = 0.7838533351777649\n",
      "11/14/2019 09:16:16 - INFO - __main__ -     eval_loss = 0.3582365825892456\n",
      "11/14/2019 09:16:16 - INFO - __main__ -     global_step = 3400\n",
      "11/14/2019 09:16:16 - INFO - __main__ -     loss = 0.2996\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.1997:  48%|███████████▌            | 14399/30000 [51:43<47:10,  5.51it/s]11/14/2019 09:18:27 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 09:18:27 - INFO - __main__ -     global_step = 3600\n",
      "11/14/2019 09:18:27 - INFO - __main__ -     train loss = 0.1997\n",
      "11/14/2019 09:18:50 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 09:18:50 - INFO - __main__ -     Num examples = 2938\n",
      "11/14/2019 09:18:50 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 09:20:56 - INFO - __main__ -     eval_F1 = 0.7909000017003459\n",
      "11/14/2019 09:20:56 - INFO - __main__ -     eval_loss = 0.4005510733132401\n",
      "11/14/2019 09:20:56 - INFO - __main__ -     global_step = 3600\n",
      "11/14/2019 09:20:56 - INFO - __main__ -     loss = 0.1997\n",
      "================================================================================\n",
      "loss 0.2342:  51%|████████████▏           | 15199/30000 [56:23<44:37,  5.53it/s]11/14/2019 09:23:08 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 09:23:08 - INFO - __main__ -     global_step = 3800\n",
      "11/14/2019 09:23:08 - INFO - __main__ -     train loss = 0.2342\n",
      "11/14/2019 09:23:30 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 09:23:30 - INFO - __main__ -     Num examples = 2938\n",
      "11/14/2019 09:23:30 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 09:25:36 - INFO - __main__ -     eval_F1 = 0.7811762127490195\n",
      "11/14/2019 09:25:36 - INFO - __main__ -     eval_loss = 0.4273136968934728\n",
      "11/14/2019 09:25:36 - INFO - __main__ -     global_step = 3800\n",
      "11/14/2019 09:25:36 - INFO - __main__ -     loss = 0.2342\n",
      "================================================================================\n",
      "loss 0.1613:  53%|███████████▋          | 15999/30000 [1:01:02<41:40,  5.60it/s]11/14/2019 09:27:47 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 09:27:47 - INFO - __main__ -     global_step = 4000\n",
      "11/14/2019 09:27:47 - INFO - __main__ -     train loss = 0.1613\n",
      "11/14/2019 09:28:09 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 09:28:09 - INFO - __main__ -     Num examples = 2938\n",
      "11/14/2019 09:28:09 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 09:30:15 - INFO - __main__ -     eval_F1 = 0.7949554617909048\n",
      "11/14/2019 09:30:15 - INFO - __main__ -     eval_loss = 0.4529699440745096\n",
      "11/14/2019 09:30:15 - INFO - __main__ -     global_step = 4000\n",
      "11/14/2019 09:30:15 - INFO - __main__ -     loss = 0.1613\n",
      "================================================================================\n",
      "Best F1 0.7949554617909048\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.2056:  56%|████████████▎         | 16799/30000 [1:05:45<39:50,  5.52it/s]11/14/2019 09:32:29 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 09:32:29 - INFO - __main__ -     global_step = 4200\n",
      "11/14/2019 09:32:29 - INFO - __main__ -     train loss = 0.2056\n",
      "11/14/2019 09:32:51 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 09:32:51 - INFO - __main__ -     Num examples = 2938\n",
      "11/14/2019 09:32:51 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 09:34:58 - INFO - __main__ -     eval_F1 = 0.7941295782987047\n",
      "11/14/2019 09:34:58 - INFO - __main__ -     eval_loss = 0.4170969907314547\n",
      "11/14/2019 09:34:58 - INFO - __main__ -     global_step = 4200\n",
      "11/14/2019 09:34:58 - INFO - __main__ -     loss = 0.2056\n",
      "================================================================================\n",
      "loss 0.2044:  59%|████████████▉         | 17599/30000 [1:10:25<37:19,  5.54it/s]11/14/2019 09:37:09 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 09:37:09 - INFO - __main__ -     global_step = 4400\n",
      "11/14/2019 09:37:09 - INFO - __main__ -     train loss = 0.2044\n",
      "11/14/2019 09:37:31 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 09:37:31 - INFO - __main__ -     Num examples = 2938\n",
      "11/14/2019 09:37:31 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 09:39:37 - INFO - __main__ -     eval_F1 = 0.7826011345724746\n",
      "11/14/2019 09:39:37 - INFO - __main__ -     eval_loss = 0.44981945148338714\n",
      "11/14/2019 09:39:37 - INFO - __main__ -     global_step = 4400\n",
      "11/14/2019 09:39:37 - INFO - __main__ -     loss = 0.2044\n",
      "================================================================================\n",
      "loss 0.243:  61%|██████████████         | 18399/30000 [1:15:03<34:41,  5.57it/s]11/14/2019 09:41:48 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 09:41:48 - INFO - __main__ -     global_step = 4600\n",
      "11/14/2019 09:41:48 - INFO - __main__ -     train loss = 0.243\n",
      "11/14/2019 09:42:10 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 09:42:10 - INFO - __main__ -     Num examples = 2938\n",
      "11/14/2019 09:42:10 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 09:44:16 - INFO - __main__ -     eval_F1 = 0.7996736070997992\n",
      "11/14/2019 09:44:16 - INFO - __main__ -     eval_loss = 0.4026139797943254\n",
      "11/14/2019 09:44:16 - INFO - __main__ -     global_step = 4600\n",
      "11/14/2019 09:44:16 - INFO - __main__ -     loss = 0.243\n",
      "================================================================================\n",
      "Best F1 0.7996736070997992\n",
      "Saving Model......\n",
      "================================================================================\n",
      "loss 0.1892:  64%|██████████████        | 19199/30000 [1:19:46<32:06,  5.61it/s]11/14/2019 09:46:30 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 09:46:30 - INFO - __main__ -     global_step = 4800\n",
      "11/14/2019 09:46:30 - INFO - __main__ -     train loss = 0.1892\n",
      "11/14/2019 09:46:53 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 09:46:53 - INFO - __main__ -     Num examples = 2938\n",
      "11/14/2019 09:46:53 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 09:48:59 - INFO - __main__ -     eval_F1 = 0.7883489016917963\n",
      "11/14/2019 09:48:59 - INFO - __main__ -     eval_loss = 0.4552186872930296\n",
      "11/14/2019 09:48:59 - INFO - __main__ -     global_step = 4800\n",
      "11/14/2019 09:48:59 - INFO - __main__ -     loss = 0.1892\n",
      "================================================================================\n",
      "loss 0.2203:  67%|██████████████▋       | 19999/30000 [1:24:26<29:54,  5.57it/s]11/14/2019 09:51:11 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 09:51:11 - INFO - __main__ -     global_step = 5000\n",
      "11/14/2019 09:51:11 - INFO - __main__ -     train loss = 0.2203\n",
      "11/14/2019 09:51:33 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 09:51:33 - INFO - __main__ -     Num examples = 2938\n",
      "11/14/2019 09:51:33 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 09:53:40 - INFO - __main__ -     eval_F1 = 0.7818584539038903\n",
      "11/14/2019 09:53:40 - INFO - __main__ -     eval_loss = 0.41959134981997553\n",
      "11/14/2019 09:53:40 - INFO - __main__ -     global_step = 5000\n",
      "11/14/2019 09:53:40 - INFO - __main__ -     loss = 0.2203\n",
      "================================================================================\n",
      "loss 0.2318:  69%|███████████████▎      | 20799/30000 [1:29:06<27:29,  5.58it/s]11/14/2019 09:55:50 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 09:55:50 - INFO - __main__ -     global_step = 5200\n",
      "11/14/2019 09:55:50 - INFO - __main__ -     train loss = 0.2318\n",
      "11/14/2019 09:56:13 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 09:56:13 - INFO - __main__ -     Num examples = 2938\n",
      "11/14/2019 09:56:13 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 09:58:19 - INFO - __main__ -     eval_F1 = 0.796831153367176\n",
      "11/14/2019 09:58:19 - INFO - __main__ -     eval_loss = 0.3945181598105738\n",
      "11/14/2019 09:58:19 - INFO - __main__ -     global_step = 5200\n",
      "11/14/2019 09:58:19 - INFO - __main__ -     loss = 0.2318\n",
      "================================================================================\n",
      "loss 0.1805:  72%|███████████████▊      | 21599/30000 [1:33:45<25:02,  5.59it/s]11/14/2019 10:00:30 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 10:00:30 - INFO - __main__ -     global_step = 5400\n",
      "11/14/2019 10:00:30 - INFO - __main__ -     train loss = 0.1805\n",
      "11/14/2019 10:00:52 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 10:00:52 - INFO - __main__ -     Num examples = 2938\n",
      "11/14/2019 10:00:52 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 10:02:58 - INFO - __main__ -     eval_F1 = 0.7887785862634704\n",
      "11/14/2019 10:02:58 - INFO - __main__ -     eval_loss = 0.4392887143598449\n",
      "11/14/2019 10:02:58 - INFO - __main__ -     global_step = 5400\n",
      "11/14/2019 10:02:58 - INFO - __main__ -     loss = 0.1805\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.2037:  75%|████████████████▍     | 22399/30000 [1:38:25<22:42,  5.58it/s]11/14/2019 10:05:09 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 10:05:09 - INFO - __main__ -     global_step = 5600\n",
      "11/14/2019 10:05:09 - INFO - __main__ -     train loss = 0.2037\n",
      "11/14/2019 10:05:32 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 10:05:32 - INFO - __main__ -     Num examples = 2938\n",
      "11/14/2019 10:05:32 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 10:07:38 - INFO - __main__ -     eval_F1 = 0.7941498931442802\n",
      "11/14/2019 10:07:38 - INFO - __main__ -     eval_loss = 0.4345087304771427\n",
      "11/14/2019 10:07:38 - INFO - __main__ -     global_step = 5600\n",
      "11/14/2019 10:07:38 - INFO - __main__ -     loss = 0.2037\n",
      "================================================================================\n",
      "loss 0.1832:  77%|█████████████████     | 23199/30000 [1:43:04<20:11,  5.61it/s]11/14/2019 10:09:48 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 10:09:48 - INFO - __main__ -     global_step = 5800\n",
      "11/14/2019 10:09:48 - INFO - __main__ -     train loss = 0.1832\n",
      "11/14/2019 10:10:11 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 10:10:11 - INFO - __main__ -     Num examples = 2938\n",
      "11/14/2019 10:10:11 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 10:12:17 - INFO - __main__ -     eval_F1 = 0.7969853998867872\n",
      "11/14/2019 10:12:17 - INFO - __main__ -     eval_loss = 0.42802011771428006\n",
      "11/14/2019 10:12:17 - INFO - __main__ -     global_step = 5800\n",
      "11/14/2019 10:12:17 - INFO - __main__ -     loss = 0.1832\n",
      "================================================================================\n",
      "loss 0.1989:  80%|█████████████████▌    | 23999/30000 [1:47:44<17:58,  5.57it/s]11/14/2019 10:14:29 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 10:14:29 - INFO - __main__ -     global_step = 6000\n",
      "11/14/2019 10:14:29 - INFO - __main__ -     train loss = 0.1989\n",
      "11/14/2019 10:14:52 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 10:14:52 - INFO - __main__ -     Num examples = 2938\n",
      "11/14/2019 10:14:52 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 10:16:59 - INFO - __main__ -     eval_F1 = 0.7994351166756221\n",
      "11/14/2019 10:16:59 - INFO - __main__ -     eval_loss = 0.4204347223764466\n",
      "11/14/2019 10:16:59 - INFO - __main__ -     global_step = 6000\n",
      "11/14/2019 10:16:59 - INFO - __main__ -     loss = 0.1989\n",
      "================================================================================\n",
      "loss 0.1339:  83%|██████████████████▏   | 24799/30000 [1:52:26<15:29,  5.59it/s]11/14/2019 10:19:10 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 10:19:10 - INFO - __main__ -     global_step = 6200\n",
      "11/14/2019 10:19:10 - INFO - __main__ -     train loss = 0.1339\n",
      "11/14/2019 10:19:32 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 10:19:32 - INFO - __main__ -     Num examples = 2938\n",
      "11/14/2019 10:19:32 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 10:21:39 - INFO - __main__ -     eval_F1 = 0.7961343919482292\n",
      "11/14/2019 10:21:39 - INFO - __main__ -     eval_loss = 0.434921944153405\n",
      "11/14/2019 10:21:39 - INFO - __main__ -     global_step = 6200\n",
      "11/14/2019 10:21:39 - INFO - __main__ -     loss = 0.1339\n",
      "================================================================================\n",
      "loss 0.1439:  85%|██████████████████▊   | 25599/30000 [1:57:05<13:06,  5.60it/s]11/14/2019 10:23:49 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 10:23:49 - INFO - __main__ -     global_step = 6400\n",
      "11/14/2019 10:23:49 - INFO - __main__ -     train loss = 0.1439\n",
      "11/14/2019 10:24:12 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 10:24:12 - INFO - __main__ -     Num examples = 2938\n",
      "11/14/2019 10:24:12 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 10:26:18 - INFO - __main__ -     eval_F1 = 0.7783540832592495\n",
      "11/14/2019 10:26:18 - INFO - __main__ -     eval_loss = 0.4351736651433091\n",
      "11/14/2019 10:26:18 - INFO - __main__ -     global_step = 6400\n",
      "11/14/2019 10:26:18 - INFO - __main__ -     loss = 0.1439\n",
      "================================================================================\n",
      "loss 0.1093:  88%|███████████████████▎  | 26399/30000 [2:01:45<10:53,  5.51it/s]11/14/2019 10:28:30 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 10:28:30 - INFO - __main__ -     global_step = 6600\n",
      "11/14/2019 10:28:30 - INFO - __main__ -     train loss = 0.1093\n",
      "11/14/2019 10:28:53 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 10:28:53 - INFO - __main__ -     Num examples = 2938\n",
      "11/14/2019 10:28:53 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 10:30:59 - INFO - __main__ -     eval_F1 = 0.7954809860781706\n",
      "11/14/2019 10:30:59 - INFO - __main__ -     eval_loss = 0.453086273684617\n",
      "11/14/2019 10:30:59 - INFO - __main__ -     global_step = 6600\n",
      "11/14/2019 10:30:59 - INFO - __main__ -     loss = 0.1093\n",
      "================================================================================\n",
      "loss 0.0851:  91%|███████████████████▉  | 27199/30000 [2:06:25<08:21,  5.58it/s]11/14/2019 10:33:10 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 10:33:10 - INFO - __main__ -     global_step = 6800\n",
      "11/14/2019 10:33:10 - INFO - __main__ -     train loss = 0.0851\n",
      "11/14/2019 10:33:36 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 10:33:36 - INFO - __main__ -     Num examples = 2938\n",
      "11/14/2019 10:33:36 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 10:35:42 - INFO - __main__ -     eval_F1 = 0.7932509195597892\n",
      "11/14/2019 10:35:42 - INFO - __main__ -     eval_loss = 0.47515816866390165\n",
      "11/14/2019 10:35:42 - INFO - __main__ -     global_step = 6800\n",
      "11/14/2019 10:35:42 - INFO - __main__ -     loss = 0.0851\n",
      "================================================================================\n",
      "loss 0.0696:  93%|████████████████████▌ | 27999/30000 [2:11:09<05:59,  5.56it/s]11/14/2019 10:37:53 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 10:37:53 - INFO - __main__ -     global_step = 7000\n",
      "11/14/2019 10:37:53 - INFO - __main__ -     train loss = 0.0696\n",
      "11/14/2019 10:38:15 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 10:38:15 - INFO - __main__ -     Num examples = 2938\n",
      "11/14/2019 10:38:15 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 10:40:21 - INFO - __main__ -     eval_F1 = 0.7986220294027633\n",
      "11/14/2019 10:40:21 - INFO - __main__ -     eval_loss = 0.49053513892595807\n",
      "11/14/2019 10:40:21 - INFO - __main__ -     global_step = 7000\n",
      "11/14/2019 10:40:21 - INFO - __main__ -     loss = 0.0696\n",
      "================================================================================\n",
      "loss 0.0806:  96%|█████████████████████ | 28799/30000 [2:15:47<03:34,  5.59it/s]11/14/2019 10:42:31 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 10:42:31 - INFO - __main__ -     global_step = 7200\n",
      "11/14/2019 10:42:31 - INFO - __main__ -     train loss = 0.0806\n",
      "11/14/2019 10:42:54 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 10:42:54 - INFO - __main__ -     Num examples = 2938\n",
      "11/14/2019 10:42:54 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 10:45:00 - INFO - __main__ -     eval_F1 = 0.7927217596807982\n",
      "11/14/2019 10:45:00 - INFO - __main__ -     eval_loss = 0.4952152328445546\n",
      "11/14/2019 10:45:00 - INFO - __main__ -     global_step = 7200\n",
      "11/14/2019 10:45:00 - INFO - __main__ -     loss = 0.0806\n",
      "================================================================================\n",
      "loss 0.1228:  99%|█████████████████████▋| 29599/30000 [2:20:26<01:11,  5.59it/s]11/14/2019 10:47:10 - INFO - __main__ -   ***** Report result *****\n",
      "11/14/2019 10:47:10 - INFO - __main__ -     global_step = 7400\n",
      "11/14/2019 10:47:10 - INFO - __main__ -     train loss = 0.1228\n",
      "11/14/2019 10:47:34 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/14/2019 10:47:34 - INFO - __main__ -     Num examples = 2938\n",
      "11/14/2019 10:47:34 - INFO - __main__ -     Batch size = 48\n",
      "11/14/2019 10:49:40 - INFO - __main__ -     eval_F1 = 0.7950616645422587\n",
      "11/14/2019 10:49:40 - INFO - __main__ -     eval_loss = 0.49793788595425503\n",
      "11/14/2019 10:49:40 - INFO - __main__ -     global_step = 7400\n",
      "11/14/2019 10:49:40 - INFO - __main__ -     loss = 0.1228\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.1056: 100%|██████████████████████| 30000/30000 [2:24:01<00:00,  6.17it/s]\n",
      "11/14/2019 10:50:46 - INFO - pytorch_transformers.modeling_utils -   loading weights file ../model/roberta_wwm_large_512_1_last2embedding_cls_replacement/roberta_wwm_large_512_1_last2embedding_cls_replacement_4/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "Traceback (most recent call last):\n",
      "  File \"./run_bert_2562_last2embedding_cls.py\", line 841, in <module>\n",
      "    main()\n",
      "  File \"./run_bert_2562_last2embedding_cls.py\", line 757, in main\n",
      "    logits = model(input_ids=input_ids, token_type_ids=segment_ids, attention_mask=input_mask).detach().cpu().numpy()\n",
      "  File \"/root/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 547, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/root/code/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 1119, in forward\n",
      "    attention_mask=flat_attention_mask, head_mask=head_mask)\n",
      "  File \"/root/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 547, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/root/code/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 727, in forward\n",
      "    head_mask=head_mask)\n",
      "  File \"/root/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 547, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/root/code/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 440, in forward\n",
      "    layer_outputs = layer_module(hidden_states, attention_mask, head_mask[i])\n",
      "  File \"/root/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 547, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/root/code/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 420, in forward\n",
      "    intermediate_output = self.intermediate(attention_output)\n",
      "  File \"/root/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 547, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/root/code/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 392, in forward\n",
      "    hidden_states = self.intermediate_act_fn(hidden_states)\n",
      "  File \"/root/code/BDCI-CCF-SENTIMENT-CLASSIFICATION/CCF-BDCI-Sentiment-Analysis-Baseline/pytorch_transformers/modeling_bert.py\", line 142, in gelu\n",
      "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 384.00 MiB (GPU 0; 10.73 GiB total capacity; 8.60 GiB already allocated; 339.62 MiB free; 906.30 MiB cached)\n"
     ]
    }
   ],
   "source": [
    "!python ./run_bert_2562_last2embedding_cls.py \\\n",
    "--model_type bert \\\n",
    "--model_name_or_path ../model/chinese_roberta_wwm_large_ext_pytorch  \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_test \\\n",
    "--data_dir ../data/data_StratifiedKFold_42/data_replacement_4 \\\n",
    "--output_dir ../model/roberta_wwm_large_512_1_last2embedding_cls_replacement/roberta_wwm_large_512_1_last2embedding_cls_replacement_4 \\\n",
    "--max_seq_length 512 \\\n",
    "--split_num 1 \\\n",
    "--lstm_hidden_size 512 \\\n",
    "--lstm_layers 1 \\\n",
    "--lstm_dropout 0.1 \\\n",
    "--eval_steps 200 \\\n",
    "--per_gpu_train_batch_size 4 \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--warmup_steps 0 \\\n",
    "--per_gpu_eval_batch_size 48 \\\n",
    "--learning_rate 1e-5 \\\n",
    "--adam_epsilon 1e-6 \\\n",
    "--weight_decay 0 \\\n",
    "--train_steps 30000 \\\n",
    "--freeze 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/14/2019 14:19:45 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "11/14/2019 14:19:45 - INFO - pytorch_transformers.tokenization_utils -   Model name '../model/chinese_roberta_wwm_large_ext_pytorch' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming '../model/chinese_roberta_wwm_large_ext_pytorch' is a path or url to a directory containing tokenizer files.\n",
      "11/14/2019 14:19:45 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../model/chinese_roberta_wwm_large_ext_pytorch/added_tokens.json. We won't load it.\n",
      "11/14/2019 14:19:45 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../model/chinese_roberta_wwm_large_ext_pytorch/special_tokens_map.json. We won't load it.\n",
      "11/14/2019 14:19:45 - INFO - pytorch_transformers.tokenization_utils -   loading file ../model/chinese_roberta_wwm_large_ext_pytorch/vocab.txt\n",
      "11/14/2019 14:19:45 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/14/2019 14:19:45 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/14/2019 14:19:45 - INFO - pytorch_transformers.modeling_utils -   loading configuration file ../model/chinese_roberta_wwm_large_ext_pytorch/config.json\n",
      "11/14/2019 14:19:45 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 3,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "11/14/2019 14:19:45 - INFO - pytorch_transformers.modeling_utils -   loading weights file ../model/chinese_roberta_wwm_large_ext_pytorch/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "11/14/2019 14:19:54 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification_last2embedding_cls not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "11/14/2019 14:19:54 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification_last2embedding_cls: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "11/14/2019 14:20:01 - INFO - pytorch_transformers.modeling_utils -   loading weights file ../model/roberta_wwm_large_512_1_last2embedding_cls_replacement/roberta_wwm_large_512_1_last2embedding_cls_replacement_0/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "dev 0.8122144428450137\n",
      "/root/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "test 0.07110787525785706\n"
     ]
    }
   ],
   "source": [
    "!python ./run_bert_2562_last2embedding_cls.py \\\n",
    "--model_type bert \\\n",
    "--model_name_or_path ../model/chinese_roberta_wwm_large_ext_pytorch  \\\n",
    "--do_test \\\n",
    "--data_dir ../data/data_StratifiedKFold_42/data_replacement_0 \\\n",
    "--output_dir ../model/roberta_wwm_large_512_1_last2embedding_cls_replacement/roberta_wwm_large_512_1_last2embedding_cls_replacement_0 \\\n",
    "--max_seq_length 512 \\\n",
    "--split_num 1 \\\n",
    "--lstm_hidden_size 512 \\\n",
    "--lstm_layers 1 \\\n",
    "--lstm_dropout 0.1 \\\n",
    "--eval_steps 200 \\\n",
    "--per_gpu_train_batch_size 4 \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--warmup_steps 0 \\\n",
    "--per_gpu_eval_batch_size 64 \\\n",
    "--learning_rate 1e-5 \\\n",
    "--adam_epsilon 1e-6 \\\n",
    "--weight_decay 0 \\\n",
    "--train_steps 30000 \\\n",
    "--freeze 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/14/2019 14:29:12 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "11/14/2019 14:29:12 - INFO - pytorch_transformers.tokenization_utils -   Model name '../model/chinese_roberta_wwm_large_ext_pytorch' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming '../model/chinese_roberta_wwm_large_ext_pytorch' is a path or url to a directory containing tokenizer files.\n",
      "11/14/2019 14:29:12 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../model/chinese_roberta_wwm_large_ext_pytorch/added_tokens.json. We won't load it.\n",
      "11/14/2019 14:29:12 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../model/chinese_roberta_wwm_large_ext_pytorch/special_tokens_map.json. We won't load it.\n",
      "11/14/2019 14:29:12 - INFO - pytorch_transformers.tokenization_utils -   loading file ../model/chinese_roberta_wwm_large_ext_pytorch/vocab.txt\n",
      "11/14/2019 14:29:12 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/14/2019 14:29:12 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/14/2019 14:29:12 - INFO - pytorch_transformers.modeling_utils -   loading configuration file ../model/chinese_roberta_wwm_large_ext_pytorch/config.json\n",
      "11/14/2019 14:29:12 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 3,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "11/14/2019 14:29:12 - INFO - pytorch_transformers.modeling_utils -   loading weights file ../model/chinese_roberta_wwm_large_ext_pytorch/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "11/14/2019 14:29:22 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification_last2embedding_cls not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "11/14/2019 14:29:22 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification_last2embedding_cls: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "11/14/2019 14:29:28 - INFO - pytorch_transformers.modeling_utils -   loading weights file ../model/roberta_wwm_large_512_1_last2embedding_cls_replacement/roberta_wwm_large_512_1_last2embedding_cls_replacement_1/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "dev 0.8061700725936812\n",
      "/root/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "test 0.06312510247581571\n"
     ]
    }
   ],
   "source": [
    "!python ./run_bert_2562_last2embedding_cls.py \\\n",
    "--model_type bert \\\n",
    "--model_name_or_path ../model/chinese_roberta_wwm_large_ext_pytorch  \\\n",
    "--do_test \\\n",
    "--data_dir ../data/data_StratifiedKFold_42/data_replacement_1 \\\n",
    "--output_dir ../model/roberta_wwm_large_512_1_last2embedding_cls_replacement/roberta_wwm_large_512_1_last2embedding_cls_replacement_1 \\\n",
    "--max_seq_length 512 \\\n",
    "--split_num 1 \\\n",
    "--lstm_hidden_size 512 \\\n",
    "--lstm_layers 1 \\\n",
    "--lstm_dropout 0.1 \\\n",
    "--eval_steps 200 \\\n",
    "--per_gpu_train_batch_size 4 \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--warmup_steps 0 \\\n",
    "--per_gpu_eval_batch_size 64 \\\n",
    "--learning_rate 1e-5 \\\n",
    "--adam_epsilon 1e-6 \\\n",
    "--weight_decay 0 \\\n",
    "--train_steps 30000 \\\n",
    "--freeze 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/14/2019 14:38:43 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "11/14/2019 14:38:43 - INFO - pytorch_transformers.tokenization_utils -   Model name '../model/chinese_roberta_wwm_large_ext_pytorch' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming '../model/chinese_roberta_wwm_large_ext_pytorch' is a path or url to a directory containing tokenizer files.\n",
      "11/14/2019 14:38:43 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../model/chinese_roberta_wwm_large_ext_pytorch/added_tokens.json. We won't load it.\n",
      "11/14/2019 14:38:43 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../model/chinese_roberta_wwm_large_ext_pytorch/special_tokens_map.json. We won't load it.\n",
      "11/14/2019 14:38:43 - INFO - pytorch_transformers.tokenization_utils -   loading file ../model/chinese_roberta_wwm_large_ext_pytorch/vocab.txt\n",
      "11/14/2019 14:38:43 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/14/2019 14:38:43 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/14/2019 14:38:43 - INFO - pytorch_transformers.modeling_utils -   loading configuration file ../model/chinese_roberta_wwm_large_ext_pytorch/config.json\n",
      "11/14/2019 14:38:43 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 3,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "11/14/2019 14:38:43 - INFO - pytorch_transformers.modeling_utils -   loading weights file ../model/chinese_roberta_wwm_large_ext_pytorch/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "11/14/2019 14:38:51 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification_last2embedding_cls not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "11/14/2019 14:38:51 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification_last2embedding_cls: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "11/14/2019 14:38:58 - INFO - pytorch_transformers.modeling_utils -   loading weights file ../model/roberta_wwm_large_512_1_last2embedding_cls_replacement/roberta_wwm_large_512_1_last2embedding_cls_replacement_2/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "dev 0.7990296042189696\n",
      "/root/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "test 0.0668133300741465\n"
     ]
    }
   ],
   "source": [
    "!python ./run_bert_2562_last2embedding_cls.py \\\n",
    "--model_type bert \\\n",
    "--model_name_or_path ../model/chinese_roberta_wwm_large_ext_pytorch  \\\n",
    "--do_test \\\n",
    "--data_dir ../data/data_StratifiedKFold_42/data_replacement_2 \\\n",
    "--output_dir ../model/roberta_wwm_large_512_1_last2embedding_cls_replacement/roberta_wwm_large_512_1_last2embedding_cls_replacement_2 \\\n",
    "--max_seq_length 512 \\\n",
    "--split_num 1 \\\n",
    "--lstm_hidden_size 512 \\\n",
    "--lstm_layers 1 \\\n",
    "--lstm_dropout 0.1 \\\n",
    "--eval_steps 200 \\\n",
    "--per_gpu_train_batch_size 4 \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--warmup_steps 0 \\\n",
    "--per_gpu_eval_batch_size 64 \\\n",
    "--learning_rate 1e-5 \\\n",
    "--adam_epsilon 1e-6 \\\n",
    "--weight_decay 0 \\\n",
    "--train_steps 30000 \\\n",
    "--freeze 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/14/2019 14:48:06 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "11/14/2019 14:48:06 - INFO - pytorch_transformers.tokenization_utils -   Model name '../model/chinese_roberta_wwm_large_ext_pytorch' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming '../model/chinese_roberta_wwm_large_ext_pytorch' is a path or url to a directory containing tokenizer files.\n",
      "11/14/2019 14:48:06 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../model/chinese_roberta_wwm_large_ext_pytorch/added_tokens.json. We won't load it.\n",
      "11/14/2019 14:48:06 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../model/chinese_roberta_wwm_large_ext_pytorch/special_tokens_map.json. We won't load it.\n",
      "11/14/2019 14:48:06 - INFO - pytorch_transformers.tokenization_utils -   loading file ../model/chinese_roberta_wwm_large_ext_pytorch/vocab.txt\n",
      "11/14/2019 14:48:06 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/14/2019 14:48:06 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/14/2019 14:48:06 - INFO - pytorch_transformers.modeling_utils -   loading configuration file ../model/chinese_roberta_wwm_large_ext_pytorch/config.json\n",
      "11/14/2019 14:48:06 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 3,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "11/14/2019 14:48:06 - INFO - pytorch_transformers.modeling_utils -   loading weights file ../model/chinese_roberta_wwm_large_ext_pytorch/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "11/14/2019 14:48:15 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification_last2embedding_cls not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "11/14/2019 14:48:15 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification_last2embedding_cls: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "11/14/2019 14:48:20 - INFO - pytorch_transformers.modeling_utils -   loading weights file ../model/roberta_wwm_large_512_1_last2embedding_cls_replacement/roberta_wwm_large_512_1_last2embedding_cls_replacement_3/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "dev 0.8130676085518607\n",
      "/root/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "test 0.0746280659429031\n"
     ]
    }
   ],
   "source": [
    "!python ./run_bert_2562_last2embedding_cls.py \\\n",
    "--model_type bert \\\n",
    "--model_name_or_path ../model/chinese_roberta_wwm_large_ext_pytorch  \\\n",
    "--do_test \\\n",
    "--data_dir ../data/data_StratifiedKFold_42/data_replacement_3 \\\n",
    "--output_dir ../model/roberta_wwm_large_512_1_last2embedding_cls_replacement/roberta_wwm_large_512_1_last2embedding_cls_replacement_3 \\\n",
    "--max_seq_length 512 \\\n",
    "--split_num 1 \\\n",
    "--lstm_hidden_size 512 \\\n",
    "--lstm_layers 1 \\\n",
    "--lstm_dropout 0.1 \\\n",
    "--eval_steps 200 \\\n",
    "--per_gpu_train_batch_size 4 \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--warmup_steps 0 \\\n",
    "--per_gpu_eval_batch_size 64 \\\n",
    "--learning_rate 1e-5 \\\n",
    "--adam_epsilon 1e-6 \\\n",
    "--weight_decay 0 \\\n",
    "--train_steps 30000 \\\n",
    "--freeze 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/14/2019 14:57:27 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "11/14/2019 14:57:27 - INFO - pytorch_transformers.tokenization_utils -   Model name '../model/chinese_roberta_wwm_large_ext_pytorch' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming '../model/chinese_roberta_wwm_large_ext_pytorch' is a path or url to a directory containing tokenizer files.\n",
      "11/14/2019 14:57:27 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../model/chinese_roberta_wwm_large_ext_pytorch/added_tokens.json. We won't load it.\n",
      "11/14/2019 14:57:27 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file ../model/chinese_roberta_wwm_large_ext_pytorch/special_tokens_map.json. We won't load it.\n",
      "11/14/2019 14:57:27 - INFO - pytorch_transformers.tokenization_utils -   loading file ../model/chinese_roberta_wwm_large_ext_pytorch/vocab.txt\n",
      "11/14/2019 14:57:27 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/14/2019 14:57:27 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/14/2019 14:57:27 - INFO - pytorch_transformers.modeling_utils -   loading configuration file ../model/chinese_roberta_wwm_large_ext_pytorch/config.json\n",
      "11/14/2019 14:57:27 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 3,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "11/14/2019 14:57:27 - INFO - pytorch_transformers.modeling_utils -   loading weights file ../model/chinese_roberta_wwm_large_ext_pytorch/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "11/14/2019 14:57:36 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification_last2embedding_cls not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "11/14/2019 14:57:36 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification_last2embedding_cls: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "11/14/2019 14:57:42 - INFO - pytorch_transformers.modeling_utils -   loading weights file ../model/roberta_wwm_large_512_1_last2embedding_cls_replacement/roberta_wwm_large_512_1_last2embedding_cls_replacement_4/pytorch_model.bin\n",
      "--------- this is freeze: 0 ----------\n",
      "dev 0.7996736070997992\n",
      "/root/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "test 0.05431482636722812\n"
     ]
    }
   ],
   "source": [
    "!python ./run_bert_2562_last2embedding_cls.py \\\n",
    "--model_type bert \\\n",
    "--model_name_or_path ../model/chinese_roberta_wwm_large_ext_pytorch  \\\n",
    "--do_test \\\n",
    "--data_dir ../data/data_StratifiedKFold_42/data_replacement_4 \\\n",
    "--output_dir ../model/roberta_wwm_large_512_1_last2embedding_cls_replacement/roberta_wwm_large_512_1_last2embedding_cls_replacement_4 \\\n",
    "--max_seq_length 512 \\\n",
    "--split_num 1 \\\n",
    "--lstm_hidden_size 512 \\\n",
    "--lstm_layers 1 \\\n",
    "--lstm_dropout 0.1 \\\n",
    "--eval_steps 200 \\\n",
    "--per_gpu_train_batch_size 4 \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--warmup_steps 0 \\\n",
    "--per_gpu_eval_batch_size 64 \\\n",
    "--learning_rate 1e-5 \\\n",
    "--adam_epsilon 1e-6 \\\n",
    "--weight_decay 0 \\\n",
    "--train_steps 30000 \\\n",
    "--freeze 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11323650545336188\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df = pd.read_csv('../model/roberta_wwm_large_512_1_last2embedding_cls_replacement/roberta_wwm_large_512_1_last2embedding_cls_replacement_0/sub.csv')\n",
    "df = df[['id']]\n",
    "df['0'] = 0\n",
    "df['1'] = 0\n",
    "df['2'] = 0\n",
    "\n",
    "k = 5\n",
    "for i in [0,1,2,3,4]:\n",
    "    temp=pd.read_csv('../model/roberta_wwm_large_512_1_last2embedding_cls_replacement/roberta_wwm_large_512_1_last2embedding_cls_replacement_{}/sub.csv'.format(i))\n",
    "    df['0']+=temp['label_0']/k\n",
    "    df['1']+=temp['label_1']/k\n",
    "    df['2']+=temp['label_2']/k\n",
    "print(df['0'].mean())\n",
    "\n",
    "df['label']=np.argmax(df[['0','1','2']].values,-1)\n",
    "df[['id','label']].to_csv('../model/roberta_wwm_large_512_1_last2embedding_cls_replacement/roberta_wwm_large_512_1_last2embedding_cls_replacement_sub.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
